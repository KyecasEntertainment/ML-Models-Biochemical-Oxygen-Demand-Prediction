{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ko93QFRaDHBp",
        "H2ddA7kQ4fDD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPROCESSING**"
      ],
      "metadata": {
        "id": "-8UJxPaOi53W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51EwJGWqnnT8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7bd32fa1-4668-4ce2-db9e-402b00bb2c25"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Registering two gradient with name 'ReduceDataset'! (Previous registration was in register /usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/registry.py:65)\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4a462479ac58>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[0;31m# line: 456\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[0;31m# line: 365\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[0;31m# line: 418\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotDifferentiable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ReduceDataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# A constant that can be used to enable auto-tuning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mno_gradient\u001b[0;34m(op_type)\u001b[0m\n\u001b[1;32m   1733\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"op_type must be a string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m   \u001b[0mgradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/registry.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, candidate, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LOCATION_TAG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       raise KeyError(\n\u001b[0m\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"Registering two %s with name '%s'! \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34m\"(Previous registration was in %s %s:%d)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Registering two gradient with name 'ReduceDataset'! (Previous registration was in register /usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/registry.py:65)\""
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the datasets\n",
        "gunao = pd.read_csv('gunao_surface.csv')\n",
        "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
        "\n",
        "# Filter for surface data in tikob dataset\n",
        "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
        "\n",
        "# Columns to exclude\n",
        "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
        "\n",
        "# Filter columns for both datasets\n",
        "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
        "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
        "\n",
        "# Define feature columns and target column\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "target_column = 'BOD (mg/L)'\n",
        "\n",
        "# Extract features and target from both datasets\n",
        "X_tikob = tikob_fil[feature_columns]\n",
        "y_tikob = tikob_fil[target_column]\n",
        "X_gunao = gunao_fil[feature_columns]\n",
        "y_gunao = gunao_fil[target_column]\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
        "y_combined = pd.concat([y_tikob, y_gunao], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING THE MODELS**"
      ],
      "metadata": {
        "id": "ko93QFRaDHBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the datasets\n",
        "gunao = pd.read_csv('gunao_surface.csv')\n",
        "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
        "\n",
        "# Filter for surface data in tikob dataset\n",
        "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
        "\n",
        "# Columns to exclude\n",
        "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
        "\n",
        "# Filter columns for both datasets\n",
        "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
        "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
        "\n",
        "# Define feature columns and target column\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "target_column = 'BOD (mg/L)'\n",
        "\n",
        "# Extract features and target from both datasets\n",
        "X_tikob = tikob_fil[feature_columns]\n",
        "y_tikob = tikob_fil[target_column]\n",
        "X_gunao = gunao_fil[feature_columns]\n",
        "y_gunao = gunao_fil[target_column]\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
        "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_combined)\n",
        "\n",
        "# Train-test split for full dataset\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_scaled, y_combined, test_size=0.2, random_state=1)\n",
        "\n",
        "# Define the ANN model function\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train_full.shape[1], activation='relu'))\n",
        "    model.add(Dense(64, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_full, y_train_full, epochs=100, batch_size=20, verbose=1)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_predictions_full = model.predict(X_test_full).flatten()\n",
        "test_mse_full = mean_squared_error(y_test_full, test_predictions_full)\n",
        "test_rmse_full = np.sqrt(test_mse_full)\n",
        "test_mae_full = mean_absolute_error(y_test_full, test_predictions_full)\n",
        "test_r2_full = r2_score(y_test_full, test_predictions_full)\n",
        "test_mape_full = np.mean(np.abs((y_test_full - test_predictions_full) / y_test_full)) * 100\n",
        "\n",
        "print('Test Results on Full Dataset:')\n",
        "print('MSE:', test_mse_full)\n",
        "print('RMSE:', test_rmse_full)\n",
        "print('MAE:', test_mae_full)\n",
        "print('R^2:', test_r2_full)\n",
        "print('MAPE:', test_mape_full, '%')\n",
        "\n",
        "# Save the model\n",
        "model.save('ANN_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEFTeTFFn9cg",
        "outputId": "92accad9-90e1-4fc5-8535-76ef70f2bb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.9091\n",
            "Epoch 2/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6839 \n",
            "Epoch 3/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3351\n",
            "Epoch 4/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1957 \n",
            "Epoch 5/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0125 \n",
            "Epoch 6/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0086 \n",
            "Epoch 7/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9709 \n",
            "Epoch 8/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6957 \n",
            "Epoch 9/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8994 \n",
            "Epoch 10/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7277 \n",
            "Epoch 11/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7521 \n",
            "Epoch 12/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7487 \n",
            "Epoch 13/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6999 \n",
            "Epoch 14/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7275 \n",
            "Epoch 15/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6499 \n",
            "Epoch 16/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7197 \n",
            "Epoch 17/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6062  \n",
            "Epoch 18/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5315  \n",
            "Epoch 19/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4778 \n",
            "Epoch 20/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4677 \n",
            "Epoch 21/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4853 \n",
            "Epoch 22/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5250 \n",
            "Epoch 23/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4490 \n",
            "Epoch 24/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4878 \n",
            "Epoch 25/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4209 \n",
            "Epoch 26/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4424 \n",
            "Epoch 27/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4488 \n",
            "Epoch 28/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4122 \n",
            "Epoch 29/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4232 \n",
            "Epoch 30/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3235 \n",
            "Epoch 31/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3011 \n",
            "Epoch 32/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3190  \n",
            "Epoch 33/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3179 \n",
            "Epoch 34/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3386 \n",
            "Epoch 35/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3836 \n",
            "Epoch 36/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2820 \n",
            "Epoch 37/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3228 \n",
            "Epoch 38/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2578 \n",
            "Epoch 39/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3000 \n",
            "Epoch 40/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2722 \n",
            "Epoch 41/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2658 \n",
            "Epoch 42/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2973 \n",
            "Epoch 43/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2540 \n",
            "Epoch 44/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2404  \n",
            "Epoch 45/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2797  \n",
            "Epoch 46/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2394 \n",
            "Epoch 47/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2115 \n",
            "Epoch 48/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2071 \n",
            "Epoch 49/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2161 \n",
            "Epoch 50/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1888 \n",
            "Epoch 51/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2004 \n",
            "Epoch 52/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2105  \n",
            "Epoch 53/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1915 \n",
            "Epoch 54/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2019 \n",
            "Epoch 55/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1732 \n",
            "Epoch 56/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2151 \n",
            "Epoch 57/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2017 \n",
            "Epoch 58/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1870  \n",
            "Epoch 59/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1802 \n",
            "Epoch 60/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2004 \n",
            "Epoch 61/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1534 \n",
            "Epoch 62/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1723 \n",
            "Epoch 63/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1249 \n",
            "Epoch 64/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1390 \n",
            "Epoch 65/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1594 \n",
            "Epoch 66/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1256 \n",
            "Epoch 67/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1534 \n",
            "Epoch 68/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1253 \n",
            "Epoch 69/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1250 \n",
            "Epoch 70/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1112 \n",
            "Epoch 71/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1316 \n",
            "Epoch 72/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1310 \n",
            "Epoch 73/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1223  \n",
            "Epoch 74/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1130  \n",
            "Epoch 75/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1234 \n",
            "Epoch 76/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1084 \n",
            "Epoch 77/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1168 \n",
            "Epoch 78/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1255 \n",
            "Epoch 79/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1183 \n",
            "Epoch 80/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0979 \n",
            "Epoch 81/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1076 \n",
            "Epoch 82/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1012 \n",
            "Epoch 83/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0993 \n",
            "Epoch 84/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0913 \n",
            "Epoch 85/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0983  \n",
            "Epoch 86/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0867 \n",
            "Epoch 87/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0793 \n",
            "Epoch 88/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0789 \n",
            "Epoch 89/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0775 \n",
            "Epoch 90/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0779 \n",
            "Epoch 91/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0735 \n",
            "Epoch 92/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0718 \n",
            "Epoch 93/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0818 \n",
            "Epoch 94/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0744 \n",
            "Epoch 95/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0711 \n",
            "Epoch 96/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0677  \n",
            "Epoch 97/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 \n",
            "Epoch 98/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0650 \n",
            "Epoch 99/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0600  \n",
            "Epoch 100/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0639  \n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on Full Dataset:\n",
            "MSE: 0.3200016982884289\n",
            "RMSE: 0.565686926036327\n",
            "MAE: 0.4368241270306784\n",
            "R^2: 0.8414652132824783\n",
            "MAPE: 39.6147447786252 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam, Adamax, Nadam, Ftrl\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
        "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
        "\n",
        "# Split the combined dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model using mean squared error loss\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.01, epsilon= 1e-8), metrics=['mean_absolute_error'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=500, batch_size=50, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test Loss (MSE): {loss}\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate additional metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100\n",
        "\n",
        "# Print metrics\n",
        "print(f\"MSE : {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MAE : {mae}\")\n",
        "print(f\"R2  : {r2}\")\n",
        "print(f\"MAPE: {mape}%\")\n",
        "\n",
        "model.save('no_opt_mlp_model.h5')\n",
        "print(\"Model saved to trained_mlp_model(ADAM).h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Pzg94No80Q",
        "outputId": "a8392833-5639-4bde-c550-0e28d0f71dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 4.1601 - mean_absolute_error: 1.6247 - val_loss: 0.6691 - val_mean_absolute_error: 0.6336\n",
            "Epoch 2/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.0282 - mean_absolute_error: 0.7886 - val_loss: 0.7463 - val_mean_absolute_error: 0.6629\n",
            "Epoch 3/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7436 - mean_absolute_error: 0.6312 - val_loss: 0.6881 - val_mean_absolute_error: 0.6370\n",
            "Epoch 4/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5341 - mean_absolute_error: 0.5187 - val_loss: 0.6852 - val_mean_absolute_error: 0.6462\n",
            "Epoch 5/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4661 - mean_absolute_error: 0.5132 - val_loss: 0.5158 - val_mean_absolute_error: 0.5417\n",
            "Epoch 6/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3976 - mean_absolute_error: 0.4437 - val_loss: 0.4153 - val_mean_absolute_error: 0.4823\n",
            "Epoch 7/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3132 - mean_absolute_error: 0.3916 - val_loss: 0.4676 - val_mean_absolute_error: 0.5219\n",
            "Epoch 8/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2724 - mean_absolute_error: 0.3772 - val_loss: 0.3275 - val_mean_absolute_error: 0.4198\n",
            "Epoch 9/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2203 - mean_absolute_error: 0.3376 - val_loss: 0.2660 - val_mean_absolute_error: 0.3892\n",
            "Epoch 10/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1815 - mean_absolute_error: 0.3070 - val_loss: 0.2525 - val_mean_absolute_error: 0.3811\n",
            "Epoch 11/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1523 - mean_absolute_error: 0.2859 - val_loss: 0.2305 - val_mean_absolute_error: 0.3569\n",
            "Epoch 12/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1434 - mean_absolute_error: 0.2675 - val_loss: 0.2561 - val_mean_absolute_error: 0.3697\n",
            "Epoch 13/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1136 - mean_absolute_error: 0.2510 - val_loss: 0.2631 - val_mean_absolute_error: 0.3710\n",
            "Epoch 14/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0967 - mean_absolute_error: 0.2279 - val_loss: 0.2730 - val_mean_absolute_error: 0.3663\n",
            "Epoch 15/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0905 - mean_absolute_error: 0.2290 - val_loss: 0.2848 - val_mean_absolute_error: 0.3734\n",
            "Epoch 16/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0929 - mean_absolute_error: 0.2292 - val_loss: 0.2750 - val_mean_absolute_error: 0.3817\n",
            "Epoch 17/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0737 - mean_absolute_error: 0.2020 - val_loss: 0.2349 - val_mean_absolute_error: 0.3394\n",
            "Epoch 18/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0672 - mean_absolute_error: 0.1905 - val_loss: 0.3379 - val_mean_absolute_error: 0.3964\n",
            "Epoch 19/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0574 - mean_absolute_error: 0.1833 - val_loss: 0.2913 - val_mean_absolute_error: 0.3675\n",
            "Epoch 20/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0558 - mean_absolute_error: 0.1813 - val_loss: 0.3074 - val_mean_absolute_error: 0.3924\n",
            "Epoch 21/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0662 - mean_absolute_error: 0.1897 - val_loss: 0.2674 - val_mean_absolute_error: 0.3764\n",
            "Epoch 22/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0484 - mean_absolute_error: 0.1679 - val_loss: 0.2755 - val_mean_absolute_error: 0.3692\n",
            "Epoch 23/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0545 - mean_absolute_error: 0.1837 - val_loss: 0.2652 - val_mean_absolute_error: 0.3565\n",
            "Epoch 24/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0393 - mean_absolute_error: 0.1520 - val_loss: 0.2745 - val_mean_absolute_error: 0.3933\n",
            "Epoch 25/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0369 - mean_absolute_error: 0.1444 - val_loss: 0.2586 - val_mean_absolute_error: 0.3362\n",
            "Epoch 26/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0331 - mean_absolute_error: 0.1377 - val_loss: 0.2488 - val_mean_absolute_error: 0.3685\n",
            "Epoch 27/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0395 - mean_absolute_error: 0.1535 - val_loss: 0.2524 - val_mean_absolute_error: 0.3349\n",
            "Epoch 28/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0404 - mean_absolute_error: 0.1533 - val_loss: 0.2817 - val_mean_absolute_error: 0.3873\n",
            "Epoch 29/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0477 - mean_absolute_error: 0.1654 - val_loss: 0.2597 - val_mean_absolute_error: 0.3565\n",
            "Epoch 30/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0660 - mean_absolute_error: 0.1965 - val_loss: 0.3341 - val_mean_absolute_error: 0.4257\n",
            "Epoch 31/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0662 - mean_absolute_error: 0.2071 - val_loss: 0.2282 - val_mean_absolute_error: 0.3457\n",
            "Epoch 32/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0737 - mean_absolute_error: 0.2108 - val_loss: 0.2902 - val_mean_absolute_error: 0.3995\n",
            "Epoch 33/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0622 - mean_absolute_error: 0.1773 - val_loss: 0.2568 - val_mean_absolute_error: 0.3501\n",
            "Epoch 34/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0535 - mean_absolute_error: 0.1766 - val_loss: 0.2672 - val_mean_absolute_error: 0.3747\n",
            "Epoch 35/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0406 - mean_absolute_error: 0.1484 - val_loss: 0.2347 - val_mean_absolute_error: 0.3369\n",
            "Epoch 36/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0383 - mean_absolute_error: 0.1495 - val_loss: 0.2461 - val_mean_absolute_error: 0.3622\n",
            "Epoch 37/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0542 - mean_absolute_error: 0.1866 - val_loss: 0.2469 - val_mean_absolute_error: 0.3426\n",
            "Epoch 38/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0457 - mean_absolute_error: 0.1655 - val_loss: 0.2394 - val_mean_absolute_error: 0.3476\n",
            "Epoch 39/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0419 - mean_absolute_error: 0.1582 - val_loss: 0.2476 - val_mean_absolute_error: 0.3453\n",
            "Epoch 40/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0370 - mean_absolute_error: 0.1474 - val_loss: 0.2224 - val_mean_absolute_error: 0.3460\n",
            "Epoch 41/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0337 - mean_absolute_error: 0.1429 - val_loss: 0.2471 - val_mean_absolute_error: 0.3540\n",
            "Epoch 42/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0328 - mean_absolute_error: 0.1434 - val_loss: 0.2844 - val_mean_absolute_error: 0.3754\n",
            "Epoch 43/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0408 - mean_absolute_error: 0.1610 - val_loss: 0.3193 - val_mean_absolute_error: 0.3990\n",
            "Epoch 44/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0552 - mean_absolute_error: 0.1887 - val_loss: 0.2103 - val_mean_absolute_error: 0.3307\n",
            "Epoch 45/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0370 - mean_absolute_error: 0.1497 - val_loss: 0.2281 - val_mean_absolute_error: 0.3283\n",
            "Epoch 46/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0304 - mean_absolute_error: 0.1382 - val_loss: 0.2098 - val_mean_absolute_error: 0.3150\n",
            "Epoch 47/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0283 - mean_absolute_error: 0.1320 - val_loss: 0.2326 - val_mean_absolute_error: 0.3209\n",
            "Epoch 48/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0256 - mean_absolute_error: 0.1231 - val_loss: 0.2210 - val_mean_absolute_error: 0.3318\n",
            "Epoch 49/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0190 - mean_absolute_error: 0.0973 - val_loss: 0.2312 - val_mean_absolute_error: 0.3191\n",
            "Epoch 50/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0178 - mean_absolute_error: 0.1021 - val_loss: 0.2236 - val_mean_absolute_error: 0.3177\n",
            "Epoch 51/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0134 - mean_absolute_error: 0.0841 - val_loss: 0.2750 - val_mean_absolute_error: 0.3561\n",
            "Epoch 52/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0149 - mean_absolute_error: 0.0879 - val_loss: 0.2300 - val_mean_absolute_error: 0.3196\n",
            "Epoch 53/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0156 - mean_absolute_error: 0.0931 - val_loss: 0.2542 - val_mean_absolute_error: 0.3389\n",
            "Epoch 54/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.2084 - val_mean_absolute_error: 0.3081\n",
            "Epoch 55/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0150 - mean_absolute_error: 0.0853 - val_loss: 0.2245 - val_mean_absolute_error: 0.3147\n",
            "Epoch 56/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0109 - mean_absolute_error: 0.0738 - val_loss: 0.2137 - val_mean_absolute_error: 0.3169\n",
            "Epoch 57/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 - mean_absolute_error: 0.0621 - val_loss: 0.2446 - val_mean_absolute_error: 0.3348\n",
            "Epoch 58/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0110 - mean_absolute_error: 0.0730 - val_loss: 0.2346 - val_mean_absolute_error: 0.3464\n",
            "Epoch 59/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0143 - mean_absolute_error: 0.0916 - val_loss: 0.2475 - val_mean_absolute_error: 0.3365\n",
            "Epoch 60/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0187 - mean_absolute_error: 0.1050 - val_loss: 0.2352 - val_mean_absolute_error: 0.3534\n",
            "Epoch 61/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0209 - mean_absolute_error: 0.1087 - val_loss: 0.2137 - val_mean_absolute_error: 0.3160\n",
            "Epoch 62/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0189 - mean_absolute_error: 0.1042 - val_loss: 0.2238 - val_mean_absolute_error: 0.3188\n",
            "Epoch 63/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0153 - mean_absolute_error: 0.0860 - val_loss: 0.2201 - val_mean_absolute_error: 0.3167\n",
            "Epoch 64/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0142 - mean_absolute_error: 0.0908 - val_loss: 0.2048 - val_mean_absolute_error: 0.3081\n",
            "Epoch 65/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0171 - mean_absolute_error: 0.0982 - val_loss: 0.2338 - val_mean_absolute_error: 0.3334\n",
            "Epoch 66/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0257 - mean_absolute_error: 0.1218 - val_loss: 0.2159 - val_mean_absolute_error: 0.3034\n",
            "Epoch 67/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0344 - mean_absolute_error: 0.1420 - val_loss: 0.2420 - val_mean_absolute_error: 0.3234\n",
            "Epoch 68/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0247 - mean_absolute_error: 0.1203 - val_loss: 0.2779 - val_mean_absolute_error: 0.3629\n",
            "Epoch 69/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0272 - mean_absolute_error: 0.1345 - val_loss: 0.2143 - val_mean_absolute_error: 0.3330\n",
            "Epoch 70/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0317 - mean_absolute_error: 0.1392 - val_loss: 0.2830 - val_mean_absolute_error: 0.3882\n",
            "Epoch 71/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0327 - mean_absolute_error: 0.1411 - val_loss: 0.2550 - val_mean_absolute_error: 0.3484\n",
            "Epoch 72/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0283 - mean_absolute_error: 0.1273 - val_loss: 0.2370 - val_mean_absolute_error: 0.3199\n",
            "Epoch 73/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0233 - mean_absolute_error: 0.1126 - val_loss: 0.2315 - val_mean_absolute_error: 0.3293\n",
            "Epoch 74/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0261 - mean_absolute_error: 0.1257 - val_loss: 0.2128 - val_mean_absolute_error: 0.3099\n",
            "Epoch 75/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0156 - mean_absolute_error: 0.0947 - val_loss: 0.2185 - val_mean_absolute_error: 0.3086\n",
            "Epoch 76/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0120 - mean_absolute_error: 0.0817 - val_loss: 0.2229 - val_mean_absolute_error: 0.3201\n",
            "Epoch 77/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0125 - mean_absolute_error: 0.0829 - val_loss: 0.2179 - val_mean_absolute_error: 0.3109\n",
            "Epoch 78/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0122 - mean_absolute_error: 0.0790 - val_loss: 0.2200 - val_mean_absolute_error: 0.3205\n",
            "Epoch 79/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0126 - mean_absolute_error: 0.0825 - val_loss: 0.2354 - val_mean_absolute_error: 0.3242\n",
            "Epoch 80/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0126 - mean_absolute_error: 0.0811 - val_loss: 0.2164 - val_mean_absolute_error: 0.3060\n",
            "Epoch 81/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0099 - mean_absolute_error: 0.0676 - val_loss: 0.2290 - val_mean_absolute_error: 0.3315\n",
            "Epoch 82/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0121 - mean_absolute_error: 0.0736 - val_loss: 0.2064 - val_mean_absolute_error: 0.3151\n",
            "Epoch 83/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0095 - mean_absolute_error: 0.0705 - val_loss: 0.2179 - val_mean_absolute_error: 0.3093\n",
            "Epoch 84/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0089 - mean_absolute_error: 0.0640 - val_loss: 0.2167 - val_mean_absolute_error: 0.3197\n",
            "Epoch 85/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0106 - mean_absolute_error: 0.0778 - val_loss: 0.2293 - val_mean_absolute_error: 0.3266\n",
            "Epoch 86/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0163 - mean_absolute_error: 0.0955 - val_loss: 0.1900 - val_mean_absolute_error: 0.3036\n",
            "Epoch 87/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0137 - mean_absolute_error: 0.0912 - val_loss: 0.2517 - val_mean_absolute_error: 0.3333\n",
            "Epoch 88/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0132 - mean_absolute_error: 0.0834 - val_loss: 0.2133 - val_mean_absolute_error: 0.3075\n",
            "Epoch 89/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0086 - mean_absolute_error: 0.0690 - val_loss: 0.2406 - val_mean_absolute_error: 0.3449\n",
            "Epoch 90/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0125 - mean_absolute_error: 0.0914 - val_loss: 0.2198 - val_mean_absolute_error: 0.3266\n",
            "Epoch 91/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0160 - mean_absolute_error: 0.0988 - val_loss: 0.2390 - val_mean_absolute_error: 0.3284\n",
            "Epoch 92/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0162 - mean_absolute_error: 0.0962 - val_loss: 0.2173 - val_mean_absolute_error: 0.3153\n",
            "Epoch 93/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0116 - mean_absolute_error: 0.0759 - val_loss: 0.2250 - val_mean_absolute_error: 0.3161\n",
            "Epoch 94/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0101 - mean_absolute_error: 0.0701 - val_loss: 0.2449 - val_mean_absolute_error: 0.3387\n",
            "Epoch 95/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0085 - mean_absolute_error: 0.0672 - val_loss: 0.2243 - val_mean_absolute_error: 0.3152\n",
            "Epoch 96/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0111 - mean_absolute_error: 0.0843 - val_loss: 0.2312 - val_mean_absolute_error: 0.3273\n",
            "Epoch 97/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086 - mean_absolute_error: 0.0673 - val_loss: 0.2280 - val_mean_absolute_error: 0.3188\n",
            "Epoch 98/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086 - mean_absolute_error: 0.0681 - val_loss: 0.2516 - val_mean_absolute_error: 0.3423\n",
            "Epoch 99/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0089 - mean_absolute_error: 0.0725 - val_loss: 0.2238 - val_mean_absolute_error: 0.3260\n",
            "Epoch 100/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0098 - mean_absolute_error: 0.0759 - val_loss: 0.2350 - val_mean_absolute_error: 0.3339\n",
            "Epoch 101/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0092 - mean_absolute_error: 0.0677 - val_loss: 0.2195 - val_mean_absolute_error: 0.3205\n",
            "Epoch 102/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0113 - mean_absolute_error: 0.0797 - val_loss: 0.2469 - val_mean_absolute_error: 0.3418\n",
            "Epoch 103/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0100 - mean_absolute_error: 0.0746 - val_loss: 0.2249 - val_mean_absolute_error: 0.3259\n",
            "Epoch 104/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0076 - mean_absolute_error: 0.0617 - val_loss: 0.2124 - val_mean_absolute_error: 0.3057\n",
            "Epoch 105/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0096 - mean_absolute_error: 0.0709 - val_loss: 0.2362 - val_mean_absolute_error: 0.3234\n",
            "Epoch 106/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0086 - mean_absolute_error: 0.0680 - val_loss: 0.2236 - val_mean_absolute_error: 0.3204\n",
            "Epoch 107/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0068 - mean_absolute_error: 0.0570 - val_loss: 0.2127 - val_mean_absolute_error: 0.3079\n",
            "Epoch 108/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0095 - mean_absolute_error: 0.0730 - val_loss: 0.2177 - val_mean_absolute_error: 0.3230\n",
            "Epoch 109/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0105 - mean_absolute_error: 0.0735 - val_loss: 0.2266 - val_mean_absolute_error: 0.3229\n",
            "Epoch 110/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0109 - mean_absolute_error: 0.0794 - val_loss: 0.2245 - val_mean_absolute_error: 0.3205\n",
            "Epoch 111/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0099 - mean_absolute_error: 0.0739 - val_loss: 0.2414 - val_mean_absolute_error: 0.3365\n",
            "Epoch 112/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0136 - mean_absolute_error: 0.0878 - val_loss: 0.2175 - val_mean_absolute_error: 0.3186\n",
            "Epoch 113/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0130 - mean_absolute_error: 0.0869 - val_loss: 0.2284 - val_mean_absolute_error: 0.3393\n",
            "Epoch 114/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0179 - mean_absolute_error: 0.1012 - val_loss: 0.2376 - val_mean_absolute_error: 0.3154\n",
            "Epoch 115/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0241 - mean_absolute_error: 0.1258 - val_loss: 0.2129 - val_mean_absolute_error: 0.3119\n",
            "Epoch 116/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0201 - mean_absolute_error: 0.1105 - val_loss: 0.2564 - val_mean_absolute_error: 0.3518\n",
            "Epoch 117/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0309 - mean_absolute_error: 0.1397 - val_loss: 0.2284 - val_mean_absolute_error: 0.3222\n",
            "Epoch 118/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0181 - mean_absolute_error: 0.1008 - val_loss: 0.2235 - val_mean_absolute_error: 0.3303\n",
            "Epoch 119/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0169 - mean_absolute_error: 0.1013 - val_loss: 0.1991 - val_mean_absolute_error: 0.2950\n",
            "Epoch 120/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0108 - mean_absolute_error: 0.0823 - val_loss: 0.2042 - val_mean_absolute_error: 0.3030\n",
            "Epoch 121/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0097 - mean_absolute_error: 0.0739 - val_loss: 0.2023 - val_mean_absolute_error: 0.3028\n",
            "Epoch 122/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0085 - mean_absolute_error: 0.0681 - val_loss: 0.2104 - val_mean_absolute_error: 0.3185\n",
            "Epoch 123/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0118 - mean_absolute_error: 0.0828 - val_loss: 0.2264 - val_mean_absolute_error: 0.3119\n",
            "Epoch 124/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0074 - mean_absolute_error: 0.0635 - val_loss: 0.2352 - val_mean_absolute_error: 0.3338\n",
            "Epoch 125/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0060 - mean_absolute_error: 0.0578 - val_loss: 0.2106 - val_mean_absolute_error: 0.3097\n",
            "Epoch 126/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0061 - mean_absolute_error: 0.0545 - val_loss: 0.2118 - val_mean_absolute_error: 0.3174\n",
            "Epoch 127/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - mean_absolute_error: 0.0508 - val_loss: 0.2169 - val_mean_absolute_error: 0.3114\n",
            "Epoch 128/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0062 - mean_absolute_error: 0.0509 - val_loss: 0.2080 - val_mean_absolute_error: 0.3050\n",
            "Epoch 129/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0071 - mean_absolute_error: 0.0644 - val_loss: 0.2145 - val_mean_absolute_error: 0.3126\n",
            "Epoch 130/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0087 - mean_absolute_error: 0.0688 - val_loss: 0.2082 - val_mean_absolute_error: 0.3080\n",
            "Epoch 131/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0053 - mean_absolute_error: 0.0519 - val_loss: 0.2283 - val_mean_absolute_error: 0.3276\n",
            "Epoch 132/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0553 - val_loss: 0.2161 - val_mean_absolute_error: 0.3064\n",
            "Epoch 133/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0047 - mean_absolute_error: 0.0485 - val_loss: 0.2240 - val_mean_absolute_error: 0.3201\n",
            "Epoch 134/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0068 - mean_absolute_error: 0.0590 - val_loss: 0.2081 - val_mean_absolute_error: 0.3071\n",
            "Epoch 135/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0049 - mean_absolute_error: 0.0531 - val_loss: 0.1997 - val_mean_absolute_error: 0.3067\n",
            "Epoch 136/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - mean_absolute_error: 0.0523 - val_loss: 0.2132 - val_mean_absolute_error: 0.3085\n",
            "Epoch 137/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0070 - mean_absolute_error: 0.0577 - val_loss: 0.2171 - val_mean_absolute_error: 0.3213\n",
            "Epoch 138/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058 - mean_absolute_error: 0.0542 - val_loss: 0.2229 - val_mean_absolute_error: 0.3148\n",
            "Epoch 139/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0080 - mean_absolute_error: 0.0608 - val_loss: 0.2126 - val_mean_absolute_error: 0.3138\n",
            "Epoch 140/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0063 - mean_absolute_error: 0.0567 - val_loss: 0.2116 - val_mean_absolute_error: 0.3076\n",
            "Epoch 141/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0067 - mean_absolute_error: 0.0577 - val_loss: 0.2252 - val_mean_absolute_error: 0.3246\n",
            "Epoch 142/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0049 - mean_absolute_error: 0.0485 - val_loss: 0.2138 - val_mean_absolute_error: 0.3004\n",
            "Epoch 143/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0055 - mean_absolute_error: 0.0515 - val_loss: 0.2152 - val_mean_absolute_error: 0.3114\n",
            "Epoch 144/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0059 - mean_absolute_error: 0.0556 - val_loss: 0.2277 - val_mean_absolute_error: 0.3296\n",
            "Epoch 145/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0086 - mean_absolute_error: 0.0687 - val_loss: 0.2059 - val_mean_absolute_error: 0.3092\n",
            "Epoch 146/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0087 - mean_absolute_error: 0.0705 - val_loss: 0.2239 - val_mean_absolute_error: 0.3231\n",
            "Epoch 147/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0094 - mean_absolute_error: 0.0672 - val_loss: 0.2128 - val_mean_absolute_error: 0.3198\n",
            "Epoch 148/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0162 - mean_absolute_error: 0.0993 - val_loss: 0.2316 - val_mean_absolute_error: 0.3360\n",
            "Epoch 149/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0149 - mean_absolute_error: 0.0880 - val_loss: 0.2033 - val_mean_absolute_error: 0.3012\n",
            "Epoch 150/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0084 - mean_absolute_error: 0.0700 - val_loss: 0.2259 - val_mean_absolute_error: 0.3370\n",
            "Epoch 151/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0091 - mean_absolute_error: 0.0672 - val_loss: 0.2281 - val_mean_absolute_error: 0.3173\n",
            "Epoch 152/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098 - mean_absolute_error: 0.0751 - val_loss: 0.2296 - val_mean_absolute_error: 0.3346\n",
            "Epoch 153/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0094 - mean_absolute_error: 0.0770 - val_loss: 0.1994 - val_mean_absolute_error: 0.2916\n",
            "Epoch 154/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0113 - mean_absolute_error: 0.0828 - val_loss: 0.2174 - val_mean_absolute_error: 0.3249\n",
            "Epoch 155/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0100 - mean_absolute_error: 0.0803 - val_loss: 0.2161 - val_mean_absolute_error: 0.3121\n",
            "Epoch 156/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0100 - mean_absolute_error: 0.0785 - val_loss: 0.2149 - val_mean_absolute_error: 0.3157\n",
            "Epoch 157/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0112 - mean_absolute_error: 0.0797 - val_loss: 0.2199 - val_mean_absolute_error: 0.3168\n",
            "Epoch 158/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0085 - mean_absolute_error: 0.0679 - val_loss: 0.2092 - val_mean_absolute_error: 0.3110\n",
            "Epoch 159/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0130 - mean_absolute_error: 0.0843 - val_loss: 0.2340 - val_mean_absolute_error: 0.3179\n",
            "Epoch 160/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - mean_absolute_error: 0.0640 - val_loss: 0.2002 - val_mean_absolute_error: 0.3061\n",
            "Epoch 161/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0109 - mean_absolute_error: 0.0714 - val_loss: 0.2123 - val_mean_absolute_error: 0.2995\n",
            "Epoch 162/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074 - mean_absolute_error: 0.0556 - val_loss: 0.2041 - val_mean_absolute_error: 0.3057\n",
            "Epoch 163/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0597 - val_loss: 0.2077 - val_mean_absolute_error: 0.3030\n",
            "Epoch 164/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0049 - mean_absolute_error: 0.0509 - val_loss: 0.2080 - val_mean_absolute_error: 0.2993\n",
            "Epoch 165/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - mean_absolute_error: 0.0497 - val_loss: 0.2096 - val_mean_absolute_error: 0.2979\n",
            "Epoch 166/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0044 - mean_absolute_error: 0.0441 - val_loss: 0.2134 - val_mean_absolute_error: 0.3094\n",
            "Epoch 167/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - mean_absolute_error: 0.0403 - val_loss: 0.2058 - val_mean_absolute_error: 0.3059\n",
            "Epoch 168/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0063 - mean_absolute_error: 0.0490 - val_loss: 0.2036 - val_mean_absolute_error: 0.3018\n",
            "Epoch 169/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - mean_absolute_error: 0.0494 - val_loss: 0.2234 - val_mean_absolute_error: 0.3185\n",
            "Epoch 170/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0070 - mean_absolute_error: 0.0569 - val_loss: 0.2072 - val_mean_absolute_error: 0.2966\n",
            "Epoch 171/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - mean_absolute_error: 0.0436 - val_loss: 0.2105 - val_mean_absolute_error: 0.3017\n",
            "Epoch 172/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0039 - mean_absolute_error: 0.0407 - val_loss: 0.2083 - val_mean_absolute_error: 0.3083\n",
            "Epoch 173/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0076 - mean_absolute_error: 0.0662 - val_loss: 0.2068 - val_mean_absolute_error: 0.3024\n",
            "Epoch 174/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0062 - mean_absolute_error: 0.0542 - val_loss: 0.2323 - val_mean_absolute_error: 0.3313\n",
            "Epoch 175/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - mean_absolute_error: 0.0606 - val_loss: 0.2144 - val_mean_absolute_error: 0.3066\n",
            "Epoch 176/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0077 - mean_absolute_error: 0.0656 - val_loss: 0.2168 - val_mean_absolute_error: 0.3154\n",
            "Epoch 177/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0065 - mean_absolute_error: 0.0611 - val_loss: 0.2140 - val_mean_absolute_error: 0.3120\n",
            "Epoch 178/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - mean_absolute_error: 0.0569 - val_loss: 0.2093 - val_mean_absolute_error: 0.3052\n",
            "Epoch 179/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0597 - val_loss: 0.2175 - val_mean_absolute_error: 0.3184\n",
            "Epoch 180/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - mean_absolute_error: 0.0588 - val_loss: 0.2096 - val_mean_absolute_error: 0.2983\n",
            "Epoch 181/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0046 - mean_absolute_error: 0.0481 - val_loss: 0.2311 - val_mean_absolute_error: 0.3236\n",
            "Epoch 182/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0047 - mean_absolute_error: 0.0500 - val_loss: 0.2146 - val_mean_absolute_error: 0.3082\n",
            "Epoch 183/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0062 - mean_absolute_error: 0.0544 - val_loss: 0.2145 - val_mean_absolute_error: 0.3148\n",
            "Epoch 184/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0079 - mean_absolute_error: 0.0637 - val_loss: 0.2145 - val_mean_absolute_error: 0.3090\n",
            "Epoch 185/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0045 - mean_absolute_error: 0.0461 - val_loss: 0.2041 - val_mean_absolute_error: 0.3049\n",
            "Epoch 186/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - mean_absolute_error: 0.0524 - val_loss: 0.2114 - val_mean_absolute_error: 0.3068\n",
            "Epoch 187/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0045 - mean_absolute_error: 0.0512 - val_loss: 0.2158 - val_mean_absolute_error: 0.3024\n",
            "Epoch 188/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0038 - mean_absolute_error: 0.0472 - val_loss: 0.1989 - val_mean_absolute_error: 0.2948\n",
            "Epoch 189/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0043 - mean_absolute_error: 0.0455 - val_loss: 0.2152 - val_mean_absolute_error: 0.3096\n",
            "Epoch 190/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - mean_absolute_error: 0.0390 - val_loss: 0.2016 - val_mean_absolute_error: 0.2995\n",
            "Epoch 191/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0053 - mean_absolute_error: 0.0532 - val_loss: 0.2263 - val_mean_absolute_error: 0.3156\n",
            "Epoch 192/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058 - mean_absolute_error: 0.0589 - val_loss: 0.2152 - val_mean_absolute_error: 0.3134\n",
            "Epoch 193/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0046 - mean_absolute_error: 0.0519 - val_loss: 0.2152 - val_mean_absolute_error: 0.3042\n",
            "Epoch 194/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0042 - mean_absolute_error: 0.0469 - val_loss: 0.2096 - val_mean_absolute_error: 0.2991\n",
            "Epoch 195/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0031 - mean_absolute_error: 0.0408 - val_loss: 0.2246 - val_mean_absolute_error: 0.3156\n",
            "Epoch 196/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mean_absolute_error: 0.0345 - val_loss: 0.2122 - val_mean_absolute_error: 0.3025\n",
            "Epoch 197/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0034 - mean_absolute_error: 0.0400 - val_loss: 0.2076 - val_mean_absolute_error: 0.3023\n",
            "Epoch 198/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0051 - mean_absolute_error: 0.0533 - val_loss: 0.2189 - val_mean_absolute_error: 0.3115\n",
            "Epoch 199/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - mean_absolute_error: 0.0502 - val_loss: 0.2105 - val_mean_absolute_error: 0.3028\n",
            "Epoch 200/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0055 - mean_absolute_error: 0.0559 - val_loss: 0.2104 - val_mean_absolute_error: 0.3069\n",
            "Epoch 201/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0041 - mean_absolute_error: 0.0465 - val_loss: 0.2230 - val_mean_absolute_error: 0.3162\n",
            "Epoch 202/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038 - mean_absolute_error: 0.0454 - val_loss: 0.2009 - val_mean_absolute_error: 0.2985\n",
            "Epoch 203/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0053 - mean_absolute_error: 0.0526 - val_loss: 0.2261 - val_mean_absolute_error: 0.3156\n",
            "Epoch 204/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0036 - mean_absolute_error: 0.0470 - val_loss: 0.2035 - val_mean_absolute_error: 0.2955\n",
            "Epoch 205/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0040 - mean_absolute_error: 0.0485 - val_loss: 0.2309 - val_mean_absolute_error: 0.3272\n",
            "Epoch 206/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0051 - mean_absolute_error: 0.0545 - val_loss: 0.2133 - val_mean_absolute_error: 0.3111\n",
            "Epoch 207/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - mean_absolute_error: 0.0512 - val_loss: 0.2128 - val_mean_absolute_error: 0.3092\n",
            "Epoch 208/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0062 - mean_absolute_error: 0.0557 - val_loss: 0.2245 - val_mean_absolute_error: 0.3191\n",
            "Epoch 209/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0107 - mean_absolute_error: 0.0761 - val_loss: 0.2034 - val_mean_absolute_error: 0.3086\n",
            "Epoch 210/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0134 - mean_absolute_error: 0.0851 - val_loss: 0.2430 - val_mean_absolute_error: 0.3313\n",
            "Epoch 211/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0090 - mean_absolute_error: 0.0716 - val_loss: 0.2182 - val_mean_absolute_error: 0.3193\n",
            "Epoch 212/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0145 - mean_absolute_error: 0.0929 - val_loss: 0.2143 - val_mean_absolute_error: 0.3141\n",
            "Epoch 213/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0089 - mean_absolute_error: 0.0700 - val_loss: 0.2109 - val_mean_absolute_error: 0.3026\n",
            "Epoch 214/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0059 - mean_absolute_error: 0.0529 - val_loss: 0.2025 - val_mean_absolute_error: 0.2942\n",
            "Epoch 215/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - mean_absolute_error: 0.0470 - val_loss: 0.2192 - val_mean_absolute_error: 0.3177\n",
            "Epoch 216/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0057 - mean_absolute_error: 0.0536 - val_loss: 0.2038 - val_mean_absolute_error: 0.3039\n",
            "Epoch 217/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049 - mean_absolute_error: 0.0508 - val_loss: 0.2176 - val_mean_absolute_error: 0.3128\n",
            "Epoch 218/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0044 - mean_absolute_error: 0.0468 - val_loss: 0.2049 - val_mean_absolute_error: 0.2970\n",
            "Epoch 219/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - mean_absolute_error: 0.0404 - val_loss: 0.2066 - val_mean_absolute_error: 0.3041\n",
            "Epoch 220/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0030 - mean_absolute_error: 0.0400 - val_loss: 0.2074 - val_mean_absolute_error: 0.3022\n",
            "Epoch 221/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030 - mean_absolute_error: 0.0378 - val_loss: 0.1898 - val_mean_absolute_error: 0.2838\n",
            "Epoch 222/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0032 - mean_absolute_error: 0.0385 - val_loss: 0.2243 - val_mean_absolute_error: 0.3164\n",
            "Epoch 223/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0043 - mean_absolute_error: 0.0488 - val_loss: 0.1923 - val_mean_absolute_error: 0.2898\n",
            "Epoch 224/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0068 - mean_absolute_error: 0.0589 - val_loss: 0.2182 - val_mean_absolute_error: 0.3142\n",
            "Epoch 225/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0096 - mean_absolute_error: 0.0778 - val_loss: 0.2382 - val_mean_absolute_error: 0.3375\n",
            "Epoch 226/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0118 - mean_absolute_error: 0.0927 - val_loss: 0.1967 - val_mean_absolute_error: 0.2919\n",
            "Epoch 227/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0093 - mean_absolute_error: 0.0752 - val_loss: 0.2091 - val_mean_absolute_error: 0.3181\n",
            "Epoch 228/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0094 - mean_absolute_error: 0.0725 - val_loss: 0.1973 - val_mean_absolute_error: 0.2988\n",
            "Epoch 229/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0086 - mean_absolute_error: 0.0661 - val_loss: 0.2108 - val_mean_absolute_error: 0.3038\n",
            "Epoch 230/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0091 - mean_absolute_error: 0.0662 - val_loss: 0.2412 - val_mean_absolute_error: 0.3233\n",
            "Epoch 231/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0095 - mean_absolute_error: 0.0700 - val_loss: 0.1959 - val_mean_absolute_error: 0.2935\n",
            "Epoch 232/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0099 - mean_absolute_error: 0.0759 - val_loss: 0.2126 - val_mean_absolute_error: 0.3118\n",
            "Epoch 233/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0080 - mean_absolute_error: 0.0718 - val_loss: 0.1970 - val_mean_absolute_error: 0.2969\n",
            "Epoch 234/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0057 - mean_absolute_error: 0.0592 - val_loss: 0.2063 - val_mean_absolute_error: 0.2985\n",
            "Epoch 235/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061 - mean_absolute_error: 0.0579 - val_loss: 0.2086 - val_mean_absolute_error: 0.2962\n",
            "Epoch 236/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0067 - mean_absolute_error: 0.0603 - val_loss: 0.2080 - val_mean_absolute_error: 0.3085\n",
            "Epoch 237/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0064 - mean_absolute_error: 0.0602 - val_loss: 0.2132 - val_mean_absolute_error: 0.3167\n",
            "Epoch 238/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0068 - mean_absolute_error: 0.0618 - val_loss: 0.2001 - val_mean_absolute_error: 0.3076\n",
            "Epoch 239/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0075 - mean_absolute_error: 0.0691 - val_loss: 0.2199 - val_mean_absolute_error: 0.3161\n",
            "Epoch 240/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0064 - mean_absolute_error: 0.0614 - val_loss: 0.2082 - val_mean_absolute_error: 0.3076\n",
            "Epoch 241/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0075 - mean_absolute_error: 0.0674 - val_loss: 0.2165 - val_mean_absolute_error: 0.3092\n",
            "Epoch 242/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0095 - mean_absolute_error: 0.0735 - val_loss: 0.2124 - val_mean_absolute_error: 0.3119\n",
            "Epoch 243/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0087 - mean_absolute_error: 0.0748 - val_loss: 0.2134 - val_mean_absolute_error: 0.3053\n",
            "Epoch 244/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0089 - mean_absolute_error: 0.0709 - val_loss: 0.2158 - val_mean_absolute_error: 0.3161\n",
            "Epoch 245/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0119 - mean_absolute_error: 0.0874 - val_loss: 0.2035 - val_mean_absolute_error: 0.2953\n",
            "Epoch 246/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0117 - mean_absolute_error: 0.0816 - val_loss: 0.2187 - val_mean_absolute_error: 0.3076\n",
            "Epoch 247/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0095 - mean_absolute_error: 0.0771 - val_loss: 0.2147 - val_mean_absolute_error: 0.3038\n",
            "Epoch 248/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0076 - mean_absolute_error: 0.0621 - val_loss: 0.2297 - val_mean_absolute_error: 0.3190\n",
            "Epoch 249/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0085 - mean_absolute_error: 0.0762 - val_loss: 0.2086 - val_mean_absolute_error: 0.2962\n",
            "Epoch 250/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0065 - mean_absolute_error: 0.0620 - val_loss: 0.2107 - val_mean_absolute_error: 0.3069\n",
            "Epoch 251/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0078 - mean_absolute_error: 0.0744 - val_loss: 0.2158 - val_mean_absolute_error: 0.3094\n",
            "Epoch 252/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057 - mean_absolute_error: 0.0586 - val_loss: 0.1943 - val_mean_absolute_error: 0.2943\n",
            "Epoch 253/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0042 - mean_absolute_error: 0.0469 - val_loss: 0.2279 - val_mean_absolute_error: 0.3175\n",
            "Epoch 254/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0041 - mean_absolute_error: 0.0456 - val_loss: 0.2143 - val_mean_absolute_error: 0.3118\n",
            "Epoch 255/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030 - mean_absolute_error: 0.0396 - val_loss: 0.2106 - val_mean_absolute_error: 0.3058\n",
            "Epoch 256/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0039 - mean_absolute_error: 0.0427 - val_loss: 0.2081 - val_mean_absolute_error: 0.2939\n",
            "Epoch 257/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - mean_absolute_error: 0.0362 - val_loss: 0.2117 - val_mean_absolute_error: 0.3060\n",
            "Epoch 258/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - mean_absolute_error: 0.0368 - val_loss: 0.2149 - val_mean_absolute_error: 0.3058\n",
            "Epoch 259/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0024 - mean_absolute_error: 0.0333 - val_loss: 0.2060 - val_mean_absolute_error: 0.2990\n",
            "Epoch 260/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - mean_absolute_error: 0.0323 - val_loss: 0.2209 - val_mean_absolute_error: 0.3119\n",
            "Epoch 261/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030 - mean_absolute_error: 0.0444 - val_loss: 0.2225 - val_mean_absolute_error: 0.3154\n",
            "Epoch 262/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - mean_absolute_error: 0.0374 - val_loss: 0.2036 - val_mean_absolute_error: 0.2958\n",
            "Epoch 263/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - mean_absolute_error: 0.0348 - val_loss: 0.2045 - val_mean_absolute_error: 0.3026\n",
            "Epoch 264/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - mean_absolute_error: 0.0296 - val_loss: 0.2161 - val_mean_absolute_error: 0.3064\n",
            "Epoch 265/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0022 - mean_absolute_error: 0.0329 - val_loss: 0.2105 - val_mean_absolute_error: 0.3023\n",
            "Epoch 266/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - mean_absolute_error: 0.0410 - val_loss: 0.2037 - val_mean_absolute_error: 0.2977\n",
            "Epoch 267/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0026 - mean_absolute_error: 0.0378 - val_loss: 0.2172 - val_mean_absolute_error: 0.3085\n",
            "Epoch 268/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - mean_absolute_error: 0.0282 - val_loss: 0.2036 - val_mean_absolute_error: 0.2956\n",
            "Epoch 269/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0022 - mean_absolute_error: 0.0346 - val_loss: 0.2206 - val_mean_absolute_error: 0.3119\n",
            "Epoch 270/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0032 - mean_absolute_error: 0.0408 - val_loss: 0.2106 - val_mean_absolute_error: 0.3072\n",
            "Epoch 271/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0033 - mean_absolute_error: 0.0411 - val_loss: 0.2140 - val_mean_absolute_error: 0.3075\n",
            "Epoch 272/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0055 - mean_absolute_error: 0.0576 - val_loss: 0.2175 - val_mean_absolute_error: 0.3096\n",
            "Epoch 273/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - mean_absolute_error: 0.0422 - val_loss: 0.2001 - val_mean_absolute_error: 0.2930\n",
            "Epoch 274/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.2068 - val_mean_absolute_error: 0.3062\n",
            "Epoch 275/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - mean_absolute_error: 0.0356 - val_loss: 0.2053 - val_mean_absolute_error: 0.2978\n",
            "Epoch 276/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058 - mean_absolute_error: 0.0579 - val_loss: 0.1963 - val_mean_absolute_error: 0.2904\n",
            "Epoch 277/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0076 - mean_absolute_error: 0.0667 - val_loss: 0.2272 - val_mean_absolute_error: 0.3135\n",
            "Epoch 278/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0069 - mean_absolute_error: 0.0616 - val_loss: 0.2356 - val_mean_absolute_error: 0.3268\n",
            "Epoch 279/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0070 - mean_absolute_error: 0.0663 - val_loss: 0.1899 - val_mean_absolute_error: 0.2988\n",
            "Epoch 280/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0089 - mean_absolute_error: 0.0739 - val_loss: 0.2195 - val_mean_absolute_error: 0.3194\n",
            "Epoch 281/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0102 - mean_absolute_error: 0.0794 - val_loss: 0.2090 - val_mean_absolute_error: 0.3007\n",
            "Epoch 282/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0117 - mean_absolute_error: 0.0879 - val_loss: 0.2054 - val_mean_absolute_error: 0.3014\n",
            "Epoch 283/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0140 - mean_absolute_error: 0.0929 - val_loss: 0.2561 - val_mean_absolute_error: 0.3422\n",
            "Epoch 284/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0174 - mean_absolute_error: 0.1028 - val_loss: 0.1823 - val_mean_absolute_error: 0.2914\n",
            "Epoch 285/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0177 - mean_absolute_error: 0.0997 - val_loss: 0.2308 - val_mean_absolute_error: 0.3171\n",
            "Epoch 286/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0119 - mean_absolute_error: 0.0857 - val_loss: 0.2043 - val_mean_absolute_error: 0.2977\n",
            "Epoch 287/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0147 - mean_absolute_error: 0.0877 - val_loss: 0.2238 - val_mean_absolute_error: 0.3077\n",
            "Epoch 288/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0133 - mean_absolute_error: 0.0870 - val_loss: 0.2385 - val_mean_absolute_error: 0.3159\n",
            "Epoch 289/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0104 - mean_absolute_error: 0.0778 - val_loss: 0.1998 - val_mean_absolute_error: 0.2934\n",
            "Epoch 290/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0082 - mean_absolute_error: 0.0727 - val_loss: 0.2176 - val_mean_absolute_error: 0.3064\n",
            "Epoch 291/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0074 - mean_absolute_error: 0.0668 - val_loss: 0.1956 - val_mean_absolute_error: 0.3008\n",
            "Epoch 292/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0090 - mean_absolute_error: 0.0708 - val_loss: 0.2237 - val_mean_absolute_error: 0.3098\n",
            "Epoch 293/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0129 - mean_absolute_error: 0.0880 - val_loss: 0.1897 - val_mean_absolute_error: 0.2949\n",
            "Epoch 294/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0109 - mean_absolute_error: 0.0819 - val_loss: 0.2297 - val_mean_absolute_error: 0.3292\n",
            "Epoch 295/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0094 - mean_absolute_error: 0.0764 - val_loss: 0.2033 - val_mean_absolute_error: 0.2965\n",
            "Epoch 296/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0080 - mean_absolute_error: 0.0699 - val_loss: 0.2415 - val_mean_absolute_error: 0.3237\n",
            "Epoch 297/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0090 - mean_absolute_error: 0.0751 - val_loss: 0.2006 - val_mean_absolute_error: 0.2898\n",
            "Epoch 298/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0057 - mean_absolute_error: 0.0580 - val_loss: 0.2056 - val_mean_absolute_error: 0.3019\n",
            "Epoch 299/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061 - mean_absolute_error: 0.0571 - val_loss: 0.2021 - val_mean_absolute_error: 0.2999\n",
            "Epoch 300/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0046 - mean_absolute_error: 0.0507 - val_loss: 0.1899 - val_mean_absolute_error: 0.2858\n",
            "Epoch 301/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - mean_absolute_error: 0.0393 - val_loss: 0.1968 - val_mean_absolute_error: 0.2931\n",
            "Epoch 302/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - mean_absolute_error: 0.0346 - val_loss: 0.2020 - val_mean_absolute_error: 0.2963\n",
            "Epoch 303/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014 - mean_absolute_error: 0.0265 - val_loss: 0.2055 - val_mean_absolute_error: 0.2983\n",
            "Epoch 304/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0271 - val_loss: 0.2010 - val_mean_absolute_error: 0.3029\n",
            "Epoch 305/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015 - mean_absolute_error: 0.0265 - val_loss: 0.1941 - val_mean_absolute_error: 0.2925\n",
            "Epoch 306/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0018 - mean_absolute_error: 0.0292 - val_loss: 0.2062 - val_mean_absolute_error: 0.3047\n",
            "Epoch 307/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0021 - mean_absolute_error: 0.0317 - val_loss: 0.1940 - val_mean_absolute_error: 0.2948\n",
            "Epoch 308/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0039 - mean_absolute_error: 0.0460 - val_loss: 0.2126 - val_mean_absolute_error: 0.3103\n",
            "Epoch 309/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0044 - mean_absolute_error: 0.0521 - val_loss: 0.1944 - val_mean_absolute_error: 0.2901\n",
            "Epoch 310/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029 - mean_absolute_error: 0.0408 - val_loss: 0.1977 - val_mean_absolute_error: 0.2963\n",
            "Epoch 311/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0035 - mean_absolute_error: 0.0476 - val_loss: 0.2077 - val_mean_absolute_error: 0.3028\n",
            "Epoch 312/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0027 - mean_absolute_error: 0.0408 - val_loss: 0.1912 - val_mean_absolute_error: 0.2883\n",
            "Epoch 313/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - mean_absolute_error: 0.0390 - val_loss: 0.2144 - val_mean_absolute_error: 0.3124\n",
            "Epoch 314/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021 - mean_absolute_error: 0.0318 - val_loss: 0.1960 - val_mean_absolute_error: 0.2903\n",
            "Epoch 315/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0015 - mean_absolute_error: 0.0264 - val_loss: 0.2086 - val_mean_absolute_error: 0.3026\n",
            "Epoch 316/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018 - mean_absolute_error: 0.0287 - val_loss: 0.1994 - val_mean_absolute_error: 0.2988\n",
            "Epoch 317/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0272 - val_loss: 0.2037 - val_mean_absolute_error: 0.3017\n",
            "Epoch 318/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013 - mean_absolute_error: 0.0271 - val_loss: 0.1983 - val_mean_absolute_error: 0.2964\n",
            "Epoch 319/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0014 - mean_absolute_error: 0.0281 - val_loss: 0.2100 - val_mean_absolute_error: 0.3033\n",
            "Epoch 320/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.1952 - val_mean_absolute_error: 0.2946\n",
            "Epoch 321/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0016 - mean_absolute_error: 0.0301 - val_loss: 0.1900 - val_mean_absolute_error: 0.2881\n",
            "Epoch 322/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0020 - mean_absolute_error: 0.0337 - val_loss: 0.1973 - val_mean_absolute_error: 0.2985\n",
            "Epoch 323/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014 - mean_absolute_error: 0.0281 - val_loss: 0.1980 - val_mean_absolute_error: 0.2959\n",
            "Epoch 324/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - mean_absolute_error: 0.0248 - val_loss: 0.1961 - val_mean_absolute_error: 0.2941\n",
            "Epoch 325/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - mean_absolute_error: 0.0232 - val_loss: 0.1977 - val_mean_absolute_error: 0.2968\n",
            "Epoch 326/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0011 - mean_absolute_error: 0.0206 - val_loss: 0.1950 - val_mean_absolute_error: 0.2921\n",
            "Epoch 327/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0013 - mean_absolute_error: 0.0228 - val_loss: 0.2008 - val_mean_absolute_error: 0.2994\n",
            "Epoch 328/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - mean_absolute_error: 0.0282 - val_loss: 0.1897 - val_mean_absolute_error: 0.2907\n",
            "Epoch 329/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - mean_absolute_error: 0.0354 - val_loss: 0.2068 - val_mean_absolute_error: 0.3044\n",
            "Epoch 330/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0026 - mean_absolute_error: 0.0390 - val_loss: 0.1852 - val_mean_absolute_error: 0.2840\n",
            "Epoch 331/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - mean_absolute_error: 0.0430 - val_loss: 0.2110 - val_mean_absolute_error: 0.3043\n",
            "Epoch 332/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - mean_absolute_error: 0.0455 - val_loss: 0.1962 - val_mean_absolute_error: 0.2944\n",
            "Epoch 333/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0049 - mean_absolute_error: 0.0500 - val_loss: 0.2021 - val_mean_absolute_error: 0.3038\n",
            "Epoch 334/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0061 - mean_absolute_error: 0.0599 - val_loss: 0.1958 - val_mean_absolute_error: 0.2917\n",
            "Epoch 335/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0082 - mean_absolute_error: 0.0656 - val_loss: 0.2177 - val_mean_absolute_error: 0.3196\n",
            "Epoch 336/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0057 - mean_absolute_error: 0.0609 - val_loss: 0.1862 - val_mean_absolute_error: 0.2832\n",
            "Epoch 337/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0045 - mean_absolute_error: 0.0524 - val_loss: 0.2226 - val_mean_absolute_error: 0.3229\n",
            "Epoch 338/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058 - mean_absolute_error: 0.0607 - val_loss: 0.1924 - val_mean_absolute_error: 0.2782\n",
            "Epoch 339/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0054 - mean_absolute_error: 0.0585 - val_loss: 0.1866 - val_mean_absolute_error: 0.2820\n",
            "Epoch 340/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0059 - mean_absolute_error: 0.0611 - val_loss: 0.2121 - val_mean_absolute_error: 0.2993\n",
            "Epoch 341/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0066 - mean_absolute_error: 0.0618 - val_loss: 0.1807 - val_mean_absolute_error: 0.2867\n",
            "Epoch 342/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0058 - mean_absolute_error: 0.0564 - val_loss: 0.2112 - val_mean_absolute_error: 0.3122\n",
            "Epoch 343/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073 - mean_absolute_error: 0.0615 - val_loss: 0.1791 - val_mean_absolute_error: 0.2805\n",
            "Epoch 344/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0058 - mean_absolute_error: 0.0613 - val_loss: 0.2269 - val_mean_absolute_error: 0.3295\n",
            "Epoch 345/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0049 - mean_absolute_error: 0.0548 - val_loss: 0.1957 - val_mean_absolute_error: 0.2921\n",
            "Epoch 346/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0037 - mean_absolute_error: 0.0481 - val_loss: 0.1968 - val_mean_absolute_error: 0.2963\n",
            "Epoch 347/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0027 - mean_absolute_error: 0.0411 - val_loss: 0.1910 - val_mean_absolute_error: 0.2940\n",
            "Epoch 348/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0029 - mean_absolute_error: 0.0389 - val_loss: 0.1975 - val_mean_absolute_error: 0.2988\n",
            "Epoch 349/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0024 - mean_absolute_error: 0.0375 - val_loss: 0.2084 - val_mean_absolute_error: 0.3028\n",
            "Epoch 350/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - mean_absolute_error: 0.0393 - val_loss: 0.1964 - val_mean_absolute_error: 0.2966\n",
            "Epoch 351/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0026 - mean_absolute_error: 0.0367 - val_loss: 0.2030 - val_mean_absolute_error: 0.3053\n",
            "Epoch 352/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - mean_absolute_error: 0.0358 - val_loss: 0.1966 - val_mean_absolute_error: 0.2983\n",
            "Epoch 353/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0029 - mean_absolute_error: 0.0377 - val_loss: 0.1978 - val_mean_absolute_error: 0.2938\n",
            "Epoch 354/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0033 - mean_absolute_error: 0.0422 - val_loss: 0.1852 - val_mean_absolute_error: 0.2835\n",
            "Epoch 355/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0044 - mean_absolute_error: 0.0532 - val_loss: 0.2053 - val_mean_absolute_error: 0.3109\n",
            "Epoch 356/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0034 - mean_absolute_error: 0.0438 - val_loss: 0.1940 - val_mean_absolute_error: 0.2916\n",
            "Epoch 357/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0027 - mean_absolute_error: 0.0381 - val_loss: 0.2045 - val_mean_absolute_error: 0.3051\n",
            "Epoch 358/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - mean_absolute_error: 0.0385 - val_loss: 0.1893 - val_mean_absolute_error: 0.2790\n",
            "Epoch 359/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - mean_absolute_error: 0.0402 - val_loss: 0.2029 - val_mean_absolute_error: 0.3021\n",
            "Epoch 360/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - mean_absolute_error: 0.0358 - val_loss: 0.1940 - val_mean_absolute_error: 0.2858\n",
            "Epoch 361/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028 - mean_absolute_error: 0.0386 - val_loss: 0.1952 - val_mean_absolute_error: 0.2950\n",
            "Epoch 362/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0024 - mean_absolute_error: 0.0369 - val_loss: 0.1996 - val_mean_absolute_error: 0.2985\n",
            "Epoch 363/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0032 - mean_absolute_error: 0.0424 - val_loss: 0.1873 - val_mean_absolute_error: 0.2818\n",
            "Epoch 364/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0015 - mean_absolute_error: 0.0299 - val_loss: 0.2072 - val_mean_absolute_error: 0.3020\n",
            "Epoch 365/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - mean_absolute_error: 0.0321 - val_loss: 0.2036 - val_mean_absolute_error: 0.2953\n",
            "Epoch 366/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - mean_absolute_error: 0.0321 - val_loss: 0.1869 - val_mean_absolute_error: 0.2831\n",
            "Epoch 367/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0019 - mean_absolute_error: 0.0304 - val_loss: 0.2017 - val_mean_absolute_error: 0.2955\n",
            "Epoch 368/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0030 - mean_absolute_error: 0.0437 - val_loss: 0.1926 - val_mean_absolute_error: 0.2916\n",
            "Epoch 369/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0040 - mean_absolute_error: 0.0489 - val_loss: 0.2156 - val_mean_absolute_error: 0.3027\n",
            "Epoch 370/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0037 - mean_absolute_error: 0.0492 - val_loss: 0.1952 - val_mean_absolute_error: 0.2910\n",
            "Epoch 371/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0036 - mean_absolute_error: 0.0455 - val_loss: 0.2110 - val_mean_absolute_error: 0.3020\n",
            "Epoch 372/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033 - mean_absolute_error: 0.0426 - val_loss: 0.1922 - val_mean_absolute_error: 0.2960\n",
            "Epoch 373/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0034 - mean_absolute_error: 0.0449 - val_loss: 0.1988 - val_mean_absolute_error: 0.2893\n",
            "Epoch 374/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0038 - mean_absolute_error: 0.0464 - val_loss: 0.2179 - val_mean_absolute_error: 0.3161\n",
            "Epoch 375/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0041 - mean_absolute_error: 0.0500 - val_loss: 0.1923 - val_mean_absolute_error: 0.2828\n",
            "Epoch 376/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - mean_absolute_error: 0.0512 - val_loss: 0.2121 - val_mean_absolute_error: 0.3043\n",
            "Epoch 377/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - mean_absolute_error: 0.0429 - val_loss: 0.2101 - val_mean_absolute_error: 0.3050\n",
            "Epoch 378/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0043 - mean_absolute_error: 0.0532 - val_loss: 0.2148 - val_mean_absolute_error: 0.2999\n",
            "Epoch 379/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0042 - mean_absolute_error: 0.0508 - val_loss: 0.2083 - val_mean_absolute_error: 0.3031\n",
            "Epoch 380/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0047 - mean_absolute_error: 0.0539 - val_loss: 0.1954 - val_mean_absolute_error: 0.2832\n",
            "Epoch 381/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0047 - mean_absolute_error: 0.0549 - val_loss: 0.2119 - val_mean_absolute_error: 0.3136\n",
            "Epoch 382/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031 - mean_absolute_error: 0.0453 - val_loss: 0.1992 - val_mean_absolute_error: 0.2873\n",
            "Epoch 383/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0040 - mean_absolute_error: 0.0470 - val_loss: 0.2184 - val_mean_absolute_error: 0.3091\n",
            "Epoch 384/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0045 - mean_absolute_error: 0.0551 - val_loss: 0.1998 - val_mean_absolute_error: 0.2904\n",
            "Epoch 385/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0037 - mean_absolute_error: 0.0454 - val_loss: 0.1987 - val_mean_absolute_error: 0.2942\n",
            "Epoch 386/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - mean_absolute_error: 0.0310 - val_loss: 0.2085 - val_mean_absolute_error: 0.3019\n",
            "Epoch 387/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016 - mean_absolute_error: 0.0305 - val_loss: 0.2005 - val_mean_absolute_error: 0.2954\n",
            "Epoch 388/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - mean_absolute_error: 0.0305 - val_loss: 0.2076 - val_mean_absolute_error: 0.3040\n",
            "Epoch 389/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0011 - mean_absolute_error: 0.0258 - val_loss: 0.1982 - val_mean_absolute_error: 0.2894\n",
            "Epoch 390/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.2222e-04 - mean_absolute_error: 0.0237 - val_loss: 0.1995 - val_mean_absolute_error: 0.2968\n",
            "Epoch 391/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.6770e-04 - mean_absolute_error: 0.0215 - val_loss: 0.2037 - val_mean_absolute_error: 0.2970\n",
            "Epoch 392/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.7525e-04 - mean_absolute_error: 0.0193 - val_loss: 0.1992 - val_mean_absolute_error: 0.2932\n",
            "Epoch 393/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.9482e-04 - mean_absolute_error: 0.0200 - val_loss: 0.2006 - val_mean_absolute_error: 0.2964\n",
            "Epoch 394/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.4037e-04 - mean_absolute_error: 0.0190 - val_loss: 0.1970 - val_mean_absolute_error: 0.2915\n",
            "Epoch 395/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.8891e-04 - mean_absolute_error: 0.0210 - val_loss: 0.2012 - val_mean_absolute_error: 0.2928\n",
            "Epoch 396/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.3740e-04 - mean_absolute_error: 0.0184 - val_loss: 0.2011 - val_mean_absolute_error: 0.2949\n",
            "Epoch 397/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - mean_absolute_error: 0.0259 - val_loss: 0.1987 - val_mean_absolute_error: 0.2943\n",
            "Epoch 398/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0011 - mean_absolute_error: 0.0210 - val_loss: 0.2025 - val_mean_absolute_error: 0.2996\n",
            "Epoch 399/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.2047e-04 - mean_absolute_error: 0.0202 - val_loss: 0.1951 - val_mean_absolute_error: 0.2883\n",
            "Epoch 400/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.8134e-04 - mean_absolute_error: 0.0209 - val_loss: 0.2101 - val_mean_absolute_error: 0.3054\n",
            "Epoch 401/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8.8607e-04 - mean_absolute_error: 0.0228 - val_loss: 0.1940 - val_mean_absolute_error: 0.2900\n",
            "Epoch 402/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.9286e-04 - mean_absolute_error: 0.0224 - val_loss: 0.2027 - val_mean_absolute_error: 0.2991\n",
            "Epoch 403/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.4531e-04 - mean_absolute_error: 0.0240 - val_loss: 0.2008 - val_mean_absolute_error: 0.2957\n",
            "Epoch 404/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mean_absolute_error: 0.0311 - val_loss: 0.2051 - val_mean_absolute_error: 0.2977\n",
            "Epoch 405/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014 - mean_absolute_error: 0.0299 - val_loss: 0.2046 - val_mean_absolute_error: 0.2951\n",
            "Epoch 406/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011 - mean_absolute_error: 0.0254 - val_loss: 0.2063 - val_mean_absolute_error: 0.2980\n",
            "Epoch 407/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.9875e-04 - mean_absolute_error: 0.0197 - val_loss: 0.2001 - val_mean_absolute_error: 0.2910\n",
            "Epoch 408/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 4.9824e-04 - mean_absolute_error: 0.0178 - val_loss: 0.2024 - val_mean_absolute_error: 0.2976\n",
            "Epoch 409/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.6803e-04 - mean_absolute_error: 0.0169 - val_loss: 0.2001 - val_mean_absolute_error: 0.2935\n",
            "Epoch 410/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.1528e-04 - mean_absolute_error: 0.0153 - val_loss: 0.1945 - val_mean_absolute_error: 0.2890\n",
            "Epoch 411/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.0456e-04 - mean_absolute_error: 0.0131 - val_loss: 0.2016 - val_mean_absolute_error: 0.2949\n",
            "Epoch 412/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.5422e-04 - mean_absolute_error: 0.0118 - val_loss: 0.2012 - val_mean_absolute_error: 0.2964\n",
            "Epoch 413/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4579e-04 - mean_absolute_error: 0.0112 - val_loss: 0.2022 - val_mean_absolute_error: 0.2962\n",
            "Epoch 414/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5739e-04 - mean_absolute_error: 0.0092 - val_loss: 0.1989 - val_mean_absolute_error: 0.2933\n",
            "Epoch 415/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.9952e-04 - mean_absolute_error: 0.0109 - val_loss: 0.2026 - val_mean_absolute_error: 0.2956\n",
            "Epoch 416/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.4397e-04 - mean_absolute_error: 0.0138 - val_loss: 0.1995 - val_mean_absolute_error: 0.2956\n",
            "Epoch 417/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.7696e-04 - mean_absolute_error: 0.0101 - val_loss: 0.2011 - val_mean_absolute_error: 0.2963\n",
            "Epoch 418/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4941e-04 - mean_absolute_error: 0.0095 - val_loss: 0.1993 - val_mean_absolute_error: 0.2948\n",
            "Epoch 419/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5499e-04 - mean_absolute_error: 0.0089 - val_loss: 0.2035 - val_mean_absolute_error: 0.2965\n",
            "Epoch 420/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7601e-04 - mean_absolute_error: 0.0099 - val_loss: 0.1997 - val_mean_absolute_error: 0.2942\n",
            "Epoch 421/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2530e-04 - mean_absolute_error: 0.0118 - val_loss: 0.1989 - val_mean_absolute_error: 0.2923\n",
            "Epoch 422/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.1411e-04 - mean_absolute_error: 0.0109 - val_loss: 0.2022 - val_mean_absolute_error: 0.2996\n",
            "Epoch 423/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5777e-04 - mean_absolute_error: 0.0128 - val_loss: 0.1965 - val_mean_absolute_error: 0.2921\n",
            "Epoch 424/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.7485e-04 - mean_absolute_error: 0.0159 - val_loss: 0.2036 - val_mean_absolute_error: 0.2981\n",
            "Epoch 425/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.0604e-04 - mean_absolute_error: 0.0155 - val_loss: 0.1986 - val_mean_absolute_error: 0.2941\n",
            "Epoch 426/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.9146e-04 - mean_absolute_error: 0.0181 - val_loss: 0.2013 - val_mean_absolute_error: 0.2992\n",
            "Epoch 427/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.1516e-04 - mean_absolute_error: 0.0210 - val_loss: 0.1997 - val_mean_absolute_error: 0.2937\n",
            "Epoch 428/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.3232e-04 - mean_absolute_error: 0.0218 - val_loss: 0.2001 - val_mean_absolute_error: 0.2975\n",
            "Epoch 429/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0014 - mean_absolute_error: 0.0310 - val_loss: 0.1987 - val_mean_absolute_error: 0.2940\n",
            "Epoch 430/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - mean_absolute_error: 0.0312 - val_loss: 0.2156 - val_mean_absolute_error: 0.3057\n",
            "Epoch 431/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0018 - mean_absolute_error: 0.0321 - val_loss: 0.2012 - val_mean_absolute_error: 0.2991\n",
            "Epoch 432/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - mean_absolute_error: 0.0318 - val_loss: 0.2068 - val_mean_absolute_error: 0.3026\n",
            "Epoch 433/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - mean_absolute_error: 0.0381 - val_loss: 0.1975 - val_mean_absolute_error: 0.2944\n",
            "Epoch 434/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - mean_absolute_error: 0.0483 - val_loss: 0.2092 - val_mean_absolute_error: 0.2990\n",
            "Epoch 435/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - mean_absolute_error: 0.0479 - val_loss: 0.2046 - val_mean_absolute_error: 0.2952\n",
            "Epoch 436/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0050 - mean_absolute_error: 0.0533 - val_loss: 0.2109 - val_mean_absolute_error: 0.2960\n",
            "Epoch 437/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - mean_absolute_error: 0.0558 - val_loss: 0.2002 - val_mean_absolute_error: 0.2983\n",
            "Epoch 438/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0069 - mean_absolute_error: 0.0622 - val_loss: 0.2160 - val_mean_absolute_error: 0.3039\n",
            "Epoch 439/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0064 - mean_absolute_error: 0.0605 - val_loss: 0.1906 - val_mean_absolute_error: 0.2872\n",
            "Epoch 440/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0077 - mean_absolute_error: 0.0679 - val_loss: 0.1914 - val_mean_absolute_error: 0.2926\n",
            "Epoch 441/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0060 - mean_absolute_error: 0.0580 - val_loss: 0.1905 - val_mean_absolute_error: 0.2877\n",
            "Epoch 442/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0056 - mean_absolute_error: 0.0585 - val_loss: 0.2215 - val_mean_absolute_error: 0.3154\n",
            "Epoch 443/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0077 - mean_absolute_error: 0.0679 - val_loss: 0.2042 - val_mean_absolute_error: 0.2978\n",
            "Epoch 444/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0054 - mean_absolute_error: 0.0550 - val_loss: 0.1751 - val_mean_absolute_error: 0.2770\n",
            "Epoch 445/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - mean_absolute_error: 0.0510 - val_loss: 0.1962 - val_mean_absolute_error: 0.2930\n",
            "Epoch 446/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - mean_absolute_error: 0.0488 - val_loss: 0.1961 - val_mean_absolute_error: 0.2976\n",
            "Epoch 447/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0040 - mean_absolute_error: 0.0486 - val_loss: 0.1904 - val_mean_absolute_error: 0.2843\n",
            "Epoch 448/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0044 - mean_absolute_error: 0.0428 - val_loss: 0.1969 - val_mean_absolute_error: 0.2944\n",
            "Epoch 449/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0048 - mean_absolute_error: 0.0503 - val_loss: 0.1958 - val_mean_absolute_error: 0.2921\n",
            "Epoch 450/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0035 - mean_absolute_error: 0.0441 - val_loss: 0.1953 - val_mean_absolute_error: 0.2865\n",
            "Epoch 451/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0049 - mean_absolute_error: 0.0530 - val_loss: 0.1970 - val_mean_absolute_error: 0.3021\n",
            "Epoch 452/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - mean_absolute_error: 0.0504 - val_loss: 0.2162 - val_mean_absolute_error: 0.3040\n",
            "Epoch 453/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0046 - mean_absolute_error: 0.0531 - val_loss: 0.1858 - val_mean_absolute_error: 0.2795\n",
            "Epoch 454/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0041 - mean_absolute_error: 0.0494 - val_loss: 0.2121 - val_mean_absolute_error: 0.3032\n",
            "Epoch 455/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0041 - mean_absolute_error: 0.0500 - val_loss: 0.2136 - val_mean_absolute_error: 0.3125\n",
            "Epoch 456/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0052 - mean_absolute_error: 0.0570 - val_loss: 0.1937 - val_mean_absolute_error: 0.2816\n",
            "Epoch 457/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - mean_absolute_error: 0.0536 - val_loss: 0.2092 - val_mean_absolute_error: 0.3029\n",
            "Epoch 458/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - mean_absolute_error: 0.0506 - val_loss: 0.1971 - val_mean_absolute_error: 0.3031\n",
            "Epoch 459/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0048 - mean_absolute_error: 0.0487 - val_loss: 0.1893 - val_mean_absolute_error: 0.2870\n",
            "Epoch 460/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0040 - mean_absolute_error: 0.0487 - val_loss: 0.1935 - val_mean_absolute_error: 0.3020\n",
            "Epoch 461/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0036 - mean_absolute_error: 0.0511 - val_loss: 0.2079 - val_mean_absolute_error: 0.3025\n",
            "Epoch 462/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0035 - mean_absolute_error: 0.0447 - val_loss: 0.1905 - val_mean_absolute_error: 0.2904\n",
            "Epoch 463/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0026 - mean_absolute_error: 0.0383 - val_loss: 0.1957 - val_mean_absolute_error: 0.3039\n",
            "Epoch 464/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - mean_absolute_error: 0.0350 - val_loss: 0.1874 - val_mean_absolute_error: 0.2886\n",
            "Epoch 465/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025 - mean_absolute_error: 0.0358 - val_loss: 0.2064 - val_mean_absolute_error: 0.2909\n",
            "Epoch 466/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - mean_absolute_error: 0.0582 - val_loss: 0.2101 - val_mean_absolute_error: 0.3084\n",
            "Epoch 467/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - mean_absolute_error: 0.0682 - val_loss: 0.1917 - val_mean_absolute_error: 0.2879\n",
            "Epoch 468/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0050 - mean_absolute_error: 0.0546 - val_loss: 0.1874 - val_mean_absolute_error: 0.2863\n",
            "Epoch 469/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0048 - mean_absolute_error: 0.0525 - val_loss: 0.1986 - val_mean_absolute_error: 0.2891\n",
            "Epoch 470/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0048 - mean_absolute_error: 0.0474 - val_loss: 0.1949 - val_mean_absolute_error: 0.2957\n",
            "Epoch 471/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - mean_absolute_error: 0.0441 - val_loss: 0.2023 - val_mean_absolute_error: 0.2943\n",
            "Epoch 472/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0033 - mean_absolute_error: 0.0461 - val_loss: 0.1921 - val_mean_absolute_error: 0.2856\n",
            "Epoch 473/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0024 - mean_absolute_error: 0.0386 - val_loss: 0.1870 - val_mean_absolute_error: 0.2813\n",
            "Epoch 474/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - mean_absolute_error: 0.0368 - val_loss: 0.2139 - val_mean_absolute_error: 0.3065\n",
            "Epoch 475/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0038 - mean_absolute_error: 0.0452 - val_loss: 0.1866 - val_mean_absolute_error: 0.2855\n",
            "Epoch 476/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0026 - mean_absolute_error: 0.0392 - val_loss: 0.2040 - val_mean_absolute_error: 0.2923\n",
            "Epoch 477/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0035 - mean_absolute_error: 0.0458 - val_loss: 0.1954 - val_mean_absolute_error: 0.2964\n",
            "Epoch 478/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0035 - mean_absolute_error: 0.0469 - val_loss: 0.1861 - val_mean_absolute_error: 0.2776\n",
            "Epoch 479/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0037 - mean_absolute_error: 0.0434 - val_loss: 0.2043 - val_mean_absolute_error: 0.2980\n",
            "Epoch 480/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0031 - mean_absolute_error: 0.0422 - val_loss: 0.2037 - val_mean_absolute_error: 0.2975\n",
            "Epoch 481/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - mean_absolute_error: 0.0420 - val_loss: 0.1911 - val_mean_absolute_error: 0.2854\n",
            "Epoch 482/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - mean_absolute_error: 0.0384 - val_loss: 0.2041 - val_mean_absolute_error: 0.3066\n",
            "Epoch 483/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0027 - mean_absolute_error: 0.0391 - val_loss: 0.1977 - val_mean_absolute_error: 0.2846\n",
            "Epoch 484/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036 - mean_absolute_error: 0.0477 - val_loss: 0.1864 - val_mean_absolute_error: 0.2854\n",
            "Epoch 485/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - mean_absolute_error: 0.0345 - val_loss: 0.2000 - val_mean_absolute_error: 0.2998\n",
            "Epoch 486/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0027 - mean_absolute_error: 0.0386 - val_loss: 0.1856 - val_mean_absolute_error: 0.2805\n",
            "Epoch 487/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0033 - mean_absolute_error: 0.0444 - val_loss: 0.2137 - val_mean_absolute_error: 0.3019\n",
            "Epoch 488/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025 - mean_absolute_error: 0.0376 - val_loss: 0.1923 - val_mean_absolute_error: 0.2858\n",
            "Epoch 489/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0021 - mean_absolute_error: 0.0356 - val_loss: 0.1890 - val_mean_absolute_error: 0.2833\n",
            "Epoch 490/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - mean_absolute_error: 0.0342 - val_loss: 0.2040 - val_mean_absolute_error: 0.2926\n",
            "Epoch 491/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016 - mean_absolute_error: 0.0289 - val_loss: 0.1952 - val_mean_absolute_error: 0.2885\n",
            "Epoch 492/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0015 - mean_absolute_error: 0.0287 - val_loss: 0.2034 - val_mean_absolute_error: 0.2935\n",
            "Epoch 493/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0015 - mean_absolute_error: 0.0293 - val_loss: 0.1948 - val_mean_absolute_error: 0.2869\n",
            "Epoch 494/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0012 - mean_absolute_error: 0.0267 - val_loss: 0.2059 - val_mean_absolute_error: 0.2967\n",
            "Epoch 495/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - mean_absolute_error: 0.0300 - val_loss: 0.2086 - val_mean_absolute_error: 0.3030\n",
            "Epoch 496/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - mean_absolute_error: 0.0337 - val_loss: 0.1908 - val_mean_absolute_error: 0.2805\n",
            "Epoch 497/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0016 - mean_absolute_error: 0.0291 - val_loss: 0.1971 - val_mean_absolute_error: 0.2944\n",
            "Epoch 498/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0012 - mean_absolute_error: 0.0289 - val_loss: 0.1897 - val_mean_absolute_error: 0.2843\n",
            "Epoch 499/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0010 - mean_absolute_error: 0.0253 - val_loss: 0.1948 - val_mean_absolute_error: 0.2909\n",
            "Epoch 500/500\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 8.4424e-04 - mean_absolute_error: 0.0235 - val_loss: 0.1966 - val_mean_absolute_error: 0.2895\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5045 - mean_absolute_error: 0.4722 \n",
            "Test Loss (MSE): 0.4318443834781647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7eb6d7b8f880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7eb6d7b8f880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE : 0.4318443394201852\n",
            "RMSE: 0.6571486433221826\n",
            "MAE : 0.44109487533569336\n",
            "R2  : 0.8282191412144658\n",
            "MAPE: 32.94821248805299%\n",
            "Model saved to trained_mlp_model(ADAM).h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mnew_wn0ht_",
        "outputId": "0a03857a-8002-4fb8-ba20-6dd6cfbc0ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.3.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-24.7.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.7.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_lIVdld07F4",
        "outputId": "428bf2c5-3580-471e-ee48-dce5b1e6b246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.4.1)\n",
            "Collecting scikit-learn>=1.4.2 (from scikeras)\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn, scikeras\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "Successfully installed scikeras-0.13.0 scikit-learn-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
        "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=1)\n",
        "\n",
        "# Define the hyperparameters\n",
        "n_estimators = 100\n",
        "max_depth = None\n",
        "min_samples_split = 2\n",
        "min_samples_leaf = 1\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=n_estimators,\n",
        "    max_depth=max_depth,\n",
        "    min_samples_split=min_samples_split,\n",
        "    min_samples_leaf=min_samples_leaf,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = rf.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
        "\n",
        "print('Test MSE:', mse)\n",
        "print('Test RMSE:', rmse)\n",
        "print('Test MAE:', mae)\n",
        "print('Test R^2:', r2)\n",
        "print('Test MAPE:', mape)\n",
        "\n",
        "# Save the trained model as a .pkl file\n",
        "with open('random_forest_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(rf, model_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHWEwdYV1Qqf",
        "outputId": "385d7403-33fd-467f-f371-8f2056c7135e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.28489952243589745\n",
            "Test RMSE: 0.5337597984448599\n",
            "Test MAE: 0.3651115384615385\n",
            "Test R^2: 0.8588554833712518\n",
            "Test MAPE: 42.38896439425764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
        "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=2)\n",
        "\n",
        "# Define the hyperparameters\n",
        "C = 1\n",
        "epsilon = 0.5\n",
        "kernel = 'linear'\n",
        "\n",
        "# Train the SVR model\n",
        "svr = SVR(\n",
        "    C=C,\n",
        "    epsilon=epsilon,\n",
        "    kernel=kernel,\n",
        "    tol=0.01,\n",
        "    shrinking=False\n",
        ")\n",
        "\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = svr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
        "\n",
        "print('Test MSE:', mse)\n",
        "print('Test RMSE:', rmse)\n",
        "print('Test MAE:', mae)\n",
        "print('Test R^2:', r2)\n",
        "print('Test MAPE:', mape)\n",
        "\n",
        "# Save the trained model as a .pkl file\n",
        "with open('svr_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(svr, model_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpxN_Ib1hiNC",
        "outputId": "d0a3b2b7-c204-4b60-d7bf-10a63453d1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.9711557115505729\n",
            "Test RMSE: 0.9854723291653464\n",
            "Test MAE: 0.7463335425409465\n",
            "Test R^2: 0.4338875489577164\n",
            "Test MAPE: 68.80460193060578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
        "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
        "\n",
        "best_mse = float('inf')\n",
        "best_model = None\n",
        "best_seed = None\n",
        "\n",
        "# Iterate over a range of random seeds\n",
        "for seed in range(5000):\n",
        "    print(seed)\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=seed)\n",
        "\n",
        "    # Train the Linear Regression model\n",
        "    linear_reg = LinearRegression()\n",
        "    linear_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = linear_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "\n",
        "    # Check if this model is better than the previous best\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_model = linear_reg\n",
        "        best_seed = seed\n",
        "\n",
        "# Evaluate the best model with all metrics\n",
        "best_predictions = best_model.predict(X_test)\n",
        "best_rmse = np.sqrt(best_mse)\n",
        "best_mae = mean_absolute_error(y_test, best_predictions)\n",
        "best_r2 = r2_score(y_test, best_predictions)\n",
        "best_mape = np.mean(np.abs((y_test - best_predictions) / y_test)) * 100\n",
        "\n",
        "print('Best Seed:', best_seed)\n",
        "print('Best Test MSE:', best_mse)\n",
        "print('Best Test RMSE:', best_rmse)\n",
        "print('Best Test MAE:', best_mae)\n",
        "print('Best Test R^2:', best_r2)\n",
        "print('Best Test MAPE:', best_mape)\n",
        "\n",
        "# Save the best model as a .pkl file\n",
        "with open('best_linear_regression_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_model, model_file)\n"
      ],
      "metadata": {
        "id": "sdEl7nqahv_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ed240b-9e52-4caa-a273-b6a998c405f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n",
            "2647\n",
            "2648\n",
            "2649\n",
            "2650\n",
            "2651\n",
            "2652\n",
            "2653\n",
            "2654\n",
            "2655\n",
            "2656\n",
            "2657\n",
            "2658\n",
            "2659\n",
            "2660\n",
            "2661\n",
            "2662\n",
            "2663\n",
            "2664\n",
            "2665\n",
            "2666\n",
            "2667\n",
            "2668\n",
            "2669\n",
            "2670\n",
            "2671\n",
            "2672\n",
            "2673\n",
            "2674\n",
            "2675\n",
            "2676\n",
            "2677\n",
            "2678\n",
            "2679\n",
            "2680\n",
            "2681\n",
            "2682\n",
            "2683\n",
            "2684\n",
            "2685\n",
            "2686\n",
            "2687\n",
            "2688\n",
            "2689\n",
            "2690\n",
            "2691\n",
            "2692\n",
            "2693\n",
            "2694\n",
            "2695\n",
            "2696\n",
            "2697\n",
            "2698\n",
            "2699\n",
            "2700\n",
            "2701\n",
            "2702\n",
            "2703\n",
            "2704\n",
            "2705\n",
            "2706\n",
            "2707\n",
            "2708\n",
            "2709\n",
            "2710\n",
            "2711\n",
            "2712\n",
            "2713\n",
            "2714\n",
            "2715\n",
            "2716\n",
            "2717\n",
            "2718\n",
            "2719\n",
            "2720\n",
            "2721\n",
            "2722\n",
            "2723\n",
            "2724\n",
            "2725\n",
            "2726\n",
            "2727\n",
            "2728\n",
            "2729\n",
            "2730\n",
            "2731\n",
            "2732\n",
            "2733\n",
            "2734\n",
            "2735\n",
            "2736\n",
            "2737\n",
            "2738\n",
            "2739\n",
            "2740\n",
            "2741\n",
            "2742\n",
            "2743\n",
            "2744\n",
            "2745\n",
            "2746\n",
            "2747\n",
            "2748\n",
            "2749\n",
            "2750\n",
            "2751\n",
            "2752\n",
            "2753\n",
            "2754\n",
            "2755\n",
            "2756\n",
            "2757\n",
            "2758\n",
            "2759\n",
            "2760\n",
            "2761\n",
            "2762\n",
            "2763\n",
            "2764\n",
            "2765\n",
            "2766\n",
            "2767\n",
            "2768\n",
            "2769\n",
            "2770\n",
            "2771\n",
            "2772\n",
            "2773\n",
            "2774\n",
            "2775\n",
            "2776\n",
            "2777\n",
            "2778\n",
            "2779\n",
            "2780\n",
            "2781\n",
            "2782\n",
            "2783\n",
            "2784\n",
            "2785\n",
            "2786\n",
            "2787\n",
            "2788\n",
            "2789\n",
            "2790\n",
            "2791\n",
            "2792\n",
            "2793\n",
            "2794\n",
            "2795\n",
            "2796\n",
            "2797\n",
            "2798\n",
            "2799\n",
            "2800\n",
            "2801\n",
            "2802\n",
            "2803\n",
            "2804\n",
            "2805\n",
            "2806\n",
            "2807\n",
            "2808\n",
            "2809\n",
            "2810\n",
            "2811\n",
            "2812\n",
            "2813\n",
            "2814\n",
            "2815\n",
            "2816\n",
            "2817\n",
            "2818\n",
            "2819\n",
            "2820\n",
            "2821\n",
            "2822\n",
            "2823\n",
            "2824\n",
            "2825\n",
            "2826\n",
            "2827\n",
            "2828\n",
            "2829\n",
            "2830\n",
            "2831\n",
            "2832\n",
            "2833\n",
            "2834\n",
            "2835\n",
            "2836\n",
            "2837\n",
            "2838\n",
            "2839\n",
            "2840\n",
            "2841\n",
            "2842\n",
            "2843\n",
            "2844\n",
            "2845\n",
            "2846\n",
            "2847\n",
            "2848\n",
            "2849\n",
            "2850\n",
            "2851\n",
            "2852\n",
            "2853\n",
            "2854\n",
            "2855\n",
            "2856\n",
            "2857\n",
            "2858\n",
            "2859\n",
            "2860\n",
            "2861\n",
            "2862\n",
            "2863\n",
            "2864\n",
            "2865\n",
            "2866\n",
            "2867\n",
            "2868\n",
            "2869\n",
            "2870\n",
            "2871\n",
            "2872\n",
            "2873\n",
            "2874\n",
            "2875\n",
            "2876\n",
            "2877\n",
            "2878\n",
            "2879\n",
            "2880\n",
            "2881\n",
            "2882\n",
            "2883\n",
            "2884\n",
            "2885\n",
            "2886\n",
            "2887\n",
            "2888\n",
            "2889\n",
            "2890\n",
            "2891\n",
            "2892\n",
            "2893\n",
            "2894\n",
            "2895\n",
            "2896\n",
            "2897\n",
            "2898\n",
            "2899\n",
            "2900\n",
            "2901\n",
            "2902\n",
            "2903\n",
            "2904\n",
            "2905\n",
            "2906\n",
            "2907\n",
            "2908\n",
            "2909\n",
            "2910\n",
            "2911\n",
            "2912\n",
            "2913\n",
            "2914\n",
            "2915\n",
            "2916\n",
            "2917\n",
            "2918\n",
            "2919\n",
            "2920\n",
            "2921\n",
            "2922\n",
            "2923\n",
            "2924\n",
            "2925\n",
            "2926\n",
            "2927\n",
            "2928\n",
            "2929\n",
            "2930\n",
            "2931\n",
            "2932\n",
            "2933\n",
            "2934\n",
            "2935\n",
            "2936\n",
            "2937\n",
            "2938\n",
            "2939\n",
            "2940\n",
            "2941\n",
            "2942\n",
            "2943\n",
            "2944\n",
            "2945\n",
            "2946\n",
            "2947\n",
            "2948\n",
            "2949\n",
            "2950\n",
            "2951\n",
            "2952\n",
            "2953\n",
            "2954\n",
            "2955\n",
            "2956\n",
            "2957\n",
            "2958\n",
            "2959\n",
            "2960\n",
            "2961\n",
            "2962\n",
            "2963\n",
            "2964\n",
            "2965\n",
            "2966\n",
            "2967\n",
            "2968\n",
            "2969\n",
            "2970\n",
            "2971\n",
            "2972\n",
            "2973\n",
            "2974\n",
            "2975\n",
            "2976\n",
            "2977\n",
            "2978\n",
            "2979\n",
            "2980\n",
            "2981\n",
            "2982\n",
            "2983\n",
            "2984\n",
            "2985\n",
            "2986\n",
            "2987\n",
            "2988\n",
            "2989\n",
            "2990\n",
            "2991\n",
            "2992\n",
            "2993\n",
            "2994\n",
            "2995\n",
            "2996\n",
            "2997\n",
            "2998\n",
            "2999\n",
            "3000\n",
            "3001\n",
            "3002\n",
            "3003\n",
            "3004\n",
            "3005\n",
            "3006\n",
            "3007\n",
            "3008\n",
            "3009\n",
            "3010\n",
            "3011\n",
            "3012\n",
            "3013\n",
            "3014\n",
            "3015\n",
            "3016\n",
            "3017\n",
            "3018\n",
            "3019\n",
            "3020\n",
            "3021\n",
            "3022\n",
            "3023\n",
            "3024\n",
            "3025\n",
            "3026\n",
            "3027\n",
            "3028\n",
            "3029\n",
            "3030\n",
            "3031\n",
            "3032\n",
            "3033\n",
            "3034\n",
            "3035\n",
            "3036\n",
            "3037\n",
            "3038\n",
            "3039\n",
            "3040\n",
            "3041\n",
            "3042\n",
            "3043\n",
            "3044\n",
            "3045\n",
            "3046\n",
            "3047\n",
            "3048\n",
            "3049\n",
            "3050\n",
            "3051\n",
            "3052\n",
            "3053\n",
            "3054\n",
            "3055\n",
            "3056\n",
            "3057\n",
            "3058\n",
            "3059\n",
            "3060\n",
            "3061\n",
            "3062\n",
            "3063\n",
            "3064\n",
            "3065\n",
            "3066\n",
            "3067\n",
            "3068\n",
            "3069\n",
            "3070\n",
            "3071\n",
            "3072\n",
            "3073\n",
            "3074\n",
            "3075\n",
            "3076\n",
            "3077\n",
            "3078\n",
            "3079\n",
            "3080\n",
            "3081\n",
            "3082\n",
            "3083\n",
            "3084\n",
            "3085\n",
            "3086\n",
            "3087\n",
            "3088\n",
            "3089\n",
            "3090\n",
            "3091\n",
            "3092\n",
            "3093\n",
            "3094\n",
            "3095\n",
            "3096\n",
            "3097\n",
            "3098\n",
            "3099\n",
            "3100\n",
            "3101\n",
            "3102\n",
            "3103\n",
            "3104\n",
            "3105\n",
            "3106\n",
            "3107\n",
            "3108\n",
            "3109\n",
            "3110\n",
            "3111\n",
            "3112\n",
            "3113\n",
            "3114\n",
            "3115\n",
            "3116\n",
            "3117\n",
            "3118\n",
            "3119\n",
            "3120\n",
            "3121\n",
            "3122\n",
            "3123\n",
            "3124\n",
            "3125\n",
            "3126\n",
            "3127\n",
            "3128\n",
            "3129\n",
            "3130\n",
            "3131\n",
            "3132\n",
            "3133\n",
            "3134\n",
            "3135\n",
            "3136\n",
            "3137\n",
            "3138\n",
            "3139\n",
            "3140\n",
            "3141\n",
            "3142\n",
            "3143\n",
            "3144\n",
            "3145\n",
            "3146\n",
            "3147\n",
            "3148\n",
            "3149\n",
            "3150\n",
            "3151\n",
            "3152\n",
            "3153\n",
            "3154\n",
            "3155\n",
            "3156\n",
            "3157\n",
            "3158\n",
            "3159\n",
            "3160\n",
            "3161\n",
            "3162\n",
            "3163\n",
            "3164\n",
            "3165\n",
            "3166\n",
            "3167\n",
            "3168\n",
            "3169\n",
            "3170\n",
            "3171\n",
            "3172\n",
            "3173\n",
            "3174\n",
            "3175\n",
            "3176\n",
            "3177\n",
            "3178\n",
            "3179\n",
            "3180\n",
            "3181\n",
            "3182\n",
            "3183\n",
            "3184\n",
            "3185\n",
            "3186\n",
            "3187\n",
            "3188\n",
            "3189\n",
            "3190\n",
            "3191\n",
            "3192\n",
            "3193\n",
            "3194\n",
            "3195\n",
            "3196\n",
            "3197\n",
            "3198\n",
            "3199\n",
            "3200\n",
            "3201\n",
            "3202\n",
            "3203\n",
            "3204\n",
            "3205\n",
            "3206\n",
            "3207\n",
            "3208\n",
            "3209\n",
            "3210\n",
            "3211\n",
            "3212\n",
            "3213\n",
            "3214\n",
            "3215\n",
            "3216\n",
            "3217\n",
            "3218\n",
            "3219\n",
            "3220\n",
            "3221\n",
            "3222\n",
            "3223\n",
            "3224\n",
            "3225\n",
            "3226\n",
            "3227\n",
            "3228\n",
            "3229\n",
            "3230\n",
            "3231\n",
            "3232\n",
            "3233\n",
            "3234\n",
            "3235\n",
            "3236\n",
            "3237\n",
            "3238\n",
            "3239\n",
            "3240\n",
            "3241\n",
            "3242\n",
            "3243\n",
            "3244\n",
            "3245\n",
            "3246\n",
            "3247\n",
            "3248\n",
            "3249\n",
            "3250\n",
            "3251\n",
            "3252\n",
            "3253\n",
            "3254\n",
            "3255\n",
            "3256\n",
            "3257\n",
            "3258\n",
            "3259\n",
            "3260\n",
            "3261\n",
            "3262\n",
            "3263\n",
            "3264\n",
            "3265\n",
            "3266\n",
            "3267\n",
            "3268\n",
            "3269\n",
            "3270\n",
            "3271\n",
            "3272\n",
            "3273\n",
            "3274\n",
            "3275\n",
            "3276\n",
            "3277\n",
            "3278\n",
            "3279\n",
            "3280\n",
            "3281\n",
            "3282\n",
            "3283\n",
            "3284\n",
            "3285\n",
            "3286\n",
            "3287\n",
            "3288\n",
            "3289\n",
            "3290\n",
            "3291\n",
            "3292\n",
            "3293\n",
            "3294\n",
            "3295\n",
            "3296\n",
            "3297\n",
            "3298\n",
            "3299\n",
            "3300\n",
            "3301\n",
            "3302\n",
            "3303\n",
            "3304\n",
            "3305\n",
            "3306\n",
            "3307\n",
            "3308\n",
            "3309\n",
            "3310\n",
            "3311\n",
            "3312\n",
            "3313\n",
            "3314\n",
            "3315\n",
            "3316\n",
            "3317\n",
            "3318\n",
            "3319\n",
            "3320\n",
            "3321\n",
            "3322\n",
            "3323\n",
            "3324\n",
            "3325\n",
            "3326\n",
            "3327\n",
            "3328\n",
            "3329\n",
            "3330\n",
            "3331\n",
            "3332\n",
            "3333\n",
            "3334\n",
            "3335\n",
            "3336\n",
            "3337\n",
            "3338\n",
            "3339\n",
            "3340\n",
            "3341\n",
            "3342\n",
            "3343\n",
            "3344\n",
            "3345\n",
            "3346\n",
            "3347\n",
            "3348\n",
            "3349\n",
            "3350\n",
            "3351\n",
            "3352\n",
            "3353\n",
            "3354\n",
            "3355\n",
            "3356\n",
            "3357\n",
            "3358\n",
            "3359\n",
            "3360\n",
            "3361\n",
            "3362\n",
            "3363\n",
            "3364\n",
            "3365\n",
            "3366\n",
            "3367\n",
            "3368\n",
            "3369\n",
            "3370\n",
            "3371\n",
            "3372\n",
            "3373\n",
            "3374\n",
            "3375\n",
            "3376\n",
            "3377\n",
            "3378\n",
            "3379\n",
            "3380\n",
            "3381\n",
            "3382\n",
            "3383\n",
            "3384\n",
            "3385\n",
            "3386\n",
            "3387\n",
            "3388\n",
            "3389\n",
            "3390\n",
            "3391\n",
            "3392\n",
            "3393\n",
            "3394\n",
            "3395\n",
            "3396\n",
            "3397\n",
            "3398\n",
            "3399\n",
            "3400\n",
            "3401\n",
            "3402\n",
            "3403\n",
            "3404\n",
            "3405\n",
            "3406\n",
            "3407\n",
            "3408\n",
            "3409\n",
            "3410\n",
            "3411\n",
            "3412\n",
            "3413\n",
            "3414\n",
            "3415\n",
            "3416\n",
            "3417\n",
            "3418\n",
            "3419\n",
            "3420\n",
            "3421\n",
            "3422\n",
            "3423\n",
            "3424\n",
            "3425\n",
            "3426\n",
            "3427\n",
            "3428\n",
            "3429\n",
            "3430\n",
            "3431\n",
            "3432\n",
            "3433\n",
            "3434\n",
            "3435\n",
            "3436\n",
            "3437\n",
            "3438\n",
            "3439\n",
            "3440\n",
            "3441\n",
            "3442\n",
            "3443\n",
            "3444\n",
            "3445\n",
            "3446\n",
            "3447\n",
            "3448\n",
            "3449\n",
            "3450\n",
            "3451\n",
            "3452\n",
            "3453\n",
            "3454\n",
            "3455\n",
            "3456\n",
            "3457\n",
            "3458\n",
            "3459\n",
            "3460\n",
            "3461\n",
            "3462\n",
            "3463\n",
            "3464\n",
            "3465\n",
            "3466\n",
            "3467\n",
            "3468\n",
            "3469\n",
            "3470\n",
            "3471\n",
            "3472\n",
            "3473\n",
            "3474\n",
            "3475\n",
            "3476\n",
            "3477\n",
            "3478\n",
            "3479\n",
            "3480\n",
            "3481\n",
            "3482\n",
            "3483\n",
            "3484\n",
            "3485\n",
            "3486\n",
            "3487\n",
            "3488\n",
            "3489\n",
            "3490\n",
            "3491\n",
            "3492\n",
            "3493\n",
            "3494\n",
            "3495\n",
            "3496\n",
            "3497\n",
            "3498\n",
            "3499\n",
            "3500\n",
            "3501\n",
            "3502\n",
            "3503\n",
            "3504\n",
            "3505\n",
            "3506\n",
            "3507\n",
            "3508\n",
            "3509\n",
            "3510\n",
            "3511\n",
            "3512\n",
            "3513\n",
            "3514\n",
            "3515\n",
            "3516\n",
            "3517\n",
            "3518\n",
            "3519\n",
            "3520\n",
            "3521\n",
            "3522\n",
            "3523\n",
            "3524\n",
            "3525\n",
            "3526\n",
            "3527\n",
            "3528\n",
            "3529\n",
            "3530\n",
            "3531\n",
            "3532\n",
            "3533\n",
            "3534\n",
            "3535\n",
            "3536\n",
            "3537\n",
            "3538\n",
            "3539\n",
            "3540\n",
            "3541\n",
            "3542\n",
            "3543\n",
            "3544\n",
            "3545\n",
            "3546\n",
            "3547\n",
            "3548\n",
            "3549\n",
            "3550\n",
            "3551\n",
            "3552\n",
            "3553\n",
            "3554\n",
            "3555\n",
            "3556\n",
            "3557\n",
            "3558\n",
            "3559\n",
            "3560\n",
            "3561\n",
            "3562\n",
            "3563\n",
            "3564\n",
            "3565\n",
            "3566\n",
            "3567\n",
            "3568\n",
            "3569\n",
            "3570\n",
            "3571\n",
            "3572\n",
            "3573\n",
            "3574\n",
            "3575\n",
            "3576\n",
            "3577\n",
            "3578\n",
            "3579\n",
            "3580\n",
            "3581\n",
            "3582\n",
            "3583\n",
            "3584\n",
            "3585\n",
            "3586\n",
            "3587\n",
            "3588\n",
            "3589\n",
            "3590\n",
            "3591\n",
            "3592\n",
            "3593\n",
            "3594\n",
            "3595\n",
            "3596\n",
            "3597\n",
            "3598\n",
            "3599\n",
            "3600\n",
            "3601\n",
            "3602\n",
            "3603\n",
            "3604\n",
            "3605\n",
            "3606\n",
            "3607\n",
            "3608\n",
            "3609\n",
            "3610\n",
            "3611\n",
            "3612\n",
            "3613\n",
            "3614\n",
            "3615\n",
            "3616\n",
            "3617\n",
            "3618\n",
            "3619\n",
            "3620\n",
            "3621\n",
            "3622\n",
            "3623\n",
            "3624\n",
            "3625\n",
            "3626\n",
            "3627\n",
            "3628\n",
            "3629\n",
            "3630\n",
            "3631\n",
            "3632\n",
            "3633\n",
            "3634\n",
            "3635\n",
            "3636\n",
            "3637\n",
            "3638\n",
            "3639\n",
            "3640\n",
            "3641\n",
            "3642\n",
            "3643\n",
            "3644\n",
            "3645\n",
            "3646\n",
            "3647\n",
            "3648\n",
            "3649\n",
            "3650\n",
            "3651\n",
            "3652\n",
            "3653\n",
            "3654\n",
            "3655\n",
            "3656\n",
            "3657\n",
            "3658\n",
            "3659\n",
            "3660\n",
            "3661\n",
            "3662\n",
            "3663\n",
            "3664\n",
            "3665\n",
            "3666\n",
            "3667\n",
            "3668\n",
            "3669\n",
            "3670\n",
            "3671\n",
            "3672\n",
            "3673\n",
            "3674\n",
            "3675\n",
            "3676\n",
            "3677\n",
            "3678\n",
            "3679\n",
            "3680\n",
            "3681\n",
            "3682\n",
            "3683\n",
            "3684\n",
            "3685\n",
            "3686\n",
            "3687\n",
            "3688\n",
            "3689\n",
            "3690\n",
            "3691\n",
            "3692\n",
            "3693\n",
            "3694\n",
            "3695\n",
            "3696\n",
            "3697\n",
            "3698\n",
            "3699\n",
            "3700\n",
            "3701\n",
            "3702\n",
            "3703\n",
            "3704\n",
            "3705\n",
            "3706\n",
            "3707\n",
            "3708\n",
            "3709\n",
            "3710\n",
            "3711\n",
            "3712\n",
            "3713\n",
            "3714\n",
            "3715\n",
            "3716\n",
            "3717\n",
            "3718\n",
            "3719\n",
            "3720\n",
            "3721\n",
            "3722\n",
            "3723\n",
            "3724\n",
            "3725\n",
            "3726\n",
            "3727\n",
            "3728\n",
            "3729\n",
            "3730\n",
            "3731\n",
            "3732\n",
            "3733\n",
            "3734\n",
            "3735\n",
            "3736\n",
            "3737\n",
            "3738\n",
            "3739\n",
            "3740\n",
            "3741\n",
            "3742\n",
            "3743\n",
            "3744\n",
            "3745\n",
            "3746\n",
            "3747\n",
            "3748\n",
            "3749\n",
            "3750\n",
            "3751\n",
            "3752\n",
            "3753\n",
            "3754\n",
            "3755\n",
            "3756\n",
            "3757\n",
            "3758\n",
            "3759\n",
            "3760\n",
            "3761\n",
            "3762\n",
            "3763\n",
            "3764\n",
            "3765\n",
            "3766\n",
            "3767\n",
            "3768\n",
            "3769\n",
            "3770\n",
            "3771\n",
            "3772\n",
            "3773\n",
            "3774\n",
            "3775\n",
            "3776\n",
            "3777\n",
            "3778\n",
            "3779\n",
            "3780\n",
            "3781\n",
            "3782\n",
            "3783\n",
            "3784\n",
            "3785\n",
            "3786\n",
            "3787\n",
            "3788\n",
            "3789\n",
            "3790\n",
            "3791\n",
            "3792\n",
            "3793\n",
            "3794\n",
            "3795\n",
            "3796\n",
            "3797\n",
            "3798\n",
            "3799\n",
            "3800\n",
            "3801\n",
            "3802\n",
            "3803\n",
            "3804\n",
            "3805\n",
            "3806\n",
            "3807\n",
            "3808\n",
            "3809\n",
            "3810\n",
            "3811\n",
            "3812\n",
            "3813\n",
            "3814\n",
            "3815\n",
            "3816\n",
            "3817\n",
            "3818\n",
            "3819\n",
            "3820\n",
            "3821\n",
            "3822\n",
            "3823\n",
            "3824\n",
            "3825\n",
            "3826\n",
            "3827\n",
            "3828\n",
            "3829\n",
            "3830\n",
            "3831\n",
            "3832\n",
            "3833\n",
            "3834\n",
            "3835\n",
            "3836\n",
            "3837\n",
            "3838\n",
            "3839\n",
            "3840\n",
            "3841\n",
            "3842\n",
            "3843\n",
            "3844\n",
            "3845\n",
            "3846\n",
            "3847\n",
            "3848\n",
            "3849\n",
            "3850\n",
            "3851\n",
            "3852\n",
            "3853\n",
            "3854\n",
            "3855\n",
            "3856\n",
            "3857\n",
            "3858\n",
            "3859\n",
            "3860\n",
            "3861\n",
            "3862\n",
            "3863\n",
            "3864\n",
            "3865\n",
            "3866\n",
            "3867\n",
            "3868\n",
            "3869\n",
            "3870\n",
            "3871\n",
            "3872\n",
            "3873\n",
            "3874\n",
            "3875\n",
            "3876\n",
            "3877\n",
            "3878\n",
            "3879\n",
            "3880\n",
            "3881\n",
            "3882\n",
            "3883\n",
            "3884\n",
            "3885\n",
            "3886\n",
            "3887\n",
            "3888\n",
            "3889\n",
            "3890\n",
            "3891\n",
            "3892\n",
            "3893\n",
            "3894\n",
            "3895\n",
            "3896\n",
            "3897\n",
            "3898\n",
            "3899\n",
            "3900\n",
            "3901\n",
            "3902\n",
            "3903\n",
            "3904\n",
            "3905\n",
            "3906\n",
            "3907\n",
            "3908\n",
            "3909\n",
            "3910\n",
            "3911\n",
            "3912\n",
            "3913\n",
            "3914\n",
            "3915\n",
            "3916\n",
            "3917\n",
            "3918\n",
            "3919\n",
            "3920\n",
            "3921\n",
            "3922\n",
            "3923\n",
            "3924\n",
            "3925\n",
            "3926\n",
            "3927\n",
            "3928\n",
            "3929\n",
            "3930\n",
            "3931\n",
            "3932\n",
            "3933\n",
            "3934\n",
            "3935\n",
            "3936\n",
            "3937\n",
            "3938\n",
            "3939\n",
            "3940\n",
            "3941\n",
            "3942\n",
            "3943\n",
            "3944\n",
            "3945\n",
            "3946\n",
            "3947\n",
            "3948\n",
            "3949\n",
            "3950\n",
            "3951\n",
            "3952\n",
            "3953\n",
            "3954\n",
            "3955\n",
            "3956\n",
            "3957\n",
            "3958\n",
            "3959\n",
            "3960\n",
            "3961\n",
            "3962\n",
            "3963\n",
            "3964\n",
            "3965\n",
            "3966\n",
            "3967\n",
            "3968\n",
            "3969\n",
            "3970\n",
            "3971\n",
            "3972\n",
            "3973\n",
            "3974\n",
            "3975\n",
            "3976\n",
            "3977\n",
            "3978\n",
            "3979\n",
            "3980\n",
            "3981\n",
            "3982\n",
            "3983\n",
            "3984\n",
            "3985\n",
            "3986\n",
            "3987\n",
            "3988\n",
            "3989\n",
            "3990\n",
            "3991\n",
            "3992\n",
            "3993\n",
            "3994\n",
            "3995\n",
            "3996\n",
            "3997\n",
            "3998\n",
            "3999\n",
            "4000\n",
            "4001\n",
            "4002\n",
            "4003\n",
            "4004\n",
            "4005\n",
            "4006\n",
            "4007\n",
            "4008\n",
            "4009\n",
            "4010\n",
            "4011\n",
            "4012\n",
            "4013\n",
            "4014\n",
            "4015\n",
            "4016\n",
            "4017\n",
            "4018\n",
            "4019\n",
            "4020\n",
            "4021\n",
            "4022\n",
            "4023\n",
            "4024\n",
            "4025\n",
            "4026\n",
            "4027\n",
            "4028\n",
            "4029\n",
            "4030\n",
            "4031\n",
            "4032\n",
            "4033\n",
            "4034\n",
            "4035\n",
            "4036\n",
            "4037\n",
            "4038\n",
            "4039\n",
            "4040\n",
            "4041\n",
            "4042\n",
            "4043\n",
            "4044\n",
            "4045\n",
            "4046\n",
            "4047\n",
            "4048\n",
            "4049\n",
            "4050\n",
            "4051\n",
            "4052\n",
            "4053\n",
            "4054\n",
            "4055\n",
            "4056\n",
            "4057\n",
            "4058\n",
            "4059\n",
            "4060\n",
            "4061\n",
            "4062\n",
            "4063\n",
            "4064\n",
            "4065\n",
            "4066\n",
            "4067\n",
            "4068\n",
            "4069\n",
            "4070\n",
            "4071\n",
            "4072\n",
            "4073\n",
            "4074\n",
            "4075\n",
            "4076\n",
            "4077\n",
            "4078\n",
            "4079\n",
            "4080\n",
            "4081\n",
            "4082\n",
            "4083\n",
            "4084\n",
            "4085\n",
            "4086\n",
            "4087\n",
            "4088\n",
            "4089\n",
            "4090\n",
            "4091\n",
            "4092\n",
            "4093\n",
            "4094\n",
            "4095\n",
            "4096\n",
            "4097\n",
            "4098\n",
            "4099\n",
            "4100\n",
            "4101\n",
            "4102\n",
            "4103\n",
            "4104\n",
            "4105\n",
            "4106\n",
            "4107\n",
            "4108\n",
            "4109\n",
            "4110\n",
            "4111\n",
            "4112\n",
            "4113\n",
            "4114\n",
            "4115\n",
            "4116\n",
            "4117\n",
            "4118\n",
            "4119\n",
            "4120\n",
            "4121\n",
            "4122\n",
            "4123\n",
            "4124\n",
            "4125\n",
            "4126\n",
            "4127\n",
            "4128\n",
            "4129\n",
            "4130\n",
            "4131\n",
            "4132\n",
            "4133\n",
            "4134\n",
            "4135\n",
            "4136\n",
            "4137\n",
            "4138\n",
            "4139\n",
            "4140\n",
            "4141\n",
            "4142\n",
            "4143\n",
            "4144\n",
            "4145\n",
            "4146\n",
            "4147\n",
            "4148\n",
            "4149\n",
            "4150\n",
            "4151\n",
            "4152\n",
            "4153\n",
            "4154\n",
            "4155\n",
            "4156\n",
            "4157\n",
            "4158\n",
            "4159\n",
            "4160\n",
            "4161\n",
            "4162\n",
            "4163\n",
            "4164\n",
            "4165\n",
            "4166\n",
            "4167\n",
            "4168\n",
            "4169\n",
            "4170\n",
            "4171\n",
            "4172\n",
            "4173\n",
            "4174\n",
            "4175\n",
            "4176\n",
            "4177\n",
            "4178\n",
            "4179\n",
            "4180\n",
            "4181\n",
            "4182\n",
            "4183\n",
            "4184\n",
            "4185\n",
            "4186\n",
            "4187\n",
            "4188\n",
            "4189\n",
            "4190\n",
            "4191\n",
            "4192\n",
            "4193\n",
            "4194\n",
            "4195\n",
            "4196\n",
            "4197\n",
            "4198\n",
            "4199\n",
            "4200\n",
            "4201\n",
            "4202\n",
            "4203\n",
            "4204\n",
            "4205\n",
            "4206\n",
            "4207\n",
            "4208\n",
            "4209\n",
            "4210\n",
            "4211\n",
            "4212\n",
            "4213\n",
            "4214\n",
            "4215\n",
            "4216\n",
            "4217\n",
            "4218\n",
            "4219\n",
            "4220\n",
            "4221\n",
            "4222\n",
            "4223\n",
            "4224\n",
            "4225\n",
            "4226\n",
            "4227\n",
            "4228\n",
            "4229\n",
            "4230\n",
            "4231\n",
            "4232\n",
            "4233\n",
            "4234\n",
            "4235\n",
            "4236\n",
            "4237\n",
            "4238\n",
            "4239\n",
            "4240\n",
            "4241\n",
            "4242\n",
            "4243\n",
            "4244\n",
            "4245\n",
            "4246\n",
            "4247\n",
            "4248\n",
            "4249\n",
            "4250\n",
            "4251\n",
            "4252\n",
            "4253\n",
            "4254\n",
            "4255\n",
            "4256\n",
            "4257\n",
            "4258\n",
            "4259\n",
            "4260\n",
            "4261\n",
            "4262\n",
            "4263\n",
            "4264\n",
            "4265\n",
            "4266\n",
            "4267\n",
            "4268\n",
            "4269\n",
            "4270\n",
            "4271\n",
            "4272\n",
            "4273\n",
            "4274\n",
            "4275\n",
            "4276\n",
            "4277\n",
            "4278\n",
            "4279\n",
            "4280\n",
            "4281\n",
            "4282\n",
            "4283\n",
            "4284\n",
            "4285\n",
            "4286\n",
            "4287\n",
            "4288\n",
            "4289\n",
            "4290\n",
            "4291\n",
            "4292\n",
            "4293\n",
            "4294\n",
            "4295\n",
            "4296\n",
            "4297\n",
            "4298\n",
            "4299\n",
            "4300\n",
            "4301\n",
            "4302\n",
            "4303\n",
            "4304\n",
            "4305\n",
            "4306\n",
            "4307\n",
            "4308\n",
            "4309\n",
            "4310\n",
            "4311\n",
            "4312\n",
            "4313\n",
            "4314\n",
            "4315\n",
            "4316\n",
            "4317\n",
            "4318\n",
            "4319\n",
            "4320\n",
            "4321\n",
            "4322\n",
            "4323\n",
            "4324\n",
            "4325\n",
            "4326\n",
            "4327\n",
            "4328\n",
            "4329\n",
            "4330\n",
            "4331\n",
            "4332\n",
            "4333\n",
            "4334\n",
            "4335\n",
            "4336\n",
            "4337\n",
            "4338\n",
            "4339\n",
            "4340\n",
            "4341\n",
            "4342\n",
            "4343\n",
            "4344\n",
            "4345\n",
            "4346\n",
            "4347\n",
            "4348\n",
            "4349\n",
            "4350\n",
            "4351\n",
            "4352\n",
            "4353\n",
            "4354\n",
            "4355\n",
            "4356\n",
            "4357\n",
            "4358\n",
            "4359\n",
            "4360\n",
            "4361\n",
            "4362\n",
            "4363\n",
            "4364\n",
            "4365\n",
            "4366\n",
            "4367\n",
            "4368\n",
            "4369\n",
            "4370\n",
            "4371\n",
            "4372\n",
            "4373\n",
            "4374\n",
            "4375\n",
            "4376\n",
            "4377\n",
            "4378\n",
            "4379\n",
            "4380\n",
            "4381\n",
            "4382\n",
            "4383\n",
            "4384\n",
            "4385\n",
            "4386\n",
            "4387\n",
            "4388\n",
            "4389\n",
            "4390\n",
            "4391\n",
            "4392\n",
            "4393\n",
            "4394\n",
            "4395\n",
            "4396\n",
            "4397\n",
            "4398\n",
            "4399\n",
            "4400\n",
            "4401\n",
            "4402\n",
            "4403\n",
            "4404\n",
            "4405\n",
            "4406\n",
            "4407\n",
            "4408\n",
            "4409\n",
            "4410\n",
            "4411\n",
            "4412\n",
            "4413\n",
            "4414\n",
            "4415\n",
            "4416\n",
            "4417\n",
            "4418\n",
            "4419\n",
            "4420\n",
            "4421\n",
            "4422\n",
            "4423\n",
            "4424\n",
            "4425\n",
            "4426\n",
            "4427\n",
            "4428\n",
            "4429\n",
            "4430\n",
            "4431\n",
            "4432\n",
            "4433\n",
            "4434\n",
            "4435\n",
            "4436\n",
            "4437\n",
            "4438\n",
            "4439\n",
            "4440\n",
            "4441\n",
            "4442\n",
            "4443\n",
            "4444\n",
            "4445\n",
            "4446\n",
            "4447\n",
            "4448\n",
            "4449\n",
            "4450\n",
            "4451\n",
            "4452\n",
            "4453\n",
            "4454\n",
            "4455\n",
            "4456\n",
            "4457\n",
            "4458\n",
            "4459\n",
            "4460\n",
            "4461\n",
            "4462\n",
            "4463\n",
            "4464\n",
            "4465\n",
            "4466\n",
            "4467\n",
            "4468\n",
            "4469\n",
            "4470\n",
            "4471\n",
            "4472\n",
            "4473\n",
            "4474\n",
            "4475\n",
            "4476\n",
            "4477\n",
            "4478\n",
            "4479\n",
            "4480\n",
            "4481\n",
            "4482\n",
            "4483\n",
            "4484\n",
            "4485\n",
            "4486\n",
            "4487\n",
            "4488\n",
            "4489\n",
            "4490\n",
            "4491\n",
            "4492\n",
            "4493\n",
            "4494\n",
            "4495\n",
            "4496\n",
            "4497\n",
            "4498\n",
            "4499\n",
            "4500\n",
            "4501\n",
            "4502\n",
            "4503\n",
            "4504\n",
            "4505\n",
            "4506\n",
            "4507\n",
            "4508\n",
            "4509\n",
            "4510\n",
            "4511\n",
            "4512\n",
            "4513\n",
            "4514\n",
            "4515\n",
            "4516\n",
            "4517\n",
            "4518\n",
            "4519\n",
            "4520\n",
            "4521\n",
            "4522\n",
            "4523\n",
            "4524\n",
            "4525\n",
            "4526\n",
            "4527\n",
            "4528\n",
            "4529\n",
            "4530\n",
            "4531\n",
            "4532\n",
            "4533\n",
            "4534\n",
            "4535\n",
            "4536\n",
            "4537\n",
            "4538\n",
            "4539\n",
            "4540\n",
            "4541\n",
            "4542\n",
            "4543\n",
            "4544\n",
            "4545\n",
            "4546\n",
            "4547\n",
            "4548\n",
            "4549\n",
            "4550\n",
            "4551\n",
            "4552\n",
            "4553\n",
            "4554\n",
            "4555\n",
            "4556\n",
            "4557\n",
            "4558\n",
            "4559\n",
            "4560\n",
            "4561\n",
            "4562\n",
            "4563\n",
            "4564\n",
            "4565\n",
            "4566\n",
            "4567\n",
            "4568\n",
            "4569\n",
            "4570\n",
            "4571\n",
            "4572\n",
            "4573\n",
            "4574\n",
            "4575\n",
            "4576\n",
            "4577\n",
            "4578\n",
            "4579\n",
            "4580\n",
            "4581\n",
            "4582\n",
            "4583\n",
            "4584\n",
            "4585\n",
            "4586\n",
            "4587\n",
            "4588\n",
            "4589\n",
            "4590\n",
            "4591\n",
            "4592\n",
            "4593\n",
            "4594\n",
            "4595\n",
            "4596\n",
            "4597\n",
            "4598\n",
            "4599\n",
            "4600\n",
            "4601\n",
            "4602\n",
            "4603\n",
            "4604\n",
            "4605\n",
            "4606\n",
            "4607\n",
            "4608\n",
            "4609\n",
            "4610\n",
            "4611\n",
            "4612\n",
            "4613\n",
            "4614\n",
            "4615\n",
            "4616\n",
            "4617\n",
            "4618\n",
            "4619\n",
            "4620\n",
            "4621\n",
            "4622\n",
            "4623\n",
            "4624\n",
            "4625\n",
            "4626\n",
            "4627\n",
            "4628\n",
            "4629\n",
            "4630\n",
            "4631\n",
            "4632\n",
            "4633\n",
            "4634\n",
            "4635\n",
            "4636\n",
            "4637\n",
            "4638\n",
            "4639\n",
            "4640\n",
            "4641\n",
            "4642\n",
            "4643\n",
            "4644\n",
            "4645\n",
            "4646\n",
            "4647\n",
            "4648\n",
            "4649\n",
            "4650\n",
            "4651\n",
            "4652\n",
            "4653\n",
            "4654\n",
            "4655\n",
            "4656\n",
            "4657\n",
            "4658\n",
            "4659\n",
            "4660\n",
            "4661\n",
            "4662\n",
            "4663\n",
            "4664\n",
            "4665\n",
            "4666\n",
            "4667\n",
            "4668\n",
            "4669\n",
            "4670\n",
            "4671\n",
            "4672\n",
            "4673\n",
            "4674\n",
            "4675\n",
            "4676\n",
            "4677\n",
            "4678\n",
            "4679\n",
            "4680\n",
            "4681\n",
            "4682\n",
            "4683\n",
            "4684\n",
            "4685\n",
            "4686\n",
            "4687\n",
            "4688\n",
            "4689\n",
            "4690\n",
            "4691\n",
            "4692\n",
            "4693\n",
            "4694\n",
            "4695\n",
            "4696\n",
            "4697\n",
            "4698\n",
            "4699\n",
            "4700\n",
            "4701\n",
            "4702\n",
            "4703\n",
            "4704\n",
            "4705\n",
            "4706\n",
            "4707\n",
            "4708\n",
            "4709\n",
            "4710\n",
            "4711\n",
            "4712\n",
            "4713\n",
            "4714\n",
            "4715\n",
            "4716\n",
            "4717\n",
            "4718\n",
            "4719\n",
            "4720\n",
            "4721\n",
            "4722\n",
            "4723\n",
            "4724\n",
            "4725\n",
            "4726\n",
            "4727\n",
            "4728\n",
            "4729\n",
            "4730\n",
            "4731\n",
            "4732\n",
            "4733\n",
            "4734\n",
            "4735\n",
            "4736\n",
            "4737\n",
            "4738\n",
            "4739\n",
            "4740\n",
            "4741\n",
            "4742\n",
            "4743\n",
            "4744\n",
            "4745\n",
            "4746\n",
            "4747\n",
            "4748\n",
            "4749\n",
            "4750\n",
            "4751\n",
            "4752\n",
            "4753\n",
            "4754\n",
            "4755\n",
            "4756\n",
            "4757\n",
            "4758\n",
            "4759\n",
            "4760\n",
            "4761\n",
            "4762\n",
            "4763\n",
            "4764\n",
            "4765\n",
            "4766\n",
            "4767\n",
            "4768\n",
            "4769\n",
            "4770\n",
            "4771\n",
            "4772\n",
            "4773\n",
            "4774\n",
            "4775\n",
            "4776\n",
            "4777\n",
            "4778\n",
            "4779\n",
            "4780\n",
            "4781\n",
            "4782\n",
            "4783\n",
            "4784\n",
            "4785\n",
            "4786\n",
            "4787\n",
            "4788\n",
            "4789\n",
            "4790\n",
            "4791\n",
            "4792\n",
            "4793\n",
            "4794\n",
            "4795\n",
            "4796\n",
            "4797\n",
            "4798\n",
            "4799\n",
            "4800\n",
            "4801\n",
            "4802\n",
            "4803\n",
            "4804\n",
            "4805\n",
            "4806\n",
            "4807\n",
            "4808\n",
            "4809\n",
            "4810\n",
            "4811\n",
            "4812\n",
            "4813\n",
            "4814\n",
            "4815\n",
            "4816\n",
            "4817\n",
            "4818\n",
            "4819\n",
            "4820\n",
            "4821\n",
            "4822\n",
            "4823\n",
            "4824\n",
            "4825\n",
            "4826\n",
            "4827\n",
            "4828\n",
            "4829\n",
            "4830\n",
            "4831\n",
            "4832\n",
            "4833\n",
            "4834\n",
            "4835\n",
            "4836\n",
            "4837\n",
            "4838\n",
            "4839\n",
            "4840\n",
            "4841\n",
            "4842\n",
            "4843\n",
            "4844\n",
            "4845\n",
            "4846\n",
            "4847\n",
            "4848\n",
            "4849\n",
            "4850\n",
            "4851\n",
            "4852\n",
            "4853\n",
            "4854\n",
            "4855\n",
            "4856\n",
            "4857\n",
            "4858\n",
            "4859\n",
            "4860\n",
            "4861\n",
            "4862\n",
            "4863\n",
            "4864\n",
            "4865\n",
            "4866\n",
            "4867\n",
            "4868\n",
            "4869\n",
            "4870\n",
            "4871\n",
            "4872\n",
            "4873\n",
            "4874\n",
            "4875\n",
            "4876\n",
            "4877\n",
            "4878\n",
            "4879\n",
            "4880\n",
            "4881\n",
            "4882\n",
            "4883\n",
            "4884\n",
            "4885\n",
            "4886\n",
            "4887\n",
            "4888\n",
            "4889\n",
            "4890\n",
            "4891\n",
            "4892\n",
            "4893\n",
            "4894\n",
            "4895\n",
            "4896\n",
            "4897\n",
            "4898\n",
            "4899\n",
            "4900\n",
            "4901\n",
            "4902\n",
            "4903\n",
            "4904\n",
            "4905\n",
            "4906\n",
            "4907\n",
            "4908\n",
            "4909\n",
            "4910\n",
            "4911\n",
            "4912\n",
            "4913\n",
            "4914\n",
            "4915\n",
            "4916\n",
            "4917\n",
            "4918\n",
            "4919\n",
            "4920\n",
            "4921\n",
            "4922\n",
            "4923\n",
            "4924\n",
            "4925\n",
            "4926\n",
            "4927\n",
            "4928\n",
            "4929\n",
            "4930\n",
            "4931\n",
            "4932\n",
            "4933\n",
            "4934\n",
            "4935\n",
            "4936\n",
            "4937\n",
            "4938\n",
            "4939\n",
            "4940\n",
            "4941\n",
            "4942\n",
            "4943\n",
            "4944\n",
            "4945\n",
            "4946\n",
            "4947\n",
            "4948\n",
            "4949\n",
            "4950\n",
            "4951\n",
            "4952\n",
            "4953\n",
            "4954\n",
            "4955\n",
            "4956\n",
            "4957\n",
            "4958\n",
            "4959\n",
            "4960\n",
            "4961\n",
            "4962\n",
            "4963\n",
            "4964\n",
            "4965\n",
            "4966\n",
            "4967\n",
            "4968\n",
            "4969\n",
            "4970\n",
            "4971\n",
            "4972\n",
            "4973\n",
            "4974\n",
            "4975\n",
            "4976\n",
            "4977\n",
            "4978\n",
            "4979\n",
            "4980\n",
            "4981\n",
            "4982\n",
            "4983\n",
            "4984\n",
            "4985\n",
            "4986\n",
            "4987\n",
            "4988\n",
            "4989\n",
            "4990\n",
            "4991\n",
            "4992\n",
            "4993\n",
            "4994\n",
            "4995\n",
            "4996\n",
            "4997\n",
            "4998\n",
            "4999\n",
            "Best Seed: 1268\n",
            "Best Test MSE: 0.5741963041966017\n",
            "Best Test RMSE: 0.7577574177773528\n",
            "Best Test MAE: 0.8149767211119482\n",
            "Best Test R^2: 0.5086227035390858\n",
            "Best Test MAPE: 62.80514873056167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING MODELS**"
      ],
      "metadata": {
        "id": "H2ddA7kQ4fDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import joblib\n",
        "\n",
        "# # Function to load the model and predict BOD from a CSV file\n",
        "# def predict_bod_from_csv(input_csv):\n",
        "#     # Load the trained model\n",
        "#     model = joblib.load('/content/svr_model.pkl')  # Use the path to your saved model\n",
        "\n",
        "#     # Define the feature columns\n",
        "#     feature_columns = [\n",
        "#         'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "#         'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "#         'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "#         'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "#     ]\n",
        "\n",
        "#     # Read the input data from CSV\n",
        "#     input_df = pd.read_csv(input_csv)\n",
        "\n",
        "#     # Ensure the input data has all necessary columns\n",
        "#     missing_cols = set(feature_columns) - set(input_df.columns)\n",
        "#     if missing_cols:\n",
        "#         raise ValueError(f\"Missing columns in input data: {missing_cols}\")\n",
        "\n",
        "#     # Predict BOD\n",
        "#     predictions = model.predict(input_df[feature_columns])\n",
        "\n",
        "#     # Print only the predictions\n",
        "#     for prediction in predictions:\n",
        "#         print(prediction)\n",
        "\n",
        "# # Example usage\n",
        "# input_csv = 'Book1.csv'  # Replace with the path to your CSV file\n",
        "# predict_bod_from_csv(input_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267bRO_DmypV",
        "outputId": "5af1474f-8501-4ced-b844-3c4dee2defa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.718974244498855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow.keras.models import load_model\n",
        "\n",
        "# # Load the saved model\n",
        "# model = load_model('/content/no_opt_mlp_model.h5')\n",
        "\n",
        "# # Load new input data from a CSV file\n",
        "# new_input_data = pd.read_csv('Book1.csv')\n",
        "\n",
        "# # Define the feature columns (should be the same as the ones used in training)\n",
        "# feature_columns = [\n",
        "#     'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "#     'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "#     'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "#     'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "# ]\n",
        "\n",
        "# # Extract features from the new input data\n",
        "# X_new = new_input_data[feature_columns]\n",
        "\n",
        "# # Load the scaler used for standardization\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# # Assume the scaler has been previously fitted to the training data\n",
        "# # Here we fit the scaler on the combined original training data as an example\n",
        "# # In practice, you should load the already fitted scaler from your training phase\n",
        "# combined_training_data = pd.concat([tikob_fil[feature_columns], gunao_fil[feature_columns]], axis=0)\n",
        "# scaler.fit(combined_training_data)\n",
        "\n",
        "# # Standardize the new input data\n",
        "# X_new_scaled = scaler.transform(X_new)\n",
        "\n",
        "# # Make predictions using the loaded model\n",
        "# predictions = model.predict(X_new_scaled).flatten()\n",
        "\n",
        "# # Output predictions to the console\n",
        "# print('Predictions for new input data:')\n",
        "# print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzYjNV2AoWMo",
        "outputId": "7fed29d5-9221-4e36-bc91-31fa311846f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "Predictions for new input data:\n",
            "[1.0552795]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREDICTION TO CSV PICKLE MODELS**"
      ],
      "metadata": {
        "id": "Y9z8t9W44Cem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Regression**"
      ],
      "metadata": {
        "id": "Us7vAmkvA9uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Genetics Algorithm**"
      ],
      "metadata": {
        "id": "SECkQvdseAGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from two CSV files\n",
        "def predict_bod_from_two_csvs(csv1, csv2, output_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/All_Shit/SVR/SVR_Genetics.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read both input data CSVs\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "\n",
        "    # Ensure both input data have all necessary columns\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "\n",
        "    # Sample 15 random rows from both dataframes\n",
        "    sampled_df1 = df1.sample(n=sample_size, random_state=40)\n",
        "    sampled_df2 = df2.sample(n=sample_size, random_state=40)\n",
        "\n",
        "    # Concatenate the two sampled dataframes\n",
        "    combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "    # Check if the actual BOD column exists\n",
        "    if 'BOD (mg/L)' not in combined_df.columns:\n",
        "        raise ValueError(\"The input data must contain an 'BOD (mg/L)' column for actual values.\")\n",
        "\n",
        "    # Predict BOD for the combined data (only feature columns)\n",
        "    predictions = model.predict(combined_df[feature_columns])\n",
        "\n",
        "    # Add predictions to the combined DataFrame\n",
        "    combined_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Keep the actual BOD values alongside predictions\n",
        "    output_df = combined_df[['BOD (mg/L)', 'Predicted_BOD']]\n",
        "\n",
        "    # Save the DataFrame with actual and predicted BOD to a new CSV file\n",
        "    output_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "output_csv = '/content/All_Shit/Exported_CSV/Combined_Pred_SVR_GA.csv'  # Output CSV file name\n",
        "predict_bod_from_two_csvs(csv1, csv2, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjZX8CaOSlPK",
        "outputId": "3dcd88d4-4f46-4776-b775-5c4671eb59ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/All_Shit/Exported_CSV/Combined_Pred_SVR_GA.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tiered Algorithm**"
      ],
      "metadata": {
        "id": "MqtW3sCueHcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from two CSV files\n",
        "def predict_bod_from_two_csvs(csv1, csv2, output_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/All_Shit/SVR/SVR_Tiered.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read both input data CSVs\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "\n",
        "    # Ensure both input data have all necessary columns\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "\n",
        "    # Sample 15 random rows from both dataframes\n",
        "    sampled_df1 = df1.sample(n=sample_size, random_state=41)\n",
        "    sampled_df2 = df2.sample(n=sample_size, random_state=41)\n",
        "\n",
        "    # Print sampled data for debugging\n",
        "    print(\"Sampled data from first CSV:\")\n",
        "    print(sampled_df1.head())\n",
        "    print(\"Sampled data from second CSV:\")\n",
        "    print(sampled_df2.head())\n",
        "\n",
        "    # Concatenate the two sampled dataframes\n",
        "    combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "    # Predict BOD for the combined data\n",
        "    predictions = model.predict(combined_df[feature_columns])\n",
        "\n",
        "    # Print predictions for debugging\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n",
        "\n",
        "    # Add predictions to the combined DataFrame\n",
        "    combined_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Save the combined DataFrame with predictions to a new CSV file\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "output_csv = '/content/All_Shit/Exported_CSV/Combined_Pred_SVR_TA.csv'  # Output CSV file name\n",
        "predict_bod_from_two_csvs(csv1, csv2, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHCoJhXG_vL1",
        "outputId": "8e761160-73f7-47f0-903a-dcce595879ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled data from first CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "278  7.33       1.26      114.20            0.12        228.00       26.4   \n",
            "341  7.27       3.10      121.20            0.12        242.00       26.5   \n",
            "364  7.04       2.11      125.20            0.20        248.00       26.5   \n",
            "61   7.45       0.42      108.16            0.11        217.07       26.7   \n",
            "261  7.34       0.52      107.80            0.11        216.00       26.8   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "278        0.83      0.0018      0.048      0.041  ...      3.8      4.7   \n",
            "341        1.12      0.0035      0.047      0.063  ...      2.9      4.6   \n",
            "364        1.13      0.0143      0.034      0.033  ...      3.4      4.8   \n",
            "61         1.94      0.0388      0.042      0.044  ...      3.7      3.6   \n",
            "261        2.31      0.0300      0.048      0.046  ...      3.8      3.7   \n",
            "\n",
            "          DATE     MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "278   9/2/2023  February  2023        3          3      Bottom  13°57.677'   \n",
            "341  6/15/2023      June  2023        4          3      Bottom  13°57.870'   \n",
            "364  8/23/2023    August  2023        2          2      Bottom  13°57.686'   \n",
            "61   1/19/2023   January  2023        1          2     Surface  13°57.872'   \n",
            "261  1/19/2023   January  2023        3          1      Bottom  13°57.666'   \n",
            "\n",
            "      Longtitude  \n",
            "278  121°18.489'  \n",
            "341  121°18.491'  \n",
            "364  121°18.266'  \n",
            "61   121°18.299'  \n",
            "261  121°18.482'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Sampled data from second CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "96   9.60      18.45        87.6            0.10         175.0       31.6   \n",
            "63   7.56       2.23        89.5            0.10         179.0       27.8   \n",
            "165  9.40       5.59        81.9            0.08         163.4       31.5   \n",
            "191  9.36       8.52       111.2            0.10         222.0       31.5   \n",
            "68   7.46       1.53        89.3            0.10         178.9       27.8   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "96         3.77      0.0094      0.041      0.061  ...      3.3      5.2   \n",
            "63         3.03      0.0041      0.055      0.024  ...      3.6      3.6   \n",
            "165        3.80      0.0018      0.039      0.065  ...      4.8      7.9   \n",
            "191        5.25      0.0049      0.038      0.062  ...      3.1      5.5   \n",
            "68         1.73      0.0221      0.053      0.024  ...      3.6      3.3   \n",
            "\n",
            "          DATE      MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "96   3/16/2023      March  2023        3          1     Surface  14°00.046'   \n",
            "63   1/26/2023    January  2023        2          1     Surface  14°00.067'   \n",
            "165  8/29/2023     August  2023        1          1     Surface  14°00.171'   \n",
            "191  9/18/2023  September  2023        4          3     Surface  14°00.153'   \n",
            "68   1/26/2023    January  2023        3          3     Surface  14°00.036'   \n",
            "\n",
            "      Longtitude  \n",
            "96   121°22.342'  \n",
            "63   121°22.191'  \n",
            "165  121°22.240'  \n",
            "191  121°22.358'  \n",
            "68   121°22.350'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Predictions:\n",
            "[1.70276 1.61188 1.69516 1.91484 1.65964 1.26608 1.41396 1.72064 2.03532\n",
            " 1.11372 1.61136 1.08532 1.91844 1.37424 0.80432 3.6552  2.77848 3.90536\n",
            " 4.62644 1.85592 0.84268 4.78456 3.10824 0.6106  3.30692 0.82236 2.98988\n",
            " 0.50392 0.77216 2.64124]\n",
            "Predictions saved to /content/All_Shit/Exported_CSV/Combined_Pred_SVR_TA.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Least Square Algorithm**"
      ],
      "metadata": {
        "id": "4afjmYnReLGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from two CSV files\n",
        "def predict_bod_from_two_csvs(csv1, csv2, output_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/All_Shit/SVR/SVR_Least.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read both input data CSVs\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "\n",
        "    # Ensure both input data have all necessary columns\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "\n",
        "    # Sample 15 random rows from both dataframes\n",
        "    sampled_df1 = df1.sample(n=sample_size, random_state=42)\n",
        "    sampled_df2 = df2.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Print sampled data for debugging\n",
        "    print(\"Sampled data from first CSV:\")\n",
        "    print(sampled_df1.head())\n",
        "    print(\"Sampled data from second CSV:\")\n",
        "    print(sampled_df2.head())\n",
        "\n",
        "    # Concatenate the two sampled dataframes\n",
        "    combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "    # Predict BOD for the combined data\n",
        "    predictions = model.predict(combined_df[feature_columns])\n",
        "\n",
        "    # Print predictions for debugging\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n",
        "\n",
        "    # Add predictions to the combined DataFrame\n",
        "    combined_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Save the combined DataFrame with predictions to a new CSV file\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "output_csv = '/content/All_Shit/Exported_CSV/Combined_Pred_SVR_LS.csv'  # Output CSV file name\n",
        "predict_bod_from_two_csvs(csv1, csv2, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfL-bzU9_0lD",
        "outputId": "6b1f0a7e-b710-4763-cc76-49c05402e23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled data from first CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "9    8.49       6.59      130.68            0.16        259.02       30.8   \n",
            "42   8.84       9.49       94.15            0.11        189.10       29.7   \n",
            "33   8.87       9.74       31.71            0.04        157.90       30.1   \n",
            "311  7.38       1.39      116.10            0.65        233.00       26.5   \n",
            "272  7.57       1.24      118.90            0.12        222.00       26.4   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "9          1.26      0.0072      0.032      0.041  ...      3.4      6.2   \n",
            "42         0.86      0.0443      0.038      0.174  ...      3.3      5.8   \n",
            "33         0.77      0.0402      0.071      0.189  ...      3.6      4.9   \n",
            "311        3.40      0.0011      0.039      0.042  ...      3.2      4.6   \n",
            "272        1.05      0.0248      0.043      0.069  ...      3.1      4.3   \n",
            "\n",
            "           DATE      MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "9     9/21/2022  September  2022        4          1     Surface  13°57.869'   \n",
            "42   11/24/2022   November  2022        5          1     Surface  13°57.767'   \n",
            "33   11/24/2022   November  2022        2          1     Surface  13°57.678'   \n",
            "311   4/19/2023      April  2023        4          3      Bottom  13°57.872'   \n",
            "272    9/2/2023   February  2023        1          3      Bottom  13°57.890'   \n",
            "\n",
            "      Longtitude  \n",
            "9    121°18.485'  \n",
            "42   121°18.403'  \n",
            "33   121°18.262'  \n",
            "311  121°18.489'  \n",
            "272  121°18.293'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Sampled data from second CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "138  9.38       9.00        78.8            0.07         156.7       33.0   \n",
            "16   7.86       4.79       115.2            0.14         229.2       30.0   \n",
            "155  9.32       7.14        81.6            0.08         163.1       30.8   \n",
            "96   9.60      18.45        87.6            0.10         175.0       31.6   \n",
            "68   7.46       1.53        89.3            0.10         178.9       27.8   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "138        2.72      0.0112      0.042      0.021  ...      2.8      3.8   \n",
            "16         0.61      0.0025      0.016      0.019  ...      3.8      5.5   \n",
            "155        3.54      0.0014      0.035      0.026  ...      3.5      4.5   \n",
            "96         3.77      0.0094      0.041      0.061  ...      3.3      5.2   \n",
            "68         1.73      0.0221      0.053      0.024  ...      3.6      3.3   \n",
            "\n",
            "           DATE    MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "138   6/23/2023     June  2023        2          1     Surface  14°00.082'   \n",
            "16   10/26/2022  October  2022        1          2     Surface  14°00.164'   \n",
            "155   7/19/2023     July  2023        2          3     Surface  14°00.062'   \n",
            "96    3/16/2023    March  2023        3          1     Surface  14°00.046'   \n",
            "68    1/26/2023  January  2023        3          3     Surface  14°00.036'   \n",
            "\n",
            "      Longtitude  \n",
            "138  121°22.191'  \n",
            "16   121°22.243'  \n",
            "155  121°22.185'  \n",
            "96   121°22.342'  \n",
            "68   121°22.350'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Predictions:\n",
            "[1.28176 1.07188 1.22664 2.90188 1.64628 1.93028 1.099   1.71496 1.68776\n",
            " 3.2532  1.76244 1.20512 1.01896 2.04564 1.70276 1.94056 0.58704 3.49952\n",
            " 3.6552  1.85592 3.28028 0.57196 0.5606  3.79988 3.76984 3.11756 0.4966\n",
            " 1.17448 0.84268 2.62756]\n",
            "Predictions saved to /content/All_Shit/Exported_CSV/Combined_Pred_SVR_LS.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "AyBlGAj1AnsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Genetics Algorithm**"
      ],
      "metadata": {
        "id": "Fvs1yHdIfS_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from two CSV files\n",
        "def predict_bod_from_two_csvs(csv1, csv2, output_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/RF/RF_Genetics.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read both input data CSVs\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "\n",
        "    # Ensure both input data have all necessary columns\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "\n",
        "    # Sample 15 random rows from both dataframes\n",
        "    sampled_df1 = df1.sample(n=sample_size, random_state=42)\n",
        "    sampled_df2 = df2.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Print sampled data for debugging\n",
        "    print(\"Sampled data from first CSV:\")\n",
        "    print(sampled_df1.head())\n",
        "    print(\"Sampled data from second CSV:\")\n",
        "    print(sampled_df2.head())\n",
        "\n",
        "    # Concatenate the two sampled dataframes\n",
        "    combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "    # Predict BOD for the combined data\n",
        "    predictions = model.predict(combined_df[feature_columns])\n",
        "\n",
        "    # Print predictions for debugging\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n",
        "\n",
        "    # Add predictions to the combined DataFrame\n",
        "    combined_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Save the combined DataFrame with predictions to a new CSV file\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "output_csv = '/content/Exported_CSV/Combined_Pred_RF_GA.csv'  # Output CSV file name\n",
        "predict_bod_from_two_csvs(csv1, csv2, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1l3UiRv_5q4",
        "outputId": "516b8b3f-b47b-4ab5-ac01-b8f51db66e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled data from first CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "9    8.49       6.59      130.68            0.16        259.02       30.8   \n",
            "42   8.84       9.49       94.15            0.11        189.10       29.7   \n",
            "33   8.87       9.74       31.71            0.04        157.90       30.1   \n",
            "311  7.38       1.39      116.10            0.65        233.00       26.5   \n",
            "272  7.57       1.24      118.90            0.12        222.00       26.4   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "9          1.26      0.0072      0.032      0.041  ...      3.4      6.2   \n",
            "42         0.86      0.0443      0.038      0.174  ...      3.3      5.8   \n",
            "33         0.77      0.0402      0.071      0.189  ...      3.6      4.9   \n",
            "311        3.40      0.0011      0.039      0.042  ...      3.2      4.6   \n",
            "272        1.05      0.0248      0.043      0.069  ...      3.1      4.3   \n",
            "\n",
            "           DATE      MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "9     9/21/2022  September  2022        4          1     Surface  13°57.869'   \n",
            "42   11/24/2022   November  2022        5          1     Surface  13°57.767'   \n",
            "33   11/24/2022   November  2022        2          1     Surface  13°57.678'   \n",
            "311   4/19/2023      April  2023        4          3      Bottom  13°57.872'   \n",
            "272    9/2/2023   February  2023        1          3      Bottom  13°57.890'   \n",
            "\n",
            "      Longtitude  \n",
            "9    121°18.485'  \n",
            "42   121°18.403'  \n",
            "33   121°18.262'  \n",
            "311  121°18.489'  \n",
            "272  121°18.293'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Sampled data from second CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "138  9.38       9.00        78.8            0.07         156.7       33.0   \n",
            "16   7.86       4.79       115.2            0.14         229.2       30.0   \n",
            "155  9.32       7.14        81.6            0.08         163.1       30.8   \n",
            "96   9.60      18.45        87.6            0.10         175.0       31.6   \n",
            "68   7.46       1.53        89.3            0.10         178.9       27.8   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "138        2.72      0.0112      0.042      0.021  ...      2.8      3.8   \n",
            "16         0.61      0.0025      0.016      0.019  ...      3.8      5.5   \n",
            "155        3.54      0.0014      0.035      0.026  ...      3.5      4.5   \n",
            "96         3.77      0.0094      0.041      0.061  ...      3.3      5.2   \n",
            "68         1.73      0.0221      0.053      0.024  ...      3.6      3.3   \n",
            "\n",
            "           DATE    MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "138   6/23/2023     June  2023        2          1     Surface  14°00.082'   \n",
            "16   10/26/2022  October  2022        1          2     Surface  14°00.164'   \n",
            "155   7/19/2023     July  2023        2          3     Surface  14°00.062'   \n",
            "96    3/16/2023    March  2023        3          1     Surface  14°00.046'   \n",
            "68    1/26/2023  January  2023        3          3     Surface  14°00.036'   \n",
            "\n",
            "      Longtitude  \n",
            "138  121°22.191'  \n",
            "16   121°22.243'  \n",
            "155  121°22.185'  \n",
            "96   121°22.342'  \n",
            "68   121°22.350'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Predictions:\n",
            "[1.23487389 0.88794066 0.97009605 2.47606061 1.69717055 1.63784759\n",
            " 1.06687299 1.78718182 1.63825758 3.45740368 1.75411364 1.19939597\n",
            " 0.98929908 1.95182487 1.66681818 2.50192424 0.59181025 3.38363636\n",
            " 3.42515909 1.92733333 3.37291667 0.54994093 0.57578941 3.75519697\n",
            " 3.74144697 4.55011422 0.5184899  1.14699242 1.11709596 2.78981061]\n",
            "Predictions saved to /content/Exported_CSV/Combined_Pred_RF_GA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tiered Algorithm**"
      ],
      "metadata": {
        "id": "8RCqPXqdfX34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from two CSV files\n",
        "def predict_bod_from_two_csvs(csv1, csv2, output_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/RF/RF_Tiered.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read both input data CSVs\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "\n",
        "    # Ensure both input data have all necessary columns\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "\n",
        "    # Sample 15 random rows from both dataframes\n",
        "    sampled_df1 = df1.sample(n=sample_size, random_state=42)\n",
        "    sampled_df2 = df2.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Print sampled data for debugging\n",
        "    print(\"Sampled data from first CSV:\")\n",
        "    print(sampled_df1.head())\n",
        "    print(\"Sampled data from second CSV:\")\n",
        "    print(sampled_df2.head())\n",
        "\n",
        "    # Concatenate the two sampled dataframes\n",
        "    combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "    # Predict BOD for the combined data\n",
        "    predictions = model.predict(combined_df[feature_columns])\n",
        "\n",
        "    # Print predictions for debugging\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n",
        "\n",
        "    # Add predictions to the combined DataFrame\n",
        "    combined_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Save the combined DataFrame with predictions to a new CSV file\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "output_csv = '/content/Exported_CSV/Combined_Pred_RF_TA.csv'  # Output CSV file name\n",
        "predict_bod_from_two_csvs(csv1, csv2, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4W2NSJF__Zv",
        "outputId": "d28f0f99-1afb-498e-b340-f90d3120474a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled data from first CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "9    8.49       6.59      130.68            0.16        259.02       30.8   \n",
            "42   8.84       9.49       94.15            0.11        189.10       29.7   \n",
            "33   8.87       9.74       31.71            0.04        157.90       30.1   \n",
            "311  7.38       1.39      116.10            0.65        233.00       26.5   \n",
            "272  7.57       1.24      118.90            0.12        222.00       26.4   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "9          1.26      0.0072      0.032      0.041  ...      3.4      6.2   \n",
            "42         0.86      0.0443      0.038      0.174  ...      3.3      5.8   \n",
            "33         0.77      0.0402      0.071      0.189  ...      3.6      4.9   \n",
            "311        3.40      0.0011      0.039      0.042  ...      3.2      4.6   \n",
            "272        1.05      0.0248      0.043      0.069  ...      3.1      4.3   \n",
            "\n",
            "           DATE      MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "9     9/21/2022  September  2022        4          1     Surface  13°57.869'   \n",
            "42   11/24/2022   November  2022        5          1     Surface  13°57.767'   \n",
            "33   11/24/2022   November  2022        2          1     Surface  13°57.678'   \n",
            "311   4/19/2023      April  2023        4          3      Bottom  13°57.872'   \n",
            "272    9/2/2023   February  2023        1          3      Bottom  13°57.890'   \n",
            "\n",
            "      Longtitude  \n",
            "9    121°18.485'  \n",
            "42   121°18.403'  \n",
            "33   121°18.262'  \n",
            "311  121°18.489'  \n",
            "272  121°18.293'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Sampled data from second CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "138  9.38       9.00        78.8            0.07         156.7       33.0   \n",
            "16   7.86       4.79       115.2            0.14         229.2       30.0   \n",
            "155  9.32       7.14        81.6            0.08         163.1       30.8   \n",
            "96   9.60      18.45        87.6            0.10         175.0       31.6   \n",
            "68   7.46       1.53        89.3            0.10         178.9       27.8   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "138        2.72      0.0112      0.042      0.021  ...      2.8      3.8   \n",
            "16         0.61      0.0025      0.016      0.019  ...      3.8      5.5   \n",
            "155        3.54      0.0014      0.035      0.026  ...      3.5      4.5   \n",
            "96         3.77      0.0094      0.041      0.061  ...      3.3      5.2   \n",
            "68         1.73      0.0221      0.053      0.024  ...      3.6      3.3   \n",
            "\n",
            "           DATE    MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "138   6/23/2023     June  2023        2          1     Surface  14°00.082'   \n",
            "16   10/26/2022  October  2022        1          2     Surface  14°00.164'   \n",
            "155   7/19/2023     July  2023        2          3     Surface  14°00.062'   \n",
            "96    3/16/2023    March  2023        3          1     Surface  14°00.046'   \n",
            "68    1/26/2023  January  2023        3          3     Surface  14°00.036'   \n",
            "\n",
            "      Longtitude  \n",
            "138  121°22.191'  \n",
            "16   121°22.243'  \n",
            "155  121°22.185'  \n",
            "96   121°22.342'  \n",
            "68   121°22.350'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Predictions:\n",
            "[3.67059226 3.6684738  3.60808656 3.68093394 4.16444191 3.69451025\n",
            " 3.91747153 3.68061503 3.69369021 3.65708428 3.68075171 4.12389522\n",
            " 4.11366743 3.81127563 4.16744875 3.80214123 3.983918   4.10589977\n",
            " 4.14457859 4.13075171 4.10872437 3.97384966 3.98446469 4.14113895\n",
            " 4.14177677 4.10589977 3.98555809 4.08694761 4.11548975 4.12125285]\n",
            "Predictions saved to /content/Exported_CSV/Combined_Pred_RF_TA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Least Square**"
      ],
      "metadata": {
        "id": "gHS1zX34fdqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from two CSV files\n",
        "def predict_bod_from_two_csvs(csv1, csv2, output_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/RF/RF_Least.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read both input data CSVs\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "\n",
        "    # Ensure both input data have all necessary columns\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "\n",
        "    # Sample 15 random rows from both dataframes\n",
        "    sampled_df1 = df1.sample(n=sample_size, random_state=42)\n",
        "    sampled_df2 = df2.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Print sampled data for debugging\n",
        "    print(\"Sampled data from first CSV:\")\n",
        "    print(sampled_df1.head())\n",
        "    print(\"Sampled data from second CSV:\")\n",
        "    print(sampled_df2.head())\n",
        "\n",
        "    # Concatenate the two sampled dataframes\n",
        "    combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "    # Predict BOD for the combined data\n",
        "    predictions = model.predict(combined_df[feature_columns])\n",
        "\n",
        "    # Print predictions for debugging\n",
        "    print(\"Predictions:\")\n",
        "    print(predictions)\n",
        "\n",
        "    # Add predictions to the combined DataFrame\n",
        "    combined_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Save the combined DataFrame with predictions to a new CSV file\n",
        "    combined_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "output_csv = '/content/Exported_CSV/Combined_Pred_RF_LS.csv'  # Output CSV file name\n",
        "predict_bod_from_two_csvs(csv1, csv2, output_csv)\n"
      ],
      "metadata": {
        "id": "PMkxnibDAFcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df1a18e-614a-4105-f954-f1fc0a2d31ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled data from first CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "9    8.49       6.59      130.68            0.16        259.02       30.8   \n",
            "42   8.84       9.49       94.15            0.11        189.10       29.7   \n",
            "33   8.87       9.74       31.71            0.04        157.90       30.1   \n",
            "311  7.38       1.39      116.10            0.65        233.00       26.5   \n",
            "272  7.57       1.24      118.90            0.12        222.00       26.4   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "9          1.26      0.0072      0.032      0.041  ...      3.4      6.2   \n",
            "42         0.86      0.0443      0.038      0.174  ...      3.3      5.8   \n",
            "33         0.77      0.0402      0.071      0.189  ...      3.6      4.9   \n",
            "311        3.40      0.0011      0.039      0.042  ...      3.2      4.6   \n",
            "272        1.05      0.0248      0.043      0.069  ...      3.1      4.3   \n",
            "\n",
            "           DATE      MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "9     9/21/2022  September  2022        4          1     Surface  13°57.869'   \n",
            "42   11/24/2022   November  2022        5          1     Surface  13°57.767'   \n",
            "33   11/24/2022   November  2022        2          1     Surface  13°57.678'   \n",
            "311   4/19/2023      April  2023        4          3      Bottom  13°57.872'   \n",
            "272    9/2/2023   February  2023        1          3      Bottom  13°57.890'   \n",
            "\n",
            "      Longtitude  \n",
            "9    121°18.485'  \n",
            "42   121°18.403'  \n",
            "33   121°18.262'  \n",
            "311  121°18.489'  \n",
            "272  121°18.293'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Sampled data from second CSV:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "138  9.38       9.00        78.8            0.07         156.7       33.0   \n",
            "16   7.86       4.79       115.2            0.14         229.2       30.0   \n",
            "155  9.32       7.14        81.6            0.08         163.1       30.8   \n",
            "96   9.60      18.45        87.6            0.10         175.0       31.6   \n",
            "68   7.46       1.53        89.3            0.10         178.9       27.8   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "138        2.72      0.0112      0.042      0.021  ...      2.8      3.8   \n",
            "16         0.61      0.0025      0.016      0.019  ...      3.8      5.5   \n",
            "155        3.54      0.0014      0.035      0.026  ...      3.5      4.5   \n",
            "96         3.77      0.0094      0.041      0.061  ...      3.3      5.2   \n",
            "68         1.73      0.0221      0.053      0.024  ...      3.6      3.3   \n",
            "\n",
            "           DATE    MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "138   6/23/2023     June  2023        2          1     Surface  14°00.082'   \n",
            "16   10/26/2022  October  2022        1          2     Surface  14°00.164'   \n",
            "155   7/19/2023     July  2023        2          3     Surface  14°00.062'   \n",
            "96    3/16/2023    March  2023        3          1     Surface  14°00.046'   \n",
            "68    1/26/2023  January  2023        3          3     Surface  14°00.036'   \n",
            "\n",
            "      Longtitude  \n",
            "138  121°22.191'  \n",
            "16   121°22.243'  \n",
            "155  121°22.185'  \n",
            "96   121°22.342'  \n",
            "68   121°22.350'  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Predictions:\n",
            "[1.285      1.28078571 0.94564286 1.61804762 1.63047619 1.31952381\n",
            " 1.0947381  1.73680952 1.49309524 3.30135714 1.70607143 1.26780952\n",
            " 1.07230952 1.73230952 1.67142857 2.48011905 0.59964286 3.32664286\n",
            " 3.666      2.09733333 3.40366667 0.58957143 0.57435714 3.82209524\n",
            " 3.74214286 4.95821429 0.5787619  1.16414286 0.86728571 2.63071429]\n",
            "Predictions saved to /content/Exported_CSV/Combined_Pred_RF_LS.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Linear Regression**"
      ],
      "metadata": {
        "id": "idDQjEHeAyDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Genetics Algorithm**"
      ],
      "metadata": {
        "id": "aSt71YgSgIwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from a CSV file with 15 random rows\n",
        "def predict_bod_from_csv(input_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/MLR/MLR_Genetics 2.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read the input data from CSV\n",
        "    input_df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Ensure the input data has all necessary columns\n",
        "    missing_cols = set(feature_columns) - set(input_df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing columns in input data: {missing_cols}\")\n",
        "\n",
        "    # Sample 15 random rows from the input data\n",
        "    sampled_df = input_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Predict BOD for the sampled data\n",
        "    predictions = model.predict(sampled_df[feature_columns])\n",
        "\n",
        "    # Add predictions to the sampled DataFrame\n",
        "    sampled_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    return sampled_df\n",
        "\n",
        "# Example usage for the first dataset\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "df1 = predict_bod_from_csv(csv1)\n",
        "\n",
        "# Example usage for the second dataset\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "df2 = predict_bod_from_csv(csv2)\n",
        "\n",
        "# Combine both datasets into a single DataFrame\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame with predictions to a new CSV file\n",
        "combined_output_csv = '/content/Exported_CSV/Combined_Pred_MLR_GA.csv'  # Combined output CSV file\n",
        "combined_df.to_csv(combined_output_csv, index=False)\n",
        "print(f\"Combined predictions saved to {combined_output_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4uiVJh4AJ0a",
        "outputId": "8708dc5d-8472-4701-aeab-1cf1bbde8a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined predictions saved to /content/Exported_CSV/Combined_Pred_MLR_GA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Ridge from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Ridge from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tiered Algorithm**"
      ],
      "metadata": {
        "id": "t4o8j6MHgQbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from a CSV file with 15 random rows\n",
        "def predict_bod_from_csv(input_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/MLR/MLR_Tiered.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'BGA-PC (ug/L)', 'Chlorophyll (ug/L)',\n",
        "        'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', 'Mn(ppm)', 'Zn(ppm)',\n",
        "        'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read the input data from CSV\n",
        "    input_df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Ensure the input data has all necessary columns\n",
        "    missing_cols = set(feature_columns) - set(input_df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing columns in input data: {missing_cols}\")\n",
        "\n",
        "    # Sample 15 random rows from the input data\n",
        "    sampled_df = input_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Predict BOD for the sampled data\n",
        "    predictions = model.predict(sampled_df[feature_columns])\n",
        "\n",
        "    # Add predictions to the sampled DataFrame\n",
        "    sampled_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    return sampled_df\n",
        "\n",
        "# Example usage for the first dataset\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "df1 = predict_bod_from_csv(csv1)\n",
        "\n",
        "# Example usage for the second dataset\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "df2 = predict_bod_from_csv(csv2)\n",
        "\n",
        "# Combine both datasets into a single DataFrame\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame with predictions to a new CSV file\n",
        "combined_output_csv = '/content/Exported_CSV/Combined_Pred_MLR_TA.csv'  # Combined output CSV file\n",
        "combined_df.to_csv(combined_output_csv, index=False)\n",
        "print(f\"Combined predictions saved to {combined_output_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4pmJI9mAeT0",
        "outputId": "c2f8d61d-6a7d-4ca1-88e8-e1f8ca2dd03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GridSearchCV from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined predictions saved to /content/Exported_CSV/Combined_Pred_MLR_TA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator GridSearchCV from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Least Square**"
      ],
      "metadata": {
        "id": "nJeAJaBzgWcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD from a CSV file with 15 random rows\n",
        "def predict_bod_from_csv(input_csv, sample_size=15):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/MLR/MLR_Least.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Read the input data from CSV\n",
        "    input_df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Ensure the input data has all necessary columns\n",
        "    missing_cols = set(feature_columns) - set(input_df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing columns in input data: {missing_cols}\")\n",
        "\n",
        "    # Sample 15 random rows from the input data\n",
        "    sampled_df = input_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "    # Predict BOD for the sampled data\n",
        "    predictions = model.predict(sampled_df[feature_columns])\n",
        "\n",
        "    # Add predictions to the sampled DataFrame\n",
        "    sampled_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    return sampled_df\n",
        "\n",
        "# Example usage for the first dataset\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "df1 = predict_bod_from_csv(csv1)\n",
        "\n",
        "# Example usage for the second dataset\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "df2 = predict_bod_from_csv(csv2)\n",
        "\n",
        "# Combine both datasets into a single DataFrame\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame with predictions to a new CSV file\n",
        "combined_output_csv = '/content/Exported_CSV/Combined_Pred_MLR_LS.csv'  # Combined output CSV file\n",
        "combined_df.to_csv(combined_output_csv, index=False)\n",
        "print(f\"Combined predictions saved to {combined_output_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScOlwbt7AUv1",
        "outputId": "3c785dab-b1f8-4839-873e-5b87590e284b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /content/Exported_CSV/Pred_MLR_GA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [12:53:19] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
            "configuration generated by an older version of XGBoost, please export the model by calling\n",
            "`Booster.save_model` from that version first, then load it back in current version. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
            "\n",
            "for more details about differences between saving model and serializing.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.5.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREDICTION TO CSV H5 MODELS**\n"
      ],
      "metadata": {
        "id": "J36R0EPh31Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Layered Peceptron**"
      ],
      "metadata": {
        "id": "l0i-h9rPBEgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Genetics Algorithm**"
      ],
      "metadata": {
        "id": "AWjSVgGGmEec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/All_Shit/MLP/MLP_Genetics.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "# Sample 15 random rows from each dataframe\n",
        "sampled_df1 = df1.sample(n=15, random_state=42)\n",
        "sampled_df2 = df2.sample(n=15, random_state=42)\n",
        "\n",
        "# Combine the two sampled dataframes\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the actual BOD column\n",
        "if 'BOD (mg/L)' in combined_df.columns:\n",
        "    actual_bod = combined_df['BOD (mg/L)']  # Extract actual BOD values\n",
        "else:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "\n",
        "# Extract features from the combined data\n",
        "X_combined = combined_df[feature_columns]\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the combined data for now (in practice, use a pre-fitted scaler)\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the input features\n",
        "X_combined_scaled = scaler.transform(X_combined)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(X_combined_scaled).flatten()\n",
        "\n",
        "# Add the actual and predicted BOD values to the combined dataframe\n",
        "combined_df['Actual BOD (mg/L)'] = actual_bod\n",
        "combined_df['Predicted BOD (mg/L)'] = predictions\n",
        "\n",
        "# Optionally, include the 'Month' column if available\n",
        "if 'Month' in combined_df.columns:\n",
        "    output_data = combined_df[['Month', 'Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "else:\n",
        "    combined_df['Month'] = 'January'  # Placeholder, modify as needed\n",
        "    output_data = combined_df[['Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_data.to_csv('/content/All_Shit/Exported_CSV/Combined_Pred_MLP_GA.csv', index=False)\n",
        "\n",
        "# Output predictions to the console\n",
        "print('Predictions for combined sampled data with month:')\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "id": "KTnEe2xYoxN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83699abc-5c53-4b2c-8018-85aebfec3e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Predictions for combined sampled data with month:\n",
            "    Actual BOD (mg/L)  Predicted BOD (mg/L)\n",
            "0                1.26              2.178982\n",
            "1                0.86              2.656128\n",
            "2                0.77              1.740568\n",
            "3                3.40              2.512649\n",
            "4                1.05              1.706314\n",
            "5                1.72              1.497130\n",
            "6                1.10              2.119810\n",
            "7                1.00              1.138694\n",
            "8                1.54              0.721108\n",
            "9                3.19              2.869582\n",
            "10               1.97              2.294593\n",
            "11               1.21              3.401082\n",
            "12               0.93              1.238418\n",
            "13               2.82              1.539461\n",
            "14               0.83              2.086167\n",
            "15               2.72              3.505661\n",
            "16               0.61              1.111352\n",
            "17               3.54              4.293103\n",
            "18               3.77              3.535656\n",
            "19               1.73              3.640819\n",
            "20               3.53              4.581535\n",
            "21               0.58              0.846322\n",
            "22               0.54              1.024642\n",
            "23               3.81              3.742275\n",
            "24               3.63              4.139516\n",
            "25               5.35              5.534104\n",
            "26               0.43              1.071695\n",
            "27               1.07              1.244103\n",
            "28               0.58              1.694885\n",
            "29               2.27              4.103318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tiered Algo**"
      ],
      "metadata": {
        "id": "QrTH_yasmLYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/All_Shit/MLP/MLP_Tiering.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "# Sample 15 random rows from each dataframe\n",
        "sampled_df1 = df1.sample(n=15, random_state=42)\n",
        "sampled_df2 = df2.sample(n=15, random_state=42)\n",
        "\n",
        "# Combine the two sampled dataframes\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the actual BOD column\n",
        "if 'BOD (mg/L)' in combined_df.columns:\n",
        "    actual_bod = combined_df['BOD (mg/L)']  # Extract actual BOD values\n",
        "else:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "\n",
        "# Extract features from the combined data\n",
        "X_combined = combined_df[feature_columns]\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the combined data for now (in practice, use a pre-fitted scaler)\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the input features\n",
        "X_combined_scaled = scaler.transform(X_combined)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(X_combined_scaled).flatten()\n",
        "\n",
        "# Add the actual and predicted BOD values to the combined dataframe\n",
        "combined_df['Actual BOD (mg/L)'] = actual_bod\n",
        "combined_df['Predicted BOD (mg/L)'] = predictions\n",
        "\n",
        "# Optionally, include the 'Month' column if available\n",
        "if 'Month' in combined_df.columns:\n",
        "    output_data = combined_df[['Month', 'Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "else:\n",
        "    combined_df['Month'] = 'January'  # Placeholder, modify as needed\n",
        "    output_data = combined_df[['Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_data.to_csv('/content/All_Shit/Exported_CSV/Combined_Pred_MLP_TA.csv', index=False)\n",
        "\n",
        "# Output predictions to the console\n",
        "print('Predictions for combined sampled data with month:')\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptXk_7PcBYI-",
        "outputId": "efcefb45-fbec-46e9-f24a-7bececfd82f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "Predictions for combined sampled data with month:\n",
            "    Actual BOD (mg/L)  Predicted BOD (mg/L)\n",
            "0                1.26              1.184218\n",
            "1                0.86              1.141109\n",
            "2                0.77              1.629049\n",
            "3                3.40              1.404057\n",
            "4                1.05              1.633756\n",
            "5                1.72              0.794916\n",
            "6                1.10              1.091761\n",
            "7                1.00              1.341880\n",
            "8                1.54              0.990906\n",
            "9                3.19              3.432413\n",
            "10               1.97              1.788775\n",
            "11               1.21              1.703771\n",
            "12               0.93              0.966348\n",
            "13               2.82              1.117143\n",
            "14               0.83              2.116500\n",
            "15               2.72              2.900607\n",
            "16               0.61              0.971863\n",
            "17               3.54              4.400919\n",
            "18               3.77              3.237895\n",
            "19               1.73              2.683054\n",
            "20               3.53              4.569846\n",
            "21               0.58              0.845349\n",
            "22               0.54              0.880302\n",
            "23               3.81              3.838685\n",
            "24               3.63              4.286467\n",
            "25               5.35              5.589184\n",
            "26               0.43              1.020792\n",
            "27               1.07              1.182071\n",
            "28               0.58              0.876677\n",
            "29               2.27              4.581858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Least Square**"
      ],
      "metadata": {
        "id": "Uywa65IomPFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/All_Shit/MLP/MLP_Least.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "# Sample 15 random rows from each dataframe\n",
        "sampled_df1 = df1.sample(n=15, random_state=42)\n",
        "sampled_df2 = df2.sample(n=15, random_state=42)\n",
        "\n",
        "# Combine the two sampled dataframes\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the actual BOD column\n",
        "if 'BOD (mg/L)' in combined_df.columns:\n",
        "    actual_bod = combined_df['BOD (mg/L)']  # Extract actual BOD values\n",
        "else:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "\n",
        "# Extract features from the combined data\n",
        "X_combined = combined_df[feature_columns]\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the combined data for now (in practice, use a pre-fitted scaler)\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the input features\n",
        "X_combined_scaled = scaler.transform(X_combined)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(X_combined_scaled).flatten()\n",
        "\n",
        "# Add the actual and predicted BOD values to the combined dataframe\n",
        "combined_df['Actual BOD (mg/L)'] = actual_bod\n",
        "combined_df['Predicted BOD (mg/L)'] = predictions\n",
        "\n",
        "# Optionally, include the 'Month' column if available\n",
        "if 'Month' in combined_df.columns:\n",
        "    output_data = combined_df[['Month', 'Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "else:\n",
        "    combined_df['Month'] = 'January'  # Placeholder, modify as needed\n",
        "    output_data = combined_df[['Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_data.to_csv('/content/All_Shit/Exported_CSV/Combined_Pred_MLP_LS.csv', index=False)\n",
        "\n",
        "# Output predictions to the console\n",
        "print('Predictions for combined sampled data with month:')\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "id": "nkXjHQOUaUxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234d5cba-ff8f-4c3e-9bb8-fb29af2e275e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "Predictions for combined sampled data with month:\n",
            "    Actual BOD (mg/L)  Predicted BOD (mg/L)\n",
            "0                1.26              2.457682\n",
            "1                0.86              1.895507\n",
            "2                0.77              2.827764\n",
            "3                3.40              1.592305\n",
            "4                1.05              1.636373\n",
            "5                1.72              0.972071\n",
            "6                1.10              2.258449\n",
            "7                1.00              2.380834\n",
            "8                1.54              0.854637\n",
            "9                3.19              3.477560\n",
            "10               1.97              2.099782\n",
            "11               1.21              2.735597\n",
            "12               0.93              1.858386\n",
            "13               2.82              0.638295\n",
            "14               0.83              1.493240\n",
            "15               2.72              3.712967\n",
            "16               0.61              1.775438\n",
            "17               3.54              4.522511\n",
            "18               3.77              3.560847\n",
            "19               1.73              3.259749\n",
            "20               3.53              4.327278\n",
            "21               0.58              1.504575\n",
            "22               0.54              1.470421\n",
            "23               3.81              4.216758\n",
            "24               3.63              4.041910\n",
            "25               5.35              5.615441\n",
            "26               0.43              1.608583\n",
            "27               1.07              1.176688\n",
            "28               0.58              1.953045\n",
            "29               2.27              4.196190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Neural Network**"
      ],
      "metadata": {
        "id": "XcTIncBOBiVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Genetics Algorithm**"
      ],
      "metadata": {
        "id": "FBqOtGBEm-Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/All_Shit/ANN/ANN_Genetics.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "# Sample 15 random rows from each dataframe\n",
        "sampled_df1 = df1.sample(n=15, random_state=42)\n",
        "sampled_df2 = df2.sample(n=15, random_state=42)\n",
        "\n",
        "# Combine the two sampled dataframes\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the actual BOD column\n",
        "if 'BOD (mg/L)' in combined_df.columns:\n",
        "    actual_bod = combined_df['BOD (mg/L)']  # Extract actual BOD values\n",
        "else:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "\n",
        "# Extract features from the combined data\n",
        "X_combined = combined_df[feature_columns]\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the combined data for now (in practice, use a pre-fitted scaler)\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the input features\n",
        "X_combined_scaled = scaler.transform(X_combined)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(X_combined_scaled).flatten()\n",
        "\n",
        "# Add the actual and predicted BOD values to the combined dataframe\n",
        "combined_df['Actual BOD (mg/L)'] = actual_bod\n",
        "combined_df['Predicted BOD (mg/L)'] = predictions\n",
        "\n",
        "# Optionally, include the 'Month' column if available\n",
        "if 'Month' in combined_df.columns:\n",
        "    output_data = combined_df[['Month', 'Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "else:\n",
        "    combined_df['Month'] = 'January'  # Placeholder, modify as needed\n",
        "    output_data = combined_df[['Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_data.to_csv('/content/All_Shit/Exported_CSV/Combined_Pred_ANN_GA.csv', index=False)\n",
        "\n",
        "# Output predictions to the console\n",
        "print('Predictions for combined sampled data with month:')\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUfHVKH8BlHq",
        "outputId": "02f89870-e169-4b65-9385-020ddccb3918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "Predictions for combined sampled data with month:\n",
            "    Actual BOD (mg/L)  Predicted BOD (mg/L)\n",
            "0                1.26              1.853683\n",
            "1                0.86              2.312531\n",
            "2                0.77              1.994415\n",
            "3                3.40              0.600375\n",
            "4                1.05              2.621516\n",
            "5                1.72              0.684571\n",
            "6                1.10              1.713614\n",
            "7                1.00              1.118037\n",
            "8                1.54              0.609683\n",
            "9                3.19              3.664783\n",
            "10               1.97              1.305569\n",
            "11               1.21              3.496611\n",
            "12               0.93              1.311317\n",
            "13               2.82              1.008914\n",
            "14               0.83              2.852187\n",
            "15               2.72              2.825196\n",
            "16               0.61              1.526996\n",
            "17               3.54              4.335528\n",
            "18               3.77              3.090878\n",
            "19               1.73              2.985421\n",
            "20               3.53              4.503248\n",
            "21               0.58              1.312710\n",
            "22               0.54              1.348347\n",
            "23               3.81              3.847139\n",
            "24               3.63              3.637823\n",
            "25               5.35              6.756965\n",
            "26               0.43              1.069291\n",
            "27               1.07              1.804947\n",
            "28               0.58              2.661651\n",
            "29               2.27              3.915634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tiered Algo**"
      ],
      "metadata": {
        "id": "HeU9iFyOnCJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/All_Shit/ANN/ANN_Tiered.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "# Sample 15 random rows from each dataframe\n",
        "sampled_df1 = df1.sample(n=15, random_state=42)\n",
        "sampled_df2 = df2.sample(n=15, random_state=42)\n",
        "\n",
        "# Combine the two sampled dataframes\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the actual BOD column\n",
        "if 'BOD (mg/L)' in combined_df.columns:\n",
        "    actual_bod = combined_df['BOD (mg/L)']  # Extract actual BOD values\n",
        "else:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "\n",
        "# Extract features from the combined data\n",
        "X_combined = combined_df[feature_columns]\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the combined data for now (in practice, use a pre-fitted scaler)\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the input features\n",
        "X_combined_scaled = scaler.transform(X_combined)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(X_combined_scaled).flatten()\n",
        "\n",
        "# Add the actual and predicted BOD values to the combined dataframe\n",
        "combined_df['Actual BOD (mg/L)'] = actual_bod\n",
        "combined_df['Predicted BOD (mg/L)'] = predictions\n",
        "\n",
        "# Optionally, include the 'Month' column if available\n",
        "if 'Month' in combined_df.columns:\n",
        "    output_data = combined_df[['Month', 'Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "else:\n",
        "    combined_df['Month'] = 'January'  # Placeholder, modify as needed\n",
        "    output_data = combined_df[['Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_data.to_csv('/content/All_Shit/Exported_CSV/Combined_Pred_ANN_TA.csv', index=False)\n",
        "\n",
        "# Output predictions to the console\n",
        "print('Predictions for combined sampled data with month:')\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzPvNz8mB_T-",
        "outputId": "b67e405f-82fc-4e39-d8fc-093dda9f5452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
            "Predictions for combined sampled data with month:\n",
            "    Actual BOD (mg/L)  Predicted BOD (mg/L)\n",
            "0                1.26              1.789281\n",
            "1                0.86              1.933297\n",
            "2                0.77              1.555702\n",
            "3                3.40              1.242415\n",
            "4                1.05              1.481246\n",
            "5                1.72              1.169735\n",
            "6                1.10              1.370100\n",
            "7                1.00              0.878690\n",
            "8                1.54              1.145615\n",
            "9                3.19              3.446601\n",
            "10               1.97              1.341378\n",
            "11               1.21              2.630800\n",
            "12               0.93              0.702352\n",
            "13               2.82              1.441138\n",
            "14               0.83              1.589655\n",
            "15               2.72              2.324499\n",
            "16               0.61              0.566675\n",
            "17               3.54              4.088719\n",
            "18               3.77              3.060534\n",
            "19               1.73              1.118680\n",
            "20               3.53              4.008060\n",
            "21               0.58              0.776686\n",
            "22               0.54              0.548289\n",
            "23               3.81              3.299182\n",
            "24               3.63              3.565150\n",
            "25               5.35              5.873604\n",
            "26               0.43              0.795483\n",
            "27               1.07              0.826401\n",
            "28               0.58              3.991897\n",
            "29               2.27              3.776019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Least Square**"
      ],
      "metadata": {
        "id": "BmNptcUinNEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/ANN_Least.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "# Sample 15 random rows from each dataframe\n",
        "sampled_df1 = df1.sample(n=15, random_state=42)\n",
        "sampled_df2 = df2.sample(n=15, random_state=42)\n",
        "\n",
        "# Combine the two sampled dataframes\n",
        "combined_df = pd.concat([sampled_df1, sampled_df2], ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the actual BOD column\n",
        "if 'BOD (mg/L)' in combined_df.columns:\n",
        "    actual_bod = combined_df['BOD (mg/L)']  # Extract actual BOD values\n",
        "else:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "\n",
        "# Extract features from the combined data\n",
        "X_combined = combined_df[feature_columns]\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the combined data for now (in practice, use a pre-fitted scaler)\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the input features\n",
        "X_combined_scaled = scaler.transform(X_combined)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(X_combined_scaled).flatten()\n",
        "\n",
        "# Add the actual and predicted BOD values to the combined dataframe\n",
        "combined_df['Actual BOD (mg/L)'] = actual_bod\n",
        "combined_df['Predicted BOD (mg/L)'] = predictions\n",
        "\n",
        "# Optionally, include the 'Month' column if available\n",
        "if 'Month' in combined_df.columns:\n",
        "    output_data = combined_df[['Month', 'Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "else:\n",
        "    combined_df['Month'] = 'January'  # Placeholder, modify as needed\n",
        "    output_data = combined_df[['Actual BOD (mg/L)', 'Predicted BOD (mg/L)']]\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_data.to_csv('/content/Combined_Pred_ANN_LS.csv', index=False)\n",
        "\n",
        "# Output predictions to the console\n",
        "print('Predictions for combined sampled data with month:')\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOsdl1XqCCmn",
        "outputId": "302794b9-1514-45c6-f3bb-c7cd43d636d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "Predictions for combined sampled data with month:\n",
            "    Actual BOD (mg/L)  Predicted BOD (mg/L)\n",
            "0                1.26              2.380203\n",
            "1                0.86              2.370511\n",
            "2                0.77              3.081838\n",
            "3                3.40              1.718762\n",
            "4                1.05              1.228480\n",
            "5                1.72              0.989553\n",
            "6                1.10              1.304582\n",
            "7                1.00              1.126583\n",
            "8                1.54              1.065019\n",
            "9                3.19              4.328072\n",
            "10               1.97              1.696573\n",
            "11               1.21              1.809787\n",
            "12               0.93              2.059227\n",
            "13               2.82              0.717234\n",
            "14               0.83              1.409939\n",
            "15               2.72              2.367879\n",
            "16               0.61              1.629561\n",
            "17               3.54              4.346528\n",
            "18               3.77              2.792667\n",
            "19               1.73              3.095304\n",
            "20               3.53              4.249895\n",
            "21               0.58              1.277506\n",
            "22               0.54              1.595885\n",
            "23               3.81              3.961299\n",
            "24               3.63              3.400129\n",
            "25               5.35              5.921906\n",
            "26               0.43              1.434016\n",
            "27               1.07              0.880111\n",
            "28               0.58              2.431573\n",
            "29               2.27              4.024612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prototype**"
      ],
      "metadata": {
        "id": "m9REoCIESBgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test prototype**"
      ],
      "metadata": {
        "id": "FXqN4IG4jumd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "# Function to load the model and predict BOD from a randomly chosen CSV file\n",
        "def predict_bod_from_random_csv(csv1, csv2, sample_size=1):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/All_Shit/RF/RF_Genetics.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    # Randomly choose one of the CSV files\n",
        "    chosen_csv = random.choice([csv1, csv2])\n",
        "    print(f\"Selected CSV for prediction: {chosen_csv}\")\n",
        "\n",
        "    # Read the input data CSV\n",
        "    df = pd.read_csv(chosen_csv)\n",
        "\n",
        "    # Ensure the input data has all necessary columns\n",
        "    missing_cols = set(feature_columns) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing columns in input data: {missing_cols}\")\n",
        "\n",
        "    # Sample 15 random rows from the chosen dataframe\n",
        "    sampled_df = df.sample(n=sample_size)\n",
        "\n",
        "    # Print sampled data for debugging\n",
        "    print(\"Sampled data for prediction:\")\n",
        "    print(sampled_df.head())\n",
        "\n",
        "    # Predict BOD for the sampled data\n",
        "    predictions = model.predict(sampled_df[feature_columns])\n",
        "\n",
        "    # Add predictions to the sampled DataFrame\n",
        "    sampled_df['Predicted_BOD'] = predictions\n",
        "\n",
        "    # Print actual BOD values and predictions\n",
        "    print(\"Actual and Predicted BOD values:\")\n",
        "    print(sampled_df[['BOD (mg/L)', 'Predicted_BOD']])\n",
        "\n",
        "# Example usage\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "predict_bod_from_random_csv(csv1, csv2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHwRyCoRSANs",
        "outputId": "c7d05995-0867-4595-9f93-f48df0e3e95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected CSV for prediction: /content/tikub_surface_bottom.csv\n",
            "Sampled data for prediction:\n",
            "       pH  DO (mg/L)  TDS (mg/L)  Salinity (ppt)  Cond (uS/cm)  Temp (°C)  \\\n",
            "180  8.58       8.45       136.6            0.17         372.0       31.2   \n",
            "\n",
            "     BOD (mg/L)  TSS (mg/L)  NO2 (ppm)  NO3 (ppm)  ...  As(ppm)  Pb(ppm)  \\\n",
            "180        0.36      0.0006      0.038      0.028  ...      3.3      3.8   \n",
            "\n",
            "          DATE      MONTH  YEAR  STATION  REPLICATE  COLLECTION    Latitude  \\\n",
            "180  9/28/2023  September  2023        1          1     Surface  13°57.873'   \n",
            "\n",
            "      Longtitude  \n",
            "180  121°18.294'  \n",
            "\n",
            "[1 rows x 35 columns]\n",
            "Actual and Predicted BOD values:\n",
            "     BOD (mg/L)  Predicted_BOD\n",
            "180        0.36       1.190295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest Least**"
      ],
      "metadata": {
        "id": "M_TgKfedj1Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Function to load the model and predict BOD for one random row\n",
        "def predict_single_bod(csv1, csv2):\n",
        "    # Load the trained model\n",
        "    model = joblib.load('/content/RF_Least.pkl')  # Use the path to your saved model\n",
        "\n",
        "    # Define the feature columns\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "    ]\n",
        "\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(csv2)\n",
        "    missing_cols_df1 = set(feature_columns) - set(df1.columns)\n",
        "    missing_cols_df2 = set(feature_columns) - set(df2.columns)\n",
        "    if missing_cols_df1:\n",
        "        raise ValueError(f\"Missing columns in first input data: {missing_cols_df1}\")\n",
        "    if missing_cols_df2:\n",
        "        raise ValueError(f\"Missing columns in second input data: {missing_cols_df2}\")\n",
        "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "    sample_row = combined_df.sample(n=1)\n",
        "    actual_bod = sample_row[\"BOD (mg/L)\"].values[0]\n",
        "    predicted_bod = model.predict(sample_row[feature_columns])[0]\n",
        "    print(f\"Actual BOD: {actual_bod}\")\n",
        "    print(f\"Predicted BOD: {predicted_bod}\")\n",
        "\n",
        "csv1 = '/content/tikub_surface_bottom.csv'  # First input CSV file\n",
        "csv2 = '/content/gunao_surface.csv'  # Second input CSV file\n",
        "predict_single_bod(csv1, csv2)\n"
      ],
      "metadata": {
        "id": "e1l952hej0BS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d9c6fd4-6561-45bc-af0a-008d9f721eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-917900a2a25e>\", line 2, in <cell line: 2>\n",
            "    import joblib\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/__init__.py\", line 129, in <module>\n",
            "    from .parallel import Parallel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 31, in <module>\n",
            "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\", line 12, in <module>\n",
            "    from ._utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/_utils.py\", line 11, in <module>\n",
            "    from .externals.loky.process_executor import _ExceptionWithTraceback\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/__init__.py\", line 18, in <module>\n",
            "    from .backend.context import cpu_count\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/__init__.py\", line 4, in <module>\n",
            "    from .context import get_context\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 875, in getmodule\n",
            "    f = getabsfile(module)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 844, in getabsfile\n",
            "    _filename = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 826, in getsourcefile\n",
            "    if os.path.exists(filename):\n",
            "  File \"/usr/lib/python3.10/genericpath.py\", line 19, in exists\n",
            "    os.stat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-917900a2a25e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmemstr_to_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n\u001b[0m\u001b[1;32m     32\u001b[0m                                  \u001b[0mThreadingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialBackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from ._utils import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0m_TracebackCapturingWrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_executor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ExceptionWithTraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_loky_pickler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "def predict_single_bod():\n",
        "    model = joblib.load('/content/RF_Least.pkl')  # Use the path to your saved model\n",
        "    print(\"type 'back' to correct the previous input,\\n 'preview' to see all inputs,\\n 'clear' to reset all inputs\")\n",
        "    feature_columns = [\n",
        "        'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "        'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "        'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "        'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)']\n",
        "    user_data = [None] * len(feature_columns)\n",
        "    i = 0\n",
        "    while i < len(feature_columns):\n",
        "        feature = feature_columns[i]\n",
        "        user_input = input(f\"Enter the value for {feature}: \")\n",
        "        if user_input.lower() == \"back\":         # Check if the user wants to go back\n",
        "            if i > 0:\n",
        "                i -= 1  # Move one step back\n",
        "                print(f\"Going back to {feature_columns[i]}\")\n",
        "            else:\n",
        "                print(\"Already at the first input, cannot go back further.\")\n",
        "            continue  # Skip the rest and restart the loop\n",
        "        elif user_input.lower() == \"preview\": # Show preview of all inputs so far\n",
        "            print(\"Current inputs:\")\n",
        "            for j in range(len(feature_columns)):\n",
        "                print(f\"{feature_columns[j]}: {user_data[j]}\")\n",
        "            continue  # Re-prompt for the same feature\n",
        "        elif user_input.lower() == \"clear\":# Clear all inputs and reset\n",
        "            user_data = [None] * len(feature_columns)  # Reset all values to None\n",
        "            i = 0  # Start over from the first feature\n",
        "            print(\"All inputs have been cleared. Starting over.\")\n",
        "            continue  # Restart input collection from the first feature\n",
        "        try:\n",
        "            user_data[i] = float(user_input)\n",
        "            i += 1  # Move to the next feature\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a numeric value.\")\n",
        "    input_df = pd.DataFrame([user_data], columns=feature_columns)\n",
        "    predicted_bod = model.predict(input_df)[0]\n",
        "    print(f\"Predicted BOD: {predicted_bod}\")\n",
        "predict_single_bod()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTM1HDkt-qgb",
        "outputId": "076f7f90-9b0f-4d3a-8bdd-722ed0b83f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type 'back' to correct the previous input,\n",
            " 'preview' to see all inputs,\n",
            " 'clear' to reset all inputs\n",
            "Enter the value for pH: 7.57\n",
            "Enter the value for DO (mg/L): 1.62\n",
            "Enter the value for TDS (mg/L): 89.2\n",
            "Enter the value for Salinity (ppt): 0.1\n",
            "Enter the value for Cond (uS/cm): 178.8\n",
            "Enter the value for Temp (°C): 28.2\n",
            "Enter the value for TSS (mg/L): 0.0253\n",
            "Enter the value for NO2 (ppm): 0.052\n",
            "Enter the value for NO3 (ppm): 0.024\n",
            "Enter the value for PO4  (ppm): 0.264\n",
            "Enter the value for NH4 (ppm): 1.388\n",
            "Enter the value for TN (ppm): 1.463\n",
            "Enter the value for TP (ppm): 0.28\n",
            "Enter the value for BGA-PC (ug/L): 1.73\n",
            "Enter the value for Chlorophyll (ug/L): 14.93\n",
            "Enter the value for Turbidity (FNU): back\n",
            "Going back to Chlorophyll (ug/L)\n",
            "Enter the value for Chlorophyll (ug/L): 14.94\n",
            "Enter the value for Turbidity (FNU): 3.29\n",
            "Enter the value for Coliform (CFU/100ml): 210\n",
            "Enter the value for Cu (ppm): 9.5\n",
            "Enter the value for Fe (ppm): 30.4\n",
            "Enter the value for Mn(ppm): 53.3\n",
            "Enter the value for Zn(ppm): 6.3\n",
            "Enter the value for Cr(ppm): 5.8\n",
            "Enter the value for Cd(ppm): 9.1\n",
            "Enter the value for Hg(ppm): 1\n",
            "Enter the value for As(ppm): 3.5\n",
            "Enter the value for Pb(ppm): 3.5\n",
            "Predicted BOD: 1.8900952380952496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Artificial Neural Network Least**"
      ],
      "metadata": {
        "id": "dThGh2yhj4yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/ANN_Least.h5')\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Load data from both CSV files\n",
        "df1 = pd.read_csv('/content/tikub_surface.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "if 'BOD (mg/L)' not in combined_df.columns:\n",
        "    raise ValueError(\"Actual BOD column 'BOD (mg/L)' not found in the dataset.\")\n",
        "    sample_row = combined_df.sample(n=1)\n",
        "    actual_bod = sample_row['BOD (mg/L)'].values[0]\n",
        "    X_sample = sample_row[feature_columns]\n",
        "    scaler = StandardScaler()\n",
        "    combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "    scaler.fit(combined_training_data)\n",
        "    X_sample_scaled = scaler.transform(X_sample)\n",
        "    predicted_bod = model.predict(X_sample_scaled).flatten()[0]\n",
        "\n",
        "# Output the actual and predicted BOD values\n",
        "print(f\"Actual BOD (mg/L): {actual_bod}\")\n",
        "print(f\"Predicted BOD (mg/L): {predicted_bod}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFH14_yPj_Cc",
        "outputId": "11745b49-b616-431e-a444-391aed8d202f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual BOD (mg/L): nan\n",
            "Predicted BOD (mg/L): nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "dctaK7PeEhuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "0import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/ANN_Least.h5')\n",
        "print(\"type 'back' to correct the previous input,\\n 'preview' to see all inputs,\\n 'clear' to reset all inputs\")\n",
        "\n",
        "\n",
        "# Define feature columns\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "def predict_bod_from_input():\n",
        "    # Initialize a list to store user inputs\n",
        "    user_data = [None] * len(feature_columns)\n",
        "\n",
        "    # Loop through each feature to collect user input\n",
        "    i = 0\n",
        "    while i < len(feature_columns):\n",
        "        feature = feature_columns[i]\n",
        "        user_input = input(f\"Enter the value for {feature}: \")\n",
        "\n",
        "        # Navigate based on user commands\n",
        "        if user_input.lower() == \"back\":\n",
        "            if i > 0:\n",
        "                i -= 1\n",
        "                print(f\"Going back to {feature_columns[i]}\")\n",
        "            else:\n",
        "                print(\"Already at the first input.\")\n",
        "            continue\n",
        "\n",
        "        elif user_input.lower() == \"preview\":\n",
        "            print(\"Current inputs:\")\n",
        "            for j in range(len(feature_columns)):\n",
        "                print(f\"{feature_columns[j]}: {user_data[j]}\")\n",
        "            continue\n",
        "\n",
        "        elif user_input.lower() == \"clear\":\n",
        "            user_data = [None] * len(feature_columns)\n",
        "            i = 0\n",
        "            print(\"All inputs cleared. Starting over.\")\n",
        "            continue\n",
        "\n",
        "        # Attempt to parse the input as a float\n",
        "        try:\n",
        "            user_data[i] = float(user_input)\n",
        "            i += 1\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a numeric value.\")\n",
        "\n",
        "    # Convert the user data to a DataFrame\n",
        "    input_df = pd.DataFrame([user_data], columns=feature_columns)\n",
        "\n",
        "    # Predict BOD using the model directly without additional scaling\n",
        "    predicted_bod = model.predict(input_df).flatten()[0]\n",
        "    print(f\"Predicted BOD (mg/L): {predicted_bod}\")\n",
        "\n",
        "# Call the function to start the prediction process\n",
        "predict_bod_from_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Rmr_uvOhnP",
        "outputId": "91abc4b5-f227-4a29-ab3f-0c9a289fc031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type 'back' to correct the previous input,\n",
            " 'preview' to see all inputs,\n",
            " 'clear' to reset all inputs\n",
            "Enter the value for pH: 7.45\n",
            "Enter the value for DO (mg/L): 0.42\n",
            "Enter the value for TDS (mg/L): 108.16\n",
            "Enter the value for Salinity (ppt): 0.11\n",
            "Enter the value for Cond (uS/cm): 217.07\n",
            "Enter the value for Temp (°C): 26.7\n",
            "Enter the value for TSS (mg/L): 0.0388\n",
            "Enter the value for NO2 (ppm): 0.042\n",
            "Enter the value for NO3 (ppm): 0.044\n",
            "Enter the value for PO4  (ppm): 0.052\n",
            "Enter the value for NH4 (ppm): 0.518\n",
            "Enter the value for TN (ppm): 0.604\n",
            "Enter the value for TP (ppm): 0.072\n",
            "Enter the value for BGA-PC (ug/L): -0.01\n",
            "Enter the value for Chlorophyll (ug/L): 0.26\n",
            "Enter the value for Turbidity (FNU): 0.69\n",
            "Enter the value for Coliform (CFU/100ml): 1400\n",
            "Enter the value for Cu (ppm): 11.5\n",
            "Enter the value for Fe (ppm): 34\n",
            "Enter the value for Mn(ppm): 54.7\n",
            "Enter the value for Zn(ppm): 7.1\n",
            "Enter the value for Cr(ppm): 5.8\n",
            "Enter the value for Cd(ppm): 10.5\n",
            "Enter the value for Hg(ppm): 1\n",
            "Enter the value for As(ppm): 3.7\n",
            "Enter the value for Pb(ppm): 3.6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Predicted BOD (mg/L): -0.6692798137664795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Neural Network**\n"
      ],
      "metadata": {
        "id": "KsBQBdWME2B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/ANN_Least.h5')\n",
        "\n",
        "# Define the feature columns (same as used during training)\n",
        "feature_columns = [\n",
        "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (°C)', 'TSS (mg/L)',\n",
        "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)',\n",
        "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)',\n",
        "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
        "]\n",
        "\n",
        "# Function to manually input data with back, clear, and preview options\n",
        "def get_manual_input():\n",
        "    user_input = {}\n",
        "    index = 0\n",
        "\n",
        "    print(\"Please enter the values for each feature. Type 'back' to go to the previous feature, 'clear' to clear all inputs, or 'preview' to see entered inputs.\")\n",
        "\n",
        "    while index < len(feature_columns):\n",
        "        feature = feature_columns[index]\n",
        "\n",
        "        # Show current inputs if requested\n",
        "        if feature in user_input:\n",
        "            print(f\"{feature}: {user_input[feature]}\")\n",
        "\n",
        "        value = input(f\"Enter value for {feature}: \")\n",
        "\n",
        "        if value.lower() == 'back':\n",
        "            # Go back to the previous feature\n",
        "            if index > 0:\n",
        "                index -= 1\n",
        "            continue\n",
        "\n",
        "        elif value.lower() == 'clear':\n",
        "            # Clear all inputs\n",
        "            user_input.clear()\n",
        "            index = 0\n",
        "            print(\"All inputs cleared. Restarting input...\")\n",
        "            continue\n",
        "\n",
        "        elif value.lower() == 'preview':\n",
        "            # Preview all inputs so far\n",
        "            print(\"\\nCurrent Inputs:\")\n",
        "            for feat in feature_columns:\n",
        "                print(f\"{feat}: {user_input.get(feat, 'Not entered')}\")\n",
        "            print(\"\\n\")\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                # Attempt to parse the input as a float\n",
        "                user_input[feature] = float(value)\n",
        "                index += 1  # Move to the next feature\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a numeric value.\")\n",
        "\n",
        "    # Convert the user inputs to a DataFrame\n",
        "    return pd.DataFrame([user_input])\n",
        "\n",
        "# Collect input from the user\n",
        "manual_input_df = get_manual_input()\n",
        "\n",
        "# Load the scaler used for standardization (this should be the same scaler used during training)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Load your dataset once to fit the scaler on all training data\n",
        "df1 = pd.read_csv('/content/tikub_surface_bottom.csv')\n",
        "df2 = pd.read_csv('/content/gunao_surface.csv')\n",
        "combined_training_data = pd.concat([df1[feature_columns], df2[feature_columns]], axis=0)\n",
        "scaler.fit(combined_training_data)\n",
        "\n",
        "# Standardize the manual input features\n",
        "manual_input_scaled = scaler.transform(manual_input_df)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predicted_bod = model.predict(manual_input_scaled).flatten()[0]\n",
        "\n",
        "# Output the prediction\n",
        "print(f\"\\nPredicted BOD (mg/L): {predicted_bod}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARwuHdM5Ashq",
        "outputId": "c2a595c4-d9fd-42d5-f44b-c19098d97e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the values for each feature. Type 'back' to go to the previous feature, 'clear' to clear all inputs, or 'preview' to see entered inputs.\n",
            "Enter value for pH: 13.64\n",
            "Enter value for DO (mg/L): 16.76\n",
            "Enter value for TDS (mg/L): 85.1\n",
            "Enter value for Salinity (ppt): 0.11\n",
            "Enter value for Cond (uS/cm): 169.4\n",
            "Enter value for Temp (°C): 31.7\n",
            "Enter value for TSS (mg/L): 0.0916\n",
            "Enter value for NO2 (ppm): 0.037\n",
            "Enter value for NO3 (ppm): 0.016\n",
            "Enter value for PO4  (ppm): 0.274\n",
            "Enter value for NH4 (ppm): 0.291\n",
            "Enter value for TN (ppm): 0.344\n",
            "Enter value for TP (ppm): 0.294\n",
            "Enter value for BGA-PC (ug/L): 1.91\n",
            "Enter value for Chlorophyll (ug/L): 0.68\n",
            "Enter value for Turbidity (FNU): 1.3\n",
            "Enter value for Coliform (CFU/100ml): 1100\n",
            "Enter value for Cu (ppm): 16.9\n",
            "Enter value for Fe (ppm): 46.9\n",
            "Enter value for Mn(ppm): 70.5\n",
            "Enter value for Zn(ppm): 10\n",
            "Enter value for Cr(ppm): 9.2\n",
            "Enter value for Cd(ppm): 13.5\n",
            "Enter value for Hg(ppm): 1.1\n",
            "Enter value for As(ppm): 3.2\n",
            "Enter value for Pb(ppm): 4.6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\n",
            "Predicted BOD (mg/L): 3.9612998962402344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Zipping**"
      ],
      "metadata": {
        "id": "QrH2k0REn6br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Path to the folder you want to zip\n",
        "folder_to_zip = '/content/All_Shit'\n",
        "\n",
        "# Output path for the zipped file\n",
        "output_zip = '/content/New_Shit.zip'\n",
        "\n",
        "# Zipping the folder\n",
        "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "\n",
        "# Verify if the file was created\n",
        "!ls -lh /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYTUq2gIn5jj",
        "outputId": "e6d56d76-ae95-4ac4-bc83-82b6c37364da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24M\n",
            "drwxr-xr-x 9 root root 4.0K Oct 20 06:51  All_Shit\n",
            "-rw-r--r-- 1 root root 9.5M Oct 20 06:58 'All_Shit(1).zip'\n",
            "-rw-r--r-- 1 root root   22 Oct 20 06:57  All_Shit.zip\n",
            "-rw-r--r-- 1 root root  38K Oct 20 05:32  gunao_surface.csv\n",
            "-rw-r--r-- 1 root root  14M Oct 20 07:39  New_Shit.zip\n",
            "drwxr-xr-x 1 root root 4.0K Oct 17 13:21  sample_data\n",
            "-rw-r--r-- 1 root root  74K Oct 20 05:32  tikub_surface_bottom.csv\n"
          ]
        }
      ]
    }
  ]
}