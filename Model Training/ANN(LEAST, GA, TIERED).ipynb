{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5d09e4-4b3f-4dc4-aa04-44d566ec59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f43f3-9405-4fd5-ba20-3a40fddf3b47",
   "metadata": {},
   "source": [
    "**TIERED ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b661faa2-5558-40c6-ae12-e7c85348f53f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 127ms/step - loss: 4.8238 - val_loss: 4.6389\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.1366 - val_loss: 2.6275\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1503 - val_loss: 2.0782\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.3218 - val_loss: 2.1003\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.7214 - val_loss: 2.1519\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.7835 - val_loss: 2.1058\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.6115 - val_loss: 2.0610\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.4146 - val_loss: 2.0920\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.2240 - val_loss: 2.2269\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1620 - val_loss: 2.4329\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1854 - val_loss: 2.6350\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2573 - val_loss: 2.7644\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.2928 - val_loss: 2.7739\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.2905 - val_loss: 2.6887\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.2487 - val_loss: 2.5688\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1997 - val_loss: 2.4339\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1642 - val_loss: 2.3084\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 1.1798 - val_loss: 2.2131\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.1749 - val_loss: 2.1683\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1982 - val_loss: 2.1495\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.1979 - val_loss: 2.1603\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1905 - val_loss: 2.1814\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1790 - val_loss: 2.2108\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1676 - val_loss: 2.2461\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.1569 - val_loss: 2.2839\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.1529 - val_loss: 2.3338\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.1592 - val_loss: 2.3805\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.1598 - val_loss: 2.3970\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.1608 - val_loss: 2.3917\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1554 - val_loss: 2.3557\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1431 - val_loss: 2.3116\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1291 - val_loss: 2.2488\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1195 - val_loss: 2.1850\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1066 - val_loss: 2.1359\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.0937 - val_loss: 2.0937\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0853 - val_loss: 2.0501\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.0558 - val_loss: 2.0263\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.0294 - val_loss: 2.0005\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.9803 - val_loss: 1.9485\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9291 - val_loss: 1.8799\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8763 - val_loss: 1.8277\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.8150 - val_loss: 1.7688\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7448 - val_loss: 1.7591\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6771 - val_loss: 1.7719\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6370 - val_loss: 1.7613\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5678 - val_loss: 1.7605\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5295 - val_loss: 1.7577\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5077 - val_loss: 1.7460\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4862 - val_loss: 1.7184\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4494 - val_loss: 1.6788\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4378 - val_loss: 1.6202\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4122 - val_loss: 1.6780\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4422 - val_loss: 1.7322\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4354 - val_loss: 1.7560\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4425 - val_loss: 1.7282\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4266 - val_loss: 1.9318\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4642 - val_loss: 1.8567\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4144 - val_loss: 1.6937\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3949 - val_loss: 1.4773\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3699 - val_loss: 1.3420\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3996 - val_loss: 1.1753\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3655 - val_loss: 1.1736\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3711 - val_loss: 1.2170\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3583 - val_loss: 1.3125\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3519 - val_loss: 1.4485\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3497 - val_loss: 1.4494\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3408 - val_loss: 1.4110\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3379 - val_loss: 1.4352\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3307 - val_loss: 1.5239\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3287 - val_loss: 1.5935\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3283 - val_loss: 1.5662\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3205 - val_loss: 1.5444\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3196 - val_loss: 1.5605\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3172 - val_loss: 1.6141\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3101 - val_loss: 1.6037\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2881 - val_loss: 1.5372\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2776 - val_loss: 1.5224\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.2708 - val_loss: 1.5808\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2632 - val_loss: 1.6309\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2561 - val_loss: 1.5515\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2675 - val_loss: 1.5138\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2470 - val_loss: 1.4696\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2480 - val_loss: 1.3345\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2353 - val_loss: 1.2530\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2420 - val_loss: 1.2683\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2323 - val_loss: 1.3312\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2303 - val_loss: 1.3604\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2211 - val_loss: 1.3227\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2187 - val_loss: 1.3095\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2166 - val_loss: 1.2886\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2115 - val_loss: 1.2426\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2078 - val_loss: 1.1511\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2015 - val_loss: 1.1064\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1972 - val_loss: 1.0781\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1948 - val_loss: 1.1480\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1875 - val_loss: 1.1912\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1839 - val_loss: 1.1519\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1761 - val_loss: 1.0303\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1706 - val_loss: 1.0604\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1625 - val_loss: 1.1075\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Validation Results on Small Subset:\n",
      "MSE: 1.1074566000431207\n",
      "RMSE: 1.052357638848657\n",
      "MAE: 0.6815562701225282\n",
      "R^2: 0.46221329448420967\n",
      "MAPE: 57.80965138134834 %\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 20ms/step - loss: 3.4740 - val_loss: 2.2036\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.2801 - val_loss: 1.7228\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9640 - val_loss: 1.7633\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.0115 - val_loss: 1.7188\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9741 - val_loss: 1.7150\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.9855 - val_loss: 1.6958\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9300 - val_loss: 1.6331\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.7801 - val_loss: 1.4282\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.4171 - val_loss: 1.0358\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9600 - val_loss: 0.8011\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8524 - val_loss: 0.8893\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7791 - val_loss: 0.9244\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7943 - val_loss: 0.9215\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7046 - val_loss: 1.1667\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7324 - val_loss: 0.9022\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7937 - val_loss: 0.9525\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7902 - val_loss: 0.9186\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7784 - val_loss: 0.8676\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7396 - val_loss: 0.8668\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7408 - val_loss: 0.8392\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7363 - val_loss: 0.8280\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7073 - val_loss: 0.9458\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7704 - val_loss: 1.0232\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7743 - val_loss: 0.8590\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7297 - val_loss: 0.8450\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7477 - val_loss: 0.8702\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6946 - val_loss: 0.8259\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6157 - val_loss: 0.7541\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5380 - val_loss: 0.7793\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5272 - val_loss: 0.8162\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5571 - val_loss: 0.8778\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5471 - val_loss: 0.6575\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4829 - val_loss: 0.6028\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4669 - val_loss: 0.6197\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4553 - val_loss: 0.6151\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4249 - val_loss: 0.6222\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4434 - val_loss: 0.6157\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4056 - val_loss: 0.5938\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3923 - val_loss: 0.5679\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3744 - val_loss: 0.5875\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3849 - val_loss: 0.5352\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3534 - val_loss: 0.5505\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3428 - val_loss: 0.5417\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3330 - val_loss: 0.5180\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3133 - val_loss: 0.5456\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2622 - val_loss: 0.5586\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2166 - val_loss: 0.6054\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1788 - val_loss: 0.5867\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1625 - val_loss: 0.5861\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1543 - val_loss: 0.6051\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1643 - val_loss: 0.6339\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1567 - val_loss: 0.5571\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1517 - val_loss: 0.5981\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1586 - val_loss: 0.5447\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1410 - val_loss: 0.5353\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1447 - val_loss: 0.5739\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1403 - val_loss: 0.5236\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1332 - val_loss: 0.5565\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1151 - val_loss: 0.5413\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1143 - val_loss: 0.5005\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1045 - val_loss: 0.4845\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1010 - val_loss: 0.5214\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1264 - val_loss: 0.5062\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1162 - val_loss: 0.5320\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1092 - val_loss: 0.5370\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1125 - val_loss: 0.5142\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1005 - val_loss: 0.5850\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0923 - val_loss: 0.5198\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0936 - val_loss: 0.5686\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0933 - val_loss: 0.5636\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0886 - val_loss: 0.5658\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0843 - val_loss: 0.5769\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0891 - val_loss: 0.5885\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0956 - val_loss: 0.5158\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0865 - val_loss: 0.6007\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0823 - val_loss: 0.5618\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0813 - val_loss: 0.5559\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0860 - val_loss: 0.6023\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0953 - val_loss: 0.5597\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0894 - val_loss: 0.5822\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0806 - val_loss: 0.5806\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0745 - val_loss: 0.6089\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0973 - val_loss: 0.6184\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1002 - val_loss: 0.6488\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1079 - val_loss: 0.5269\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0896 - val_loss: 0.5671\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0992 - val_loss: 0.5526\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0868 - val_loss: 0.6138\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0774 - val_loss: 0.6431\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0677 - val_loss: 0.6123\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0613 - val_loss: 0.5565\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0663 - val_loss: 0.5773\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0606 - val_loss: 0.5695\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0606 - val_loss: 0.5917\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0624 - val_loss: 0.5310\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0625 - val_loss: 0.5444\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0646 - val_loss: 0.5431\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0616 - val_loss: 0.5801\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0603 - val_loss: 0.5565\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0639 - val_loss: 0.5889\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Validation Results on Large Subset:\n",
      "MSE: 0.5889339549007133\n",
      "RMSE: 0.7674203247899506\n",
      "MAE: 0.5593151864027365\n",
      "R^2: 0.6578794805100621\n",
      "MAPE: 30.47781897138202 %\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\MODEL\\ANN\\A\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1700: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 10ms/step - loss: 4.0916 - val_loss: 2.9522\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.2560 - val_loss: 1.3095\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.1150 - val_loss: 1.4505\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.0839 - val_loss: 1.4997\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.0873 - val_loss: 1.4123\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.0815 - val_loss: 1.5132\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.9504 - val_loss: 1.0892\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.4763 - val_loss: 0.5974\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8879 - val_loss: 0.4345\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8365 - val_loss: 0.6849\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.1197 - val_loss: 0.6725\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.0056 - val_loss: 0.4712\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8448 - val_loss: 0.4733\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8929 - val_loss: 0.6272\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7231 - val_loss: 0.4606\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6444 - val_loss: 0.3944\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5911 - val_loss: 0.3676\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5076 - val_loss: 0.3216\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4413 - val_loss: 0.3140\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4219 - val_loss: 0.3171\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4142 - val_loss: 0.2597\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3837 - val_loss: 0.2537\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3450 - val_loss: 0.2576\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3357 - val_loss: 0.2442\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3418 - val_loss: 0.2459\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3359 - val_loss: 0.2946\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2795 - val_loss: 0.2526\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2443 - val_loss: 0.2308\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2194 - val_loss: 0.2723\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2122 - val_loss: 0.2924\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.2016 - val_loss: 0.2594\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1841 - val_loss: 0.1992\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1669 - val_loss: 0.3229\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1721 - val_loss: 0.2647\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 0.2357\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1410 - val_loss: 0.2636\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1466 - val_loss: 0.2898\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1369 - val_loss: 0.2540\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 0.2376\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 0.2315\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1168 - val_loss: 0.2925\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1251 - val_loss: 0.2876\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1291 - val_loss: 0.2944\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 0.2866\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 0.3065\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 0.2641\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1198 - val_loss: 0.2562\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 0.3030\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1587 - val_loss: 0.4332\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1480 - val_loss: 0.2983\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 0.3373\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1234 - val_loss: 0.2409\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1226 - val_loss: 0.2397\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 0.2836\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 0.2106\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 0.2459\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0950 - val_loss: 0.2597\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0766 - val_loss: 0.2507\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0703 - val_loss: 0.2354\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.2459\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0734 - val_loss: 0.2221\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0706 - val_loss: 0.2303\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 0.2519\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 0.2724\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 0.3189\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 0.2291\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.3022\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0735 - val_loss: 0.2265\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0680 - val_loss: 0.2167\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.2069\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.2632\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.2325\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.2631\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.2906\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.2325\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.2963\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0650 - val_loss: 0.2846\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.2778\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.2379\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.2421\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.2729\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.2973\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.2606\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2749\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2754\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.2683\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.2603\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.2537\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.2922\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.2376\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.2623\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.2376\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.3458\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.2156\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.2708\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.2534\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2651\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2842\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3314\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.2839\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "Test Results on Full Dataset:\n",
      "MSE: 0.2974474003777039\n",
      "RMSE: 0.5453873856055931\n",
      "MAE: 0.3506759653641627\n",
      "R^2: 0.8526390315089596\n",
      "MAPE: 43.29786788963371 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# First tier: small subset for initial parameter tuning\n",
    "X_small, X_rest, y_small, y_rest = train_test_split(X_scaled, y_combined, test_size=0.9, random_state=1)\n",
    "\n",
    "# Train-test split for small subset\n",
    "X_train_small, X_val_small, y_train_small, y_val_small = train_test_split(X_small, y_small, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define and train ANN model on small subset\n",
    "ann_small = Sequential()\n",
    "ann_small.add(Dense(128, input_dim=X_train_small.shape[1], activation='relu'))\n",
    "ann_small.add(Dense(64, activation='sigmoid'))\n",
    "ann_small.add(Dense(32, activation='sigmoid'))\n",
    "ann_small.add(Dense(32, activation='sigmoid'))\n",
    "ann_small.add(Dense(32, activation='sigmoid'))\n",
    "ann_small.add(Dense(32, activation='sigmoid'))\n",
    "ann_small.add(Dense(1, activation='linear'))\n",
    "\n",
    "ann_small.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "ann_small.fit(X_train_small, y_train_small, epochs=100, batch_size=20, validation_data=(X_val_small, y_val_small), verbose=1)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions_small = ann_small.predict(X_val_small).flatten()\n",
    "val_mse_small = mean_squared_error(y_val_small, val_predictions_small)\n",
    "val_rmse_small = np.sqrt(val_mse_small)\n",
    "val_mae_small = mean_absolute_error(y_val_small, val_predictions_small)\n",
    "val_r2_small = r2_score(y_val_small, val_predictions_small)\n",
    "val_mape_small = np.mean(np.abs((y_val_small - val_predictions_small) / y_val_small)) * 100\n",
    "\n",
    "print('Validation Results on Small Subset:')\n",
    "print('MSE:', val_mse_small)\n",
    "print('RMSE:', val_rmse_small)\n",
    "print('MAE:', val_mae_small)\n",
    "print('R^2:', val_r2_small)\n",
    "print('MAPE:', val_mape_small, '%')\n",
    "\n",
    "# Second tier: larger subset for more refined training\n",
    "X_large, X_rest, y_large, y_rest = train_test_split(X_scaled, y_combined, test_size=0.5, random_state=1)\n",
    "\n",
    "# Train-test split for large subset\n",
    "X_train_large, X_val_large, y_train_large, y_val_large = train_test_split(X_large, y_large, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define and train ANN model on large subset\n",
    "ann_large = Sequential()\n",
    "ann_large.add(Dense(128, input_dim=X_train_large.shape[1], activation='relu'))\n",
    "ann_large.add(Dense(64, activation='sigmoid'))\n",
    "ann_large.add(Dense(32, activation='sigmoid'))\n",
    "ann_large.add(Dense(32, activation='sigmoid'))\n",
    "ann_large.add(Dense(32, activation='sigmoid'))\n",
    "ann_large.add(Dense(32, activation='sigmoid'))\n",
    "ann_large.add(Dense(1, activation='linear'))\n",
    "\n",
    "ann_large.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "ann_large.fit(X_train_large, y_train_large, epochs=100, batch_size=20, validation_data=(X_val_large, y_val_large), verbose=1)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions_large = ann_large.predict(X_val_large).flatten()\n",
    "val_mse_large = mean_squared_error(y_val_large, val_predictions_large)\n",
    "val_rmse_large = np.sqrt(val_mse_large)\n",
    "val_mae_large = mean_absolute_error(y_val_large, val_predictions_large)\n",
    "val_r2_large = r2_score(y_val_large, val_predictions_large)\n",
    "val_mape_large = np.mean(np.abs((y_val_large - val_predictions_large) / y_val_large)) * 100\n",
    "\n",
    "print('Validation Results on Large Subset:')\n",
    "print('MSE:', val_mse_large)\n",
    "print('RMSE:', val_rmse_large)\n",
    "print('MAE:', val_mae_large)\n",
    "print('R^2:', val_r2_large)\n",
    "print('MAPE:', val_mape_large, '%')\n",
    "\n",
    "# Third tier: full dataset for final training and testing\n",
    "# Train-test split for full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_scaled, y_combined, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define and train ANN model on full dataset\n",
    "ann_full = Sequential()\n",
    "ann_full.add(Dense(128, input_dim=X_train_full.shape[1], activation='relu'))\n",
    "ann_full.add(Dense(64, activation='sigmoid'))\n",
    "ann_full.add(Dense(32, activation='sigmoid'))\n",
    "ann_full.add(Dense(32, activation='sigmoid'))\n",
    "ann_full.add(Dense(32, activation='sigmoid'))\n",
    "ann_full.add(Dense(32, activation='sigmoid'))\n",
    "ann_full.add(Dense(1, activation='linear'))\n",
    "\n",
    "ann_full.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "ann_full.fit(X_train_full, y_train_full, epochs=100, batch_size=20, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions_full = ann_full.predict(X_test_full).flatten()\n",
    "test_mse_full = mean_squared_error(y_test_full, test_predictions_full)\n",
    "test_rmse_full = np.sqrt(test_mse_full)\n",
    "test_mae_full = mean_absolute_error(y_test_full, test_predictions_full)\n",
    "test_r2_full = r2_score(y_test_full, test_predictions_full)\n",
    "test_mape_full = np.mean(np.abs((y_test_full - test_predictions_full) / y_test_full)) * 100\n",
    "\n",
    "print('Test Results on Full Dataset:')\n",
    "print('MSE:', test_mse_full)\n",
    "print('RMSE:', test_rmse_full)\n",
    "print('MAE:', test_mae_full)\n",
    "print('R^2:', test_r2_full)\n",
    "print('MAPE:', test_mape_full, '%')\n",
    "\n",
    "ann_full.save('ANN_TIERED.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67aac12-15de-4a7d-8004-32ded2bfa789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predictions for new input data:\n",
      "[1.0504938]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('ANN_BayesSearch1.h5')\n",
    "\n",
    "# Load new input data from a CSV file\n",
    "new_input_data = pd.read_csv('Book1.csv')\n",
    "\n",
    "# Define the feature columns (should be the same as the ones used in training)\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "\n",
    "# Extract features from the new input data\n",
    "X_new = new_input_data[feature_columns]\n",
    "\n",
    "# Load the scaler used for standardization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Assume the scaler has been previously fitted to the training data\n",
    "# Here we fit the scaler on the combined original training data as an example\n",
    "# In practice, you should load the already fitted scaler from your training phase\n",
    "combined_training_data = pd.concat([tikob_fil[feature_columns], gunao_fil[feature_columns]], axis=0)\n",
    "scaler.fit(combined_training_data)\n",
    "\n",
    "# Standardize the new input data\n",
    "X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = model.predict(X_new_scaled).flatten()\n",
    "\n",
    "# Output predictions to the console\n",
    "print('Predictions for new input data:')\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbe93f-a426-4cd6-86d1-ed86dba9b194",
   "metadata": {},
   "source": [
    "**LEAST ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afa10233-5c77-417f-9e4a-a4b0f51cb6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33704\\3266760728.py:65: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=20, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.7674\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4199\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1618\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0409\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9811\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9078\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8607\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8462\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7942\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.7576\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.7061\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6496\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5969\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5286\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5015\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4471\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 973us/step - loss: 0.4172\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3591\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.3207\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3030\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.2716\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2543\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2515\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2294\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.2278\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2310\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.1951\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1844\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1708\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.1628\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.1630\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1450\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.1488\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1475\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1309\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.1188\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.1154\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.1179\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.1270\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.1182\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.1073\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0961\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0932\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0967\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0835\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0808\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0792\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0730\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0679\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0697\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0634\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0639\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0597\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0585\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0584\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0575\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0500\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0680\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0561\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0518\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0675\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0840\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0698\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0529\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0439\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0398\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 868us/step - loss: 0.0447\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 866us/step - loss: 0.0437\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0368\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 987us/step - loss: 0.0500\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 853us/step - loss: 0.0369\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 952us/step - loss: 0.0335\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 863us/step - loss: 0.0359\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0307\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 919us/step - loss: 0.0311\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0399\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 958us/step - loss: 0.0384\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0487\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 901us/step - loss: 0.0468\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0231\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0223\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0238\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0249\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0319\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.0331\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0318\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0273\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0228\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0228\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0239\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.0255\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0244\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.0266\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.0228\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 1000us/step - loss: 0.0272\n",
      "Best parameters found:  OrderedDict([('activation', 'sigmoid'), ('learning_rate', 0.012832864617957343), ('n_layers', 2), ('n_neurons', 32)])\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Test Results on Full Dataset:\n",
      "MSE: 0.18740557966809432\n",
      "RMSE: 0.4329036609548299\n",
      "MAE: 0.31169370899597804\n",
      "R^2: 0.9071557939808934\n",
      "MAPE: 30.7517651455556 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Train-test split for full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_scaled, y_combined, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the ANN model function\n",
    "def create_model(n_layers=1, n_neurons=32, activation='relu', learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, input_dim=X_train_full.shape[1], activation=activation))\n",
    "    for _ in range(n_layers - 1):\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Create the KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=20, verbose=1)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_layers': Integer(1, 10),\n",
    "    'n_neurons': Integer(32, 256),\n",
    "    'activation': Categorical(['relu', 'sigmoid']),\n",
    "    'learning_rate': Real(1e-4, 1e-1, prior='log-uniform')\n",
    "}\n",
    "\n",
    "# Create the BayesSearchCV object\n",
    "opt = BayesSearchCV(estimator=model, search_spaces=param_grid, n_iter=50, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Perform the search\n",
    "opt.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters found: ', opt.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions_full = opt.predict(X_test_full).flatten()\n",
    "test_mse_full = mean_squared_error(y_test_full, test_predictions_full)\n",
    "test_rmse_full = np.sqrt(test_mse_full)\n",
    "test_mae_full = mean_absolute_error(y_test_full, test_predictions_full)\n",
    "test_r2_full = r2_score(y_test_full, test_predictions_full)\n",
    "test_mape_full = np.mean(np.abs((y_test_full - test_predictions_full) / y_test_full)) * 100\n",
    "\n",
    "print('Test Results on Full Dataset:')\n",
    "print('MSE:', test_mse_full)\n",
    "print('RMSE:', test_rmse_full)\n",
    "print('MAE:', test_mae_full)\n",
    "print('R^2:', test_r2_full)\n",
    "print('MAPE:', test_mape_full, '%')\n",
    "\n",
    "# Save the best model\n",
    "opt.best_estimator_.model.save('ANN_BayesSearch1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9082da-a6a1-436f-9202-b759295f95d9",
   "metadata": {},
   "source": [
    "**GENETICS ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf39bac-460a-46ac-ab0a-f932be4ebfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\ann_opt1\\tuner0.json\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\MODEL\\ANN\\A\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1700: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 18ms/step - loss: 17.1209 - val_loss: 2.4840\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.4455 - val_loss: 1.9114\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4801 - val_loss: 0.9197\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0617 - val_loss: 0.7132\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9124 - val_loss: 0.5856\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7699 - val_loss: 0.5746\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6078 - val_loss: 0.5866\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5575 - val_loss: 0.5587\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5318 - val_loss: 0.4908\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4916 - val_loss: 0.4419\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4535 - val_loss: 0.4305\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4401 - val_loss: 0.4051\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4220 - val_loss: 0.3928\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3676 - val_loss: 0.3677\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3362 - val_loss: 0.3440\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3273 - val_loss: 0.3670\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3165 - val_loss: 0.3279\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2836 - val_loss: 0.3089\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2420 - val_loss: 0.2701\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2112 - val_loss: 0.2568\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2043 - val_loss: 0.2380\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2003 - val_loss: 0.2448\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1936 - val_loss: 0.3060\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1768 - val_loss: 0.2316\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1626 - val_loss: 0.2148\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1530 - val_loss: 0.2337\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1571 - val_loss: 0.2118\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1498 - val_loss: 0.2331\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1345 - val_loss: 0.2460\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1242 - val_loss: 0.2372\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1140 - val_loss: 0.2181\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1196 - val_loss: 0.2891\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1386 - val_loss: 0.2483\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1396 - val_loss: 0.2379\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1196 - val_loss: 0.2283\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1117 - val_loss: 0.2034\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1064 - val_loss: 0.2257\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0940 - val_loss: 0.2116\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0878 - val_loss: 0.2350\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0865 - val_loss: 0.2010\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0971 - val_loss: 0.2588\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0961 - val_loss: 0.2052\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0855 - val_loss: 0.2057\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0803 - val_loss: 0.1903\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0710 - val_loss: 0.2101\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0718 - val_loss: 0.1982\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0797 - val_loss: 0.2286\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0909 - val_loss: 0.2098\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0929 - val_loss: 0.2290\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0807 - val_loss: 0.1800\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0699 - val_loss: 0.1955\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0702 - val_loss: 0.2286\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0703 - val_loss: 0.2744\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0784 - val_loss: 0.2072\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0749 - val_loss: 0.1972\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0579 - val_loss: 0.2040\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0521 - val_loss: 0.2040\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0490 - val_loss: 0.2038\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0448 - val_loss: 0.1910\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0446 - val_loss: 0.2128\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0478 - val_loss: 0.1988\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0597 - val_loss: 0.2330\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0675 - val_loss: 0.2194\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0763 - val_loss: 0.2363\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0610 - val_loss: 0.2046\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0701 - val_loss: 0.2155\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0548 - val_loss: 0.1917\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0420 - val_loss: 0.2073\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0388 - val_loss: 0.2243\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0356 - val_loss: 0.2001\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0295 - val_loss: 0.2144\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0326 - val_loss: 0.2181\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.2365\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0376 - val_loss: 0.2153\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0381 - val_loss: 0.2031\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0362 - val_loss: 0.2282\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0381 - val_loss: 0.2201\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0362 - val_loss: 0.1997\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0371 - val_loss: 0.1740\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.1977\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0271 - val_loss: 0.2033\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0260 - val_loss: 0.2035\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0287 - val_loss: 0.2081\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0264 - val_loss: 0.2194\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0310 - val_loss: 0.2340\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - val_loss: 0.2042\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0286 - val_loss: 0.2170\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0275 - val_loss: 0.2057\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0249 - val_loss: 0.2026\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0244 - val_loss: 0.2096\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0222 - val_loss: 0.2120\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0220 - val_loss: 0.2126\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0217 - val_loss: 0.1989\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0210 - val_loss: 0.2139\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0185 - val_loss: 0.2116\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0173 - val_loss: 0.2137\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0185 - val_loss: 0.2214\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.2147\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0198 - val_loss: 0.2183\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0179 - val_loss: 0.2306\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002E8F87F51C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "Test Results:\n",
      "MSE: 0.2632644324131533\n",
      "RMSE: 0.5130930056170648\n",
      "MAE: 0.3647216502519754\n",
      "R^2: 0.8695739089318519\n",
      "MAPE: 35.069629073148846 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import kerastuner as kt\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Train-test split for full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_combined, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the model-building function for keras-tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units1', min_value=32, max_value=512, step=32), input_dim=X_train.shape[1], activation='relu'))\n",
    "    for i in range(hp.Int('num_layers', 1, 10)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), \n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='ann_opt1'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = model.predict(X_test).flatten()\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "test_mape = np.mean(np.abs((y_test - test_predictions) / y_test)) * 100\n",
    "\n",
    "print('Test Results:')\n",
    "print('MSE:', test_mse)\n",
    "print('RMSE:', test_rmse)\n",
    "print('MAE:', test_mae)\n",
    "print('R^2:', test_r2)\n",
    "print('MAPE:', test_mape, '%')\n",
    "\n",
    "model.save('ANN_TUNED.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4bc2d47-65e9-4485-bd84-45245fc88948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "     -------------------------------------- 129.1/129.1 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: keras in h:\\model\\ann\\a\\lib\\site-packages (from keras-tuner) (2.12.0)\n",
      "Requirement already satisfied: packaging in h:\\model\\ann\\a\\lib\\site-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in h:\\model\\ann\\a\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in h:\\model\\ann\\a\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\model\\ann\\a\\lib\\site-packages (from requests->keras-tuner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\model\\ann\\a\\lib\\site-packages (from requests->keras-tuner) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\model\\ann\\a\\lib\\site-packages (from requests->keras-tuner) (2024.7.4)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f52219-b130-4cfe-b2e1-8169cc851a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Physical Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "No GPUs available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "\n",
    "# Print out the physical devices\n",
    "print(\"All Physical Devices:\", physical_devices)\n",
    "\n",
    "# Check if there are any GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs are available:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbc62dc-369b-49b8-8d34-9cf0fe3de1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.12.0\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting python_version>\"3.7\"\n",
      "  Using cached python_version-0.0.2-py2.py3-none-any.whl (3.4 kB)\n",
      "Building wheels for collected packages: tensorflow-gpu\n",
      "  Building wheel for tensorflow-gpu (setup.py): started\n",
      "  Building wheel for tensorflow-gpu (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for tensorflow-gpu\n",
      "Failed to build tensorflow-gpu\n",
      "Installing collected packages: python_version, tensorflow-gpu\n",
      "  Running setup.py install for tensorflow-gpu: started\n",
      "  Running setup.py install for tensorflow-gpu: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [18 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-wyomwncf\\tensorflow-gpu_f11440cb216e4814b267ec79312f039e\\setup.py\", line 37, in <module>\n",
      "      raise Exception(TF_REMOVAL_WARNING)\n",
      "  Exception:\n",
      "  \n",
      "  =========================================================\n",
      "  The \"tensorflow-gpu\" package has been removed!\n",
      "  \n",
      "  Please install \"tensorflow\" instead.\n",
      "  \n",
      "  Other than the name, the two packages have been identical\n",
      "  since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "  information, see: pypi.org/project/tensorflow-gpu\n",
      "  =========================================================\n",
      "  \n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tensorflow-gpu\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for tensorflow-gpu did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [18 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-wyomwncf\\tensorflow-gpu_f11440cb216e4814b267ec79312f039e\\setup.py\", line 37, in <module>\n",
      "      raise Exception(TF_REMOVAL_WARNING)\n",
      "  Exception:\n",
      "  \n",
      "  =========================================================\n",
      "  The \"tensorflow-gpu\" package has been removed!\n",
      "  \n",
      "  Please install \"tensorflow\" instead.\n",
      "  \n",
      "  Other than the name, the two packages have been identical\n",
      "  since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "  information, see: pypi.org/project/tensorflow-gpu\n",
      "  =========================================================\n",
      "  \n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "tensorflow-gpu\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819669bc-e30e-4b96-bb6f-fb2f369bdb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dec5736-4945-4874-9d9a-a044dc6e06c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 00m 21s]\n",
      "val_loss: 0.34654878079891205\n",
      "\n",
      "Best val_loss So Far: 0.15153104811906815\n",
      "Total elapsed time: 00h 21m 31s\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Use a distribution strategy for parallelism\u001b[39;00m\n\u001b[0;32m     65\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mMirroredStrategy()\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Instantiate the tuner\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuner\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHyperband\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutions_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Run multiple trials at the same time\u001b[39;49;00m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Perform the hyperparameter search\u001b[39;49;00m\n",
      "File \u001b[1;32mh:\\MODEL\\ANN\\A\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:455\u001b[0m, in \u001b[0;36m_CurrentDistributionContext.__exit__\u001b[1;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[0;32m    450\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable scope nesting error: move call to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.distribute.set_strategy() out of `with` scope.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    454\u001b[0m         e)\n\u001b[1;32m--> 455\u001b[0m \u001b[43m_pop_per_thread_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\MODEL\\ANN\\A\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribution_strategy_context.py:62\u001b[0m, in \u001b[0;36m_pop_per_thread_mode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pop_per_thread_mode\u001b[39m():\n\u001b[1;32m---> 62\u001b[0m   ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_distribution_strategy_stack\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Train-test split for full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_combined, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the model-building function for keras-tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units1', min_value=32, max_value=512, step=32), input_dim=X_train.shape[1], activation='relu'))\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), \n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Use a distribution strategy for parallelism\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Instantiate the tuner\n",
    "    tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_epochs=100,\n",
    "        factor=3,\n",
    "        directory='my_dir',\n",
    "        project_name='ann_opt2',\n",
    "        executions_per_trial=2  # Run multiple trials at the same time\n",
    "    )\n",
    "\n",
    "    # Perform the hyperparameter search\n",
    "    tuner.search(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = model.predict(X_test).flatten()\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "test_mape = np.mean(np.abs((y_test - test_predictions) / y_test)) * 100\n",
    "\n",
    "print('Test Results:')\n",
    "print('MSE:', test_mse)\n",
    "print('RMSE:', test_rmse)\n",
    "print('MAE:', test_mae)\n",
    "print('R^2:', test_r2)\n",
    "print('MAPE:', test_mape, '%')\n",
    "\n",
    "model.save('ANN_TUNED1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abe119-a214-48a3-83c1-7b638d1115e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A",
   "language": "python",
   "name": "a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
