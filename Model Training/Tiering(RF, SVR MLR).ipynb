{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a76282-4e0f-439f-8096-6bf78de1108a",
   "metadata": {},
   "source": [
    "**RANDOM FOREST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0adec2cf-b45c-4fba-876b-3d2591fa0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Using cached scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\bod\\bod\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Using cached pyaml-24.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\bod\\bod\\lib\\site-packages (from scikit-optimize) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\bod\\bod\\lib\\site-packages (from scikit-optimize) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in d:\\bod\\bod\\lib\\site-packages (from scikit-optimize) (1.5.1)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\bod\\bod\\lib\\site-packages (from scikit-optimize) (24.1)\n",
      "Requirement already satisfied: PyYAML in d:\\bod\\bod\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\bod\\bod\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Using cached scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
      "Using cached pyaml-24.7.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-24.7.0 scikit-optimize-0.10.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be841ed8-12c1-4c44-a895-c3d08ea56157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Optimized Test Results with RandomForestRegressor:\n",
      "MSE: 0.8222236784901877\n",
      "RMSE: 0.9067655035841338\n",
      "MAE: 0.5511328193446645\n",
      "R^2: 0.6729324047769527\n",
      "MAPE: 37.553254571344105 %\n",
      "Cross-validated RMSE: 0.6414226341924691\n",
      "Feature ranking:\n",
      "1. feature 0 (0.43414914829057627)\n",
      "2. feature 3 (0.08824780065916397)\n",
      "3. feature 13 (0.07139143989349468)\n",
      "4. feature 7 (0.04934455195705266)\n",
      "5. feature 1 (0.03747207954278143)\n",
      "6. feature 4 (0.03745826499274455)\n",
      "7. feature 5 (0.033710264667102446)\n",
      "8. feature 25 (0.025276113169896464)\n",
      "9. feature 14 (0.021135899226781878)\n",
      "10. feature 20 (0.01860973958555198)\n",
      "11. feature 12 (0.016689567418339303)\n",
      "12. feature 8 (0.016666810987927043)\n",
      "13. feature 2 (0.01637564179209395)\n",
      "14. feature 6 (0.01612904191535396)\n",
      "15. feature 18 (0.015567929388461724)\n",
      "16. feature 9 (0.0150633453766333)\n",
      "17. feature 11 (0.014206523834595572)\n",
      "18. feature 10 (0.013497239454827654)\n",
      "19. feature 21 (0.012186665832709952)\n",
      "20. feature 15 (0.011043704724399505)\n",
      "21. feature 19 (0.009533210744351513)\n",
      "22. feature 17 (0.008391597561451147)\n",
      "23. feature 22 (0.008335376797750604)\n",
      "24. feature 24 (0.007252354054466276)\n",
      "25. feature 16 (0.0016448977195705058)\n",
      "26. feature 23 (0.0006207904119217343)\n",
      "Optimized Test Results with RandomForestRegressor (Top Features):\n",
      "MSE: 0.8122615086522708\n",
      "RMSE: 0.9012555179594024\n",
      "MAE: 0.5439842883009178\n",
      "R^2: 0.6768951986216571\n",
      "MAPE: 37.30044292455374 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "import pickle\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_combined_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Train-test split for full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_combined_scaled, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning using Bayesian Optimization on RandomForestRegressor\n",
    "param_grid_rf = {\n",
    "    'n_estimators': (100, 500),  # Reduced upper limit\n",
    "    'max_depth': (1, 30),  # Reduced upper limit\n",
    "    'min_samples_split': (2, 10),  # Reduced upper limit\n",
    "    'min_samples_leaf': (1, 10),  # Reduced upper limit\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "bayes_search_rf = BayesSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                                search_spaces=param_grid_rf,\n",
    "                                n_iter=30,  # Reduced number of iterations\n",
    "                                cv=3,  # Reduced number of cross-validation folds\n",
    "                                n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "bayes_search_rf.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Best parameters from Bayesian optimization\n",
    "best_params_rf = bayes_search_rf.best_params_\n",
    "\n",
    "# Train the model with the best parameters\n",
    "rf_optimized = RandomForestRegressor(**best_params_rf, random_state=42)\n",
    "rf_optimized.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Save the model to a pickle file\n",
    "with open('rf_optimized_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_optimized, model_file)\n",
    "\n",
    "# Load the model from the pickle file (for demonstration purposes)\n",
    "with open('rf_optimized_model.pkl', 'rb') as model_file:\n",
    "    loaded_rf_model = pickle.load(model_file)\n",
    "\n",
    "# Cross-validation to check the performance consistency\n",
    "cv_scores = cross_val_score(loaded_rf_model, X_train_full, y_train_full, cv=3, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions_optimized = loaded_rf_model.predict(X_test_full)\n",
    "test_mse_optimized = mean_squared_error(y_test_full, test_predictions_optimized)\n",
    "test_rmse_optimized = np.sqrt(test_mse_optimized)\n",
    "test_mae_optimized = mean_absolute_error(y_test_full, test_predictions_optimized)\n",
    "test_r2_optimized = r2_score(y_test_full, test_predictions_optimized)\n",
    "test_mape_optimized = np.mean(np.abs((y_test_full - test_predictions_optimized) / y_test_full)) * 100\n",
    "\n",
    "print('Optimized Test Results with RandomForestRegressor:')\n",
    "print('MSE:', test_mse_optimized)\n",
    "print('RMSE:', test_rmse_optimized)\n",
    "print('MAE:', test_mae_optimized)\n",
    "print('R^2:', test_r2_optimized)\n",
    "print('MAPE:', test_mape_optimized, '%')\n",
    "print('Cross-validated RMSE:', cv_rmse)\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = loaded_rf_model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(X_combined.shape[1]):\n",
    "    print(f\"{i + 1}. feature {sorted_indices[i]} ({feature_importances[sorted_indices[i]]})\")\n",
    "\n",
    "# Re-train model using only the top features\n",
    "top_n = 10  # Number of top features to use\n",
    "top_features = sorted_indices[:top_n]\n",
    "X_combined_top = X_combined_scaled[:, top_features]\n",
    "\n",
    "X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(X_combined_top, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-train the optimized model using only top features\n",
    "rf_optimized_top = RandomForestRegressor(**best_params_rf, random_state=42)\n",
    "rf_optimized_top.fit(X_train_top, y_train_top)\n",
    "\n",
    "# Save the model trained on top features to a pickle file\n",
    "with open('rf_optimized_top_features_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_optimized_top, model_file)\n",
    "\n",
    "# Load the model from the pickle file (for demonstration purposes)\n",
    "with open('rf_optimized_top_features_model.pkl', 'rb') as model_file:\n",
    "    loaded_rf_model_top = pickle.load(model_file)\n",
    "\n",
    "# Evaluate on test set with top features\n",
    "test_predictions_optimized_top = loaded_rf_model_top.predict(X_test_top)\n",
    "test_mse_optimized_top = mean_squared_error(y_test_top, test_predictions_optimized_top)\n",
    "test_rmse_optimized_top = np.sqrt(test_mse_optimized_top)\n",
    "test_mae_optimized_top = mean_absolute_error(y_test_top, test_predictions_optimized_top)\n",
    "test_r2_optimized_top = r2_score(y_test_top, test_predictions_optimized_top)\n",
    "test_mape_optimized_top = np.mean(np.abs((y_test_top - test_predictions_optimized_top) / y_test_top)) * 100\n",
    "\n",
    "print('Optimized Test Results with RandomForestRegressor (Top Features):')\n",
    "print('MSE:', test_mse_optimized_top)\n",
    "print('RMSE:', test_rmse_optimized_top)\n",
    "print('MAE:', test_mae_optimized_top)\n",
    "print('R^2:', test_r2_optimized_top)\n",
    "print('MAPE:', test_mape_optimized_top, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4979c5-5dd9-4225-b0a8-4fea0ee5c4cf",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89aa5fae-5f7b-4718-a4e1-01d33b9381f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results on Small Subset:\n",
      "MSE: 1.0056481446537993\n",
      "RMSE: 1.0028200958565794\n",
      "MAE: 0.8141328563814624\n",
      "R^2: 0.16289726012742034\n",
      "MAPE: 52.539823930365706 %\n",
      "Validation Results on Large Subset:\n",
      "MSE: 0.7889408430331978\n",
      "RMSE: 0.888223419547806\n",
      "MAE: 0.5904872332040191\n",
      "R^2: 0.6806536032580996\n",
      "MAPE: 35.22313912378321 %\n",
      "Test Results on Full Dataset:\n",
      "MSE: 0.3861713071978852\n",
      "RMSE: 0.6214268317331375\n",
      "MAE: 0.4387562303294686\n",
      "R^2: 0.8463871521904118\n",
      "MAPE: 29.02174693916996 %\n",
      "Model saved to trained_svr_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_combined_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# First tier: small subset for initial parameter tuning\n",
    "X_small, X_rest, y_small, y_rest = train_test_split(X_combined_scaled, y_combined, test_size=0.9, random_state=42)\n",
    "\n",
    "# Train-test split for small subset\n",
    "X_train_small, X_val_small, y_train_small, y_val_small = train_test_split(X_small, y_small, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'svr__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'svr__epsilon': [0.001, 0.01, 0.1, 1],\n",
    "    'poly__degree': [1, 2, 3]\n",
    "}\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('svr', SVR(kernel='rbf'))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Train SVR model on small subset with best parameters\n",
    "svr_small = grid_search.best_estimator_\n",
    "svr_small.fit(X_train_small, y_train_small)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions_small = svr_small.predict(X_val_small)\n",
    "val_mse_small = mean_squared_error(y_val_small, val_predictions_small)\n",
    "val_rmse_small = np.sqrt(val_mse_small)\n",
    "val_mae_small = mean_absolute_error(y_val_small, val_predictions_small)\n",
    "val_r2_small = r2_score(y_val_small, val_predictions_small)\n",
    "val_mape_small = np.mean(np.abs((y_val_small - val_predictions_small) / y_val_small)) * 100\n",
    "\n",
    "print('Validation Results on Small Subset:')\n",
    "print('MSE:', val_mse_small)\n",
    "print('RMSE:', val_rmse_small)\n",
    "print('MAE:', val_mae_small)\n",
    "print('R^2:', val_r2_small)\n",
    "print('MAPE:', val_mape_small, '%')\n",
    "\n",
    "# Second tier: larger subset for more refined training\n",
    "X_large, X_rest, y_large, y_rest = train_test_split(X_combined_scaled, y_combined, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train-test split for large subset\n",
    "X_train_large, X_val_large, y_train_large, y_val_large = train_test_split(X_large, y_large, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVR model on large subset with best parameters\n",
    "svr_large = grid_search.best_estimator_\n",
    "svr_large.fit(X_train_large, y_train_large)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions_large = svr_large.predict(X_val_large)\n",
    "val_mse_large = mean_squared_error(y_val_large, val_predictions_large)\n",
    "val_rmse_large = np.sqrt(val_mse_large)\n",
    "val_mae_large = mean_absolute_error(y_val_large, val_predictions_large)\n",
    "val_r2_large = r2_score(y_val_large, val_predictions_large)\n",
    "val_mape_large = np.mean(np.abs((y_val_large - val_predictions_large) / y_val_large)) * 100\n",
    "\n",
    "print('Validation Results on Large Subset:')\n",
    "print('MSE:', val_mse_large)\n",
    "print('RMSE:', val_rmse_large)\n",
    "print('MAE:', val_mae_large)\n",
    "print('R^2:', val_r2_large)\n",
    "print('MAPE:', val_mape_large, '%')\n",
    "\n",
    "# Third tier: full dataset for final training and testing\n",
    "# Train-test split for full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_combined_scaled, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVR model on full dataset with best parameters\n",
    "svr_full = grid_search.best_estimator_\n",
    "svr_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions_full = svr_full.predict(X_test_full)\n",
    "test_mse_full = mean_squared_error(y_test_full, test_predictions_full)\n",
    "test_rmse_full = np.sqrt(test_mse_full)\n",
    "test_mae_full = mean_absolute_error(y_test_full, test_predictions_full)\n",
    "test_r2_full = r2_score(y_test_full, test_predictions_full)\n",
    "test_mape_full = np.mean(np.abs((y_test_full - test_predictions_full) / y_test_full)) * 100\n",
    "\n",
    "print('Test Results on Full Dataset:')\n",
    "print('MSE:', test_mse_full)\n",
    "print('RMSE:', test_rmse_full)\n",
    "print('MAE:', test_mae_full)\n",
    "print('R^2:', test_r2_full)\n",
    "print('MAPE:', test_mape_full, '%')\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "with open('trained_svr_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svr_full, f)\n",
    "\n",
    "print(\"Model saved to trained_svr_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343bf90-695b-4d2b-86d2-7d2c6841f933",
   "metadata": {},
   "source": [
    "**MLR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30e1d64-8a77-47cc-94f9-30226f463fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated features: ['TN (ppm)', 'TP (ppm)']\n",
      "Best parameters for Random Forest regression: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Test Results with Random Forest Regression:\n",
      "MSE: 0.8120282453938545\n",
      "RMSE: 0.9011260984977932\n",
      "MAE: 0.5622765057251262\n",
      "R^2: 0.6769879870622968\n",
      "MAPE: 37.96740898199653 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Feature Selection: Check for highly correlated features and drop them\n",
    "correlation_matrix = X_combined.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "print(\"Highly correlated features:\", high_corr_features)\n",
    "X_combined_filtered = X_combined.drop(columns=high_corr_features)\n",
    "\n",
    "# Train-test split for full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_combined_filtered, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Regression\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "parameters = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_regressor = GridSearchCV(rf, parameters, scoring='r2', cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the Random Forest model\n",
    "rf_regressor.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Best parameters\n",
    "best_params_rf = rf_regressor.best_params_\n",
    "print(\"Best parameters for Random Forest regression:\", best_params_rf)\n",
    "\n",
    "# Save the model to a .pkl file\n",
    "with open('random_forest_model.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_regressor, file)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions_rf = rf_regressor.predict(X_test_full)\n",
    "test_mse_rf = mean_squared_error(y_test_full, test_predictions_rf)\n",
    "test_rmse_rf = np.sqrt(test_mse_rf)\n",
    "test_mae_rf = mean_absolute_error(y_test_full, test_predictions_rf)\n",
    "test_r2_rf = r2_score(y_test_full, test_predictions_rf)\n",
    "test_mape_rf = np.mean(np.abs((y_test_full - test_predictions_rf) / y_test_full)) * 100\n",
    "\n",
    "print('Test Results with Random Forest Regression:')\n",
    "print('MSE:', test_mse_rf)\n",
    "print('RMSE:', test_rmse_rf)\n",
    "print('MAE:', test_mae_rf)\n",
    "print('R^2:', test_r2_rf)\n",
    "print('MAPE:', test_mape_rf, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b2c35-29bf-42ab-9121-18ef654f89c0",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2d6c10a-b898-4983-8bc2-0f881c74bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results with Best Model:\n",
      "MSE: 0.3100669079910257\n",
      "RMSE: 0.5568365181909549\n",
      "MAE: 0.4088966799670373\n",
      "R^2: 0.8766602803982847\n",
      "MAPE: 31.649853842599118 %\n",
      "Best Parameters: {'mlp__activation': 'tanh', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100, 50, 100), 'mlp__learning_rate': 'constant', 'mlp__solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the datasets\n",
    "gunao = pd.read_csv('gunao_surface.csv')\n",
    "tikob = pd.read_csv('tikub_surface_bottom.csv')\n",
    "\n",
    "# Filter for surface data in tikob dataset\n",
    "tikob_surface = tikob[tikob['COLLECTION'] == 'Surface']\n",
    "\n",
    "# Columns to exclude\n",
    "columns_to_exclude = ['DATE', 'MONTH', 'YEAR', 'STATION', 'REPLICATE', 'COLLECTION', 'Latitude', 'Longtitude']\n",
    "\n",
    "# Filter columns for both datasets\n",
    "tikob_fil = tikob_surface.drop(columns=columns_to_exclude)\n",
    "gunao_fil = gunao.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'pH', 'DO (mg/L)', 'TDS (mg/L)', 'Salinity (ppt)', 'Cond (uS/cm)', 'Temp (Â°C)', 'TSS (mg/L)', \n",
    "    'NO2 (ppm)', 'NO3 (ppm)', 'PO4  (ppm)', 'NH4 (ppm)', 'TN (ppm)', 'TP (ppm)', 'BGA-PC (ug/L)', \n",
    "    'Chlorophyll (ug/L)', 'Turbidity (FNU)', 'Coliform (CFU/100ml)', 'Cu (ppm)', 'Fe (ppm)', \n",
    "    'Mn(ppm)', 'Zn(ppm)', 'Cr(ppm)', 'Cd(ppm)', 'Hg(ppm)', 'As(ppm)', 'Pb(ppm)'\n",
    "]\n",
    "target_column = 'BOD (mg/L)'\n",
    "\n",
    "# Extract features and target from both datasets\n",
    "X_tikob = tikob_fil[feature_columns]\n",
    "y_tikob = tikob_fil[target_column]\n",
    "X_gunao = gunao_fil[feature_columns]\n",
    "y_gunao = gunao_fil[target_column]\n",
    "\n",
    "# Combine the datasets\n",
    "X_combined = pd.concat([X_tikob, X_gunao], axis=0)\n",
    "y_combined = pd.concat([y_tikob, y_gunao], axis=0)\n",
    "\n",
    "# Train-test split for full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a pipeline with standard scaling and MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(50, 50), (100, 100), (100, 50, 100)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__solver': ['adam', 'lbfgs'],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01],\n",
    "    'mlp__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "test_mape = np.mean(np.abs((y_test - test_predictions) / y_test)) * 100\n",
    "\n",
    "print('Test Results with Best Model:')\n",
    "print('MSE:', test_mse)\n",
    "print('RMSE:', test_rmse)\n",
    "print('MAE:', test_mae)\n",
    "print('R^2:', test_r2)\n",
    "print('MAPE:', test_mape, '%')\n",
    "print('Best Parameters:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2821f-b1a0-4a85-a1b5-17e0b8d259ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
